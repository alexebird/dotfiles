	exit
response.body
File.open("~/tmp/out.html") {|f| f.write response.body }
File.open("/home/bird/tmp/out.html") {|f| f.write response.body }
File.open("/home/bird/tmp/out.html", 'w') {|f| f.write response.body }
kase
I18n.t('case.message.notified_expert', name: kase.doctor.last_name, date: kase.doctor_notified_at.strftime('%m/%d/%Y'))I18n.t('case.message.notified_expert', name: kase.doctor.last_name, date: kase.doctor_notified_at.strftime('%m/%d/%Y'))
I18n.t('case.message.notified_expert', name: kase.doctor.last_name, date: kase.doctor_notified_at.strftime('%m/%d/%Y'))
kase.doctor
exit
@case
@case.doctor_assigned?
exit
c
@case.consultation?
@case.service_type
exit
c
h
help
finish
wtf?
exit-program
c
exit
n
kas
kase
kase.opinion_approved_at
exit
n
kase.opinion_approved_at
exit
n
s
kase
kase.opinion
kase.opinion.opinion_review
opinion_review
whereami
s
f
kase
kase.opinion
kase.opinion.opinion_review
kase.opinion.opinion_review.reload
kase.id
s
f
exit
kase
kase.opinion_approved_at
kase
exit
kase.opinion_approved_at
response
response.status
response.errors
response
ap response
ap response ; nil
ap response.to_h
ap response.to_hash
ap response.to_hsh
ap response.to_s
ap response.body
exit
ap response.body
controller.current_user
asd as   asd 
whereami
exit
current_user.roles
a = Ability.new(current_user)
a
a.can?(:approve_opinion, Case)
exit
exit-all
ability.can?(:approve_opinion, CAse)
ability.can?(:approve_opinion, Case)
ability.can?(:approve_opinion, case)
ability.can?(:approve_opinion, kase)
ability.can?(:approve_opinion, @case)
ability.user
ability.role
exit
c
x = "asdf "
x = "asdf " ; "bds"
exit
c
User.first
Case.first
OpinionReview.all
exit
Hirb.disable
OpinionReview.all
Hirb.enable
OpinionReview.all
OpinionReview.all.first
exit
response.body
response.code
response.status
response.body
File.open('/home/bird/tmp/out2.html', 'w') {|f| f.write response.body }
kase.state
kase.relative_to_state("after_or_equal", "opinion_written")
kase.opinions.present?
exit
Case.find_by_identifier("6594b75ef403f597")
c = Case.find_by_identifier("6594b75ef403f597")
ca = Case.find_by_identifier("6594b75ef403f597")
ca.last_state
ca.state
ca.update_attribute!(:state, :opinion_written)
ca.update_attribute(:state, :opinion_written)
exit
x = `uname`
exit
x = `uname`.chomp
x = `uname`.chop
x = `uname`.chop!
x = `uname`.chop("x")
x = `uname`.chomp("x")
x = `uname`.chomp("x\n")
x = %x[uname].chomp("x\n")
x = %x(uname ``).chomp("x\n")
%w[a b c]
%i[a b c]
%q[a b c]
%Q[a b c]
%q[a b c #{1}]
%Q[a b c #{1}]
ncpu = 4
ncpu.times {|i| puts i}
ncpu.times {|i| puts i} ; nil
2.upto(ncpu)
2.upto(ncpu).each {|i| puts i} ; nil
2.upto(ncpu).times {|i| puts i} ; nil
2.upto(ncpu).each {|i| puts i} ; nil
(2..ncpu).each {|i| puts i} ; nil
2..4
def x ; def y; return 'hi' ; end; puts y; end
x
x()
2..ncpu.each {|i| puts i} ; nil
x = "hi" || "default"
x
x = arg || "default"
exit
STDIN.gets
x = STDIN.gets
x
x = gets
exit
x? = 1
exit
x? = 1
exit
format("%s %s", "hii", 2)
exit
def x
  return x: "1"
end
def x
  return :x => "1"
end
def x
  x: "1"
end
def x
  return { x: "1" }
end
x
exit
{x: "hi", y: "ho"}.values_at(:x)
{x: "hi", y: "ho", z: nil}.values_at(:x, :y).compact
{x: "hi", y: "ho", z: nil}.values_at(:x, :y).compact.join(", ")
{x: "hi", y: "ho", z: nil}.values_at(:x, :y, :z).compact.join(", ")
{x: nil, y: nil, z: nil}.values_at(:x, :y, :z).compact.join(", ")
{x: nil, y: nil, z: nil}.values_at(:x, :y, :z).compact.join(", ")exit
exit
%i[a b c]
%i[a b c d e f].each_slice(1)
%i[a b c d e f].each_slice(1).all
%i[a b c d e f].each_slice(1).to_a
%i[a b c d e f].each_slice(2).to_a
%i[a b c d e f].each_slice(2).map(&:last)to_a
%i[a b c d e f].each_slice(2).map(&:last).to_a
%i[a b c d e f].each_slice(2).map(&:first).to_a
exit
%i[a b c d e f].each_slice(2).map(&:first).to_a
%i[a b c d e f].partition.each_with_index {|e,i| i.even? }
%i[a b c d e f].partition.each_index {|i| i.even? }
%i[a b c d e f].partition.each_index{|i| i.even? }.map{|i| i.join }
%i[a b c d e f].partition.each_with_index{|e,i| i.even? }.map{|i| i.join }
%i[a b c d e f].partition.each_with_index{|e,i| i.even? }.map{|i| i.join(' ') }
%i[a b c d e f].partition.each_with_index{|e,i| i.even? }.map{|i| i.join(' ') }.join(' ::: ')
%w[e1 a1 e2 a2].partition.each_with_index{|e,i| i.even? }.map{|i| i.join(' ') }.join(' ::: ')
%w[e1 a1 e2 a2].partition
%w[e1 a1 e2 a2].each_with_index{|e,i| i.even? }.map{|i| i.join(' ') }.join(' ::: ')
%w[e1 a1 e2 a2].partition.each_with_index{|e,i| i.even? }.map{|i| i.join(' ') }.join(' ::: ')
%w[e1 a1 e2 a2].partition.to_a
%w[e1 a1 e2 a2].partition{|e,i| i.even? }.map{|i| i.join(' ') }.join(' ::: ')
%w[e1 a1 e2 a2].partition{|i| i.even? }.map{|i| i.join(' ') }.join(' ::: ')
%w[e1 a1 e2 a2].partition.with_index{|e,i| i.even? }.map{|i| i.join(' ') }.join(' ::: ')
%w[e1 a1 e2 a2].partition.each_with_index{|e,i| i.even? }.map{|i| i.join(' ') }.join(' ::: ')
%w[e1 a1 t1 e2 a2 t2].partition.each_with_index{|e,i| i.even? }.map{|i| i.join(' ') }.join(' ::: ')
%w[e1 a1 t1 e2 a2 t2].partition.each_with_index{|e,i| i % 3 }.map{|i| i.join(' ') }.join(' ::: ')
%w[e1 a1 t1 e2 a2 t2].partition.each_with_index{|e,i| i % 3 == 0 }.map{|i| i.join(' ') }.join(' ::: ')
%w[[e1 a1 t1] [e2 a2 t2]].partition.each_with_index{|e,i| i % 3 == 0 }.map{|i| i.join(' ') }.join(' ::: ')
%w[[e1 a1 t1] [e2 a2 t2]]
%w[[e1 a1 t1] [e2 a2 t2]].map{|a| a[0] }
[%w[e1 a1 t1] %w[e2 a2 t2]].map{|a| a[0] }
[%w[e1 a1 t1], %w[e2 a2 t2]].map{|a| a[0] }
[%w[e1 a1 t1], %w[e2 a2 t2]].map{|a| a[1] }
[%w[e1 a1 t1], %w[e2 a2 t2]].map{|a| a[3] }
[%w[e1 a1 t1], %w[e2 a2 t2]].map{|a| a[2] }
[%w[e1 a1 t1], %w[e2 a2 t2]]
[%w[e1 a1 t1], %w[e2 a2 t2], %w[e3 a3 t3]]
x = [%w[e1 a1 t1], %w[e2 a2 t2], %w[e3 a3 t3]]
x.size.times {|i| put i }
x.size.times {|i| puts i }
x.size.times {|i| x.map{|e| e[i] } }
x.size.times {|i| x.map{|e| e[i] }.join(' ') }
x.map.with_index {|i| x.map{|e| e[i] }.join(' ') }
x.map.with_index {|e,i| x.map{|e| e[i] }.join(' ') }
x.map.with_index {|e,i| x.map{|e| e[i] }.join(' ') }.join(' ::: ')
fg
exi
exit
[%w[e1 a1 t1], %w[e2 a2 t2], %w[e3 a2 t2]]
[%w[e1 a1 t1], %w[e2 a2 t2], %w[e3 a2 t2]].zip
x = [%w[e1 a1 t1], %w[e2 a2 t2], %w[e3 a2 t2]]
x[0].zip(x[1])
x[0].reduce {|m,v| m.zip(v) }
x.reduce {|m,v| m.zip(v) }
x[0].zip(x[1])
x[0].zip(x[1]).zip(x[2])
x[0].zip(x[1])
x[0]
exit
exit
p "hi"
fg
puts 'hi'
p 'hi'
p 'hi' ; p 'ho'
system(%Q[echo -e "sleep 3\nsleep 6" | parallel])
puts(%Q[echo -e "sleep 3\nsleep 6" | parallel])
puts(%Q[echo -e "sleep 3\\nsleep 6" | parallel])
system %Q[echo -e "sleep 3\\nsleep 6" | parallel]
system %q[echo -e "sleep 3\\nsleep 6" | parallel]
%q[echo -e "sleep 3\\nsleep 6" | parallel]
puts %q[echo -e "sleep 3\\nsleep 6" | parallel]
puts %q[echo -e "sleep 3\\nsleep 6"]
system %q[echo -e "sleep 3\\nsleep 6"]
system ['echo', '-e', '"sleep 3\nsleep 6"'\]
system ['echo', '-e', '"sleep 3\nsleep 6"']
system('echo', '-e', '"sleep 3\nsleep 6"')
system('echo', '-e', 'sleep 3\nsleep 6')
system('echo', '-e', 'sleep 3\nsleep 6', '|', 'parallel')
%x[echo -e "sleep 3\\nsleep 6"]
%x[echo hi]
system %Q[echo hi]
system %Q[echo -e hi\n]
system %Q[echo -e hi\\n]
system %Q[echo -e hi\n]
system %q[echo -e hi\n]
system %q[echo -e hi\\n]
system %q[echo -e hi\n]
system %q[echo -e hi]
system %Q[parallel ::: "sleep 5" "sleep 7"]
system %Q[parallel ::: "sleep 50" "sleep 70"]
system 'parallel', ':::', "sleep 50", "sleep 70"
system 'parallel', ':::', *["sleep 50", "sleep 70"]
FileUtils
__FILE__
exit
File.directory?('jarvis')
Time.now
Time.now.strftime("%Y")
Time.now.strftime("%Y%m")
Time.now.strftime("%Y%m%d")
Time.now.strftime("%Y%m%d%H")
Time.now.strftime("%Y%m%d%H%M")
Time.now.strftime("%Y%m%d%H%M%s")
Time.now.strftime("%Y%m%d%H%M%S")
puts "%10s -> %s" % ['jarvis', 'asdf']
puts "%-10s -> %s" % ['jarvis', 'asdf']
File.expand_path('log')
"asdf asdf".include?(/ /)
"asdf asdf".include?(' ')
exit
x = Hash.new([])
x[:x]
x[:x] << "a"
x
x[:x] << "a"
x
puts x
x[:x}
x[:x]
x
x[:x] = []
x
x[:x]
x = Hash.new({})
x[:a] = 3
x
x[:a][:b] = 3
x[:x][:b] = 3
x
x[:x] ||= {}
x
x[:x]
puts x
x[:x] = {}
x
{"xx" => 2, "yyy" => 3}.keys
{"xx" => 2, "yyy" => 3}.keys.map(&:size)
{"xx" => 2, "yyy" => 3}.keys.map(&:size).max
File.touch
File.write('/tmp/asdf.txt')
File.write('/tmp/asdf.txt', 'w')
File.ctime('/tmp/asdf.txt')
File.ctime('/tmp/asdf.txt') > Time.now
File.ctime('/tmp/asdf.txt') < Time.now
File.ctime('/tmp/asdf.txt') < (Time.now - 60)
File.ctime('/tmp/asdf.txt') > (Time.now - 60)
{x: 3, y: 5, z: 55}.select(:x, y:)
{x: 3, y: 5, z: 55}.select(:x, :y)
{x: 3, y: 5, z: 55}.values_at(:x, :y)
{x: 3, y: 5, z: 55}.map{|k,v| {k=>v} }.reduce(:merge)
exit
require 'slim'
exit
require 'slim'
debugger
exit
debugger
require 'byebug'
debugger
x = 123
%w[a b #{x}]
%w[a b #{x}].join
x = 'feature'
/feature/ =~ x
x =~ /feature/
x.match /feature/
x.match /tfeature/
x.match /^feature/
x.match /!feature/
'!feature'.match /!feature/
'feature'.match /[^x]eature/
'feature'.match /feature|bees/
'bees'.match /feature|bees/
x = []
1000000.times { x << rand }
x
x = []
1000000.times { x << rand }
x.size
%w/asdf asdf/
%w=asdf asdf=
%w$asdf asdf$
%wxasdf asdfx
%w@asdf asdf@
%w4asdf asdf4
%w~asdf asdf~
%w.asdf asdf.
%w,asdf asdf,
%w{asdf asdf{
%w.asdf asdf.
require 'hashie'
x = Hashie.new(x: 3)
x = Hashie::Mash.new(x: 3)
x.x
%w[x y z y]
%w.x y z y.
%w?x y z y?
%w?x y z y?.partition{|x| x.even? }
%w?x y z y?.partition.with_index {|x,i| puts i }
xx = %w?x y z y?; xx.partition.with_index {|x,i| i < xx.size/2 }
xx = %w?x y z y?; xx.partition.with_index {|x,i| i.even? }
xx = %w?a b c d?; xx.partition.with_index {|x,i| i.even? }
xx = %w?x y z y?; xx.partition.with_index {|x,i| i < xx.size/2 }
xx = %w?a b c d?; xx.partition.with_index {|x,i| i < xx.size/2 }
"asdfasdf".first
"asdfasdf"[0]
lambda(kase) { kase.to_s }
lambda(kase) {|kase| kase.to_s }
->(kase) {|kase| kase.to_s }
->(kase) {\ kase.to_s }
->(kase) { kase.to_s }
lambda{|kase| kase.to_s }
lambda{|kase| kase.to_s }.call('asdf')
->(kase){ kase.to_s }.call('asdf')
%w[a b c].take(2)
%w[a b c].drop(2)
%w[a b c].take(2)
%w[a b c d e f g].take(2)
def make_2d(c=2, l) ; if l.empty? ; return l ; end ; end
make_2d(2, [])
make_2d(2, %w[])
make_2d(2, %w[x])
def make_2d(c, l) ; if l.empty? ; return l ; end ; end
def make_2d(c, l) ; if l.empty? ; return l ; end ; return [l.take(c)] + make_2d(c, l.drop(c)) ; end
make_2d(2, %w[x])
puts make_2d(2, %w[x])
puts make_2d(2, %w[x]).inspect
puts make_2d(2, %w[x y]).inspect
puts make_2d(2, %w[x y z]).inspect
puts make_2d(2, %w[x y z y a b c]).inspect
make_2d(2, %w[x y z y a b c])
make_2d(2, %w[x y z y a b c]).each { |x,y| puts x ; puts y }
make_2d(2, %w[x y z y a b c]).each { |x,y| puts '%s:%s' % [x,y] }
make_2d(2, %w[x y z y a b c]).each { |x| puts '%s:%s' % [x] }
make_2d(2, %w[x y z y a b c]).each { |x| puts '%s' % [x] }
puts make_2d(0, %w[x y z y a b c]).inspect
puts make_2d(1, %w[x y z y a b c]).inspect
puts make_2d(100, %w[x y z y a b c]).inspect
puts make_2d(-1, %w[x y z y a b c]).inspect
def make_2d(c, l) ; if l.empty? || c <= 0 ; return l ; end ; return [l.take(c)] + make_2d(c, l.drop(c)) ; end
puts make_2d(-1, %w[x y z y a b c]).inspect
puts make_2d(0, %w[x y z y a b c]).inspect
puts make_2d(1, %w[x y z y a b c]).inspect
puts make_2d(2, %w[x y z y a b c]).inspect
puts make_2d(2, %w[x y z y a b ]).inspect
puts make_2d(2, %w[x y z y a b c]).inspect
puts make_2d(2, %w[x y z y a b c d]).inspect
make_2d(2, %w[x y z y a b c]).each { |x,y| puts '%s:%s' % [x,y] }
make_2d(2, %w[x y z y a b c]).each { |x,y| puts '%s:%s' % [x,y.nil?] }
make_2d(2, %w[x y z y a b c]).each { |x,y| puts '%s:%s' % [x.nil?,y.nil?] }
%w[x y z y a b c].each_slice(2) { |x,y| puts '%s:%s' % [x.nil?,y.nil?] }
doc
doc.css('#builds')
doc.css('#builds tr')
doc.css('#builds tr').select{ |tr| tr.css('td a:nth-child(4)').text == '2521' }
doc.css('#builds tr').select{ |tr| puts tr.css('td a:nth-child(4)').text }
doc.css('#builds tr').select{ |tr| puts tr.css('td a:nth-child(5)').text }
doc.css('#builds tr').select{ |tr| puts tr.css('td a:nth-child(5)') }
doc.css('#builds tr').select{ |tr| puts tr.css('td a') }
doc.css('#builds tr').select{ |tr| puts tr.css('td:nth-child(4) a') }
doc.css('#builds tr').select{ |tr| puts tr.css('td:nth-child(4) a').text }
doc.css('#builds tr').select{ |tr| tr.css('td:nth-child(4) a').text == '2550' }
fg
service
bid
rows.first {|x| x.css('td:nth-child(3) a') }
rows.first {|x| puts x.css('td:nth-child(3) a') }
rows.first {|x| puts x.css('td:nth-child(3) a') } ; nil
rows.first {|x| puts x.css('td:nth-child(3)') } ; nil
rows.first {|x| puts x.css('td') } ; nil
rows.first {|x| puts x } ; nil
rows.each {|x| puts x } ; nil
rows.each {|x| puts x.css('td') } ; nil
rows.each {|x| puts x.css('td:nth-child(3)') } ; nil
rows.first {|x| !!x.css('td:nth-child(3)') }
rows.first {|x| !!x.css('td:nth-child(3)').text == '2545' }
rows.first {|x| !!(x.css('td:nth-child(3)').text == '2545') }
rows.first {|x| puts x.css('td:nth-child(3)').text }
rows.first {|x| puts x.css('td:nth-child(3)').text ; false }
rows.select {|x| puts x.css('td:nth-child(3)').text ; false }
rows.select {|x| puts x.css('td:nth-child(4)').text ; false }
rows.select {|x| !!(x.css('td:nth-child(3)').text == '2545') }
rows.select {|x| !!(x.css('td:nth-child(4)').text == '2545') }
puts "\e[0;31mhi"
puts "hi"
[:a, :b, nil, :c].reject(&:nil?)
[:a, :b, nil, :c].compact
URI.parse
URI.parse(ENV['INTEGRITY_DATABASE_URL'])
URI.parse(ENV['INTEGRITY_DATABASE_URL']).to_h
URI.parse(ENV['INTEGRITY_DATABASE_URL'])
URI.parse(ENV['INTEGRITY_DATABASE_URL']).to_s
URI.parse(ENV['INTEGRITY_DATABASE_URL']).scheme
x = URI.parse(ENV['INTEGRITY_DATABASE_URL'])
%i[scheme]
%i[scheme host password user].each {|y| s.send(y) }
%i[scheme host password user].each {|y| puts x.send(y) }
%i[scheme host password user].each {|y| puts URI.parse('').send(y) }
fg
ENV['DATABASE_URL'] = 'mysql2://integrity:MyPassword@gruzii5m6o5mis.c61hxobewq2s.us-east-1.rds.amazonaws.com/integrity'
'mysql2://integrity:MyPassword@gruzii5m6o5mis.c61hxobewq2s.us-east-1.rds.amazonaws.com/integrity'
%i[scheme host password user].each {|y| puts URI.parse('ENV['DATABASE_URL'] = 'mysql2://integrity:MyPassword@gruzii5m6o5mis.c61hxobewq2s.us-east-1.rds.amazonaws.com/integrity'').send(y) }
%i[scheme host password user].each {|y| puts URI.parse('mysql2://integrity:MyPassword@gruzii5m6o5mis.c61hxobewq2s.us-east-1.rds.amazonaws.com/integrity'').send(y) }
%i[scheme host password user].each {|y| puts URI.parse('mysql2://integrity:MyPassword@gruzii5m6o5mis.c61hxobewq2s.us-east-1.rds.amazonaws.com/integrity').send(y) }
%i[scheme user password password host].each {|y| puts URI.parse('mysql2://integrity:MyPassword@gruzii5m6o5mis.c61hxobewq2s.us-east-1.rds.amazonaws.com/integrity').send(y) }
%i[scheme user password host].each {|y| puts URI.parse('mysql2://integrity:MyPassword@gruzii5m6o5mis.c61hxobewq2s.us-east-1.rds.amazonaws.com/integrity').send(y) }
%i[scheme user password host path].each {|y| puts URI.parse('mysql2://integrity:MyPassword@gruzii5m6o5mis.c61hxobewq2s.us-east-1.rds.amazonaws.com/integrity').send(y) }
"asdf"['df']
fg
"mysql://asdf:fdsa@asdf/asdf"
"mysql://asdf:fdsa@asdf/asdf".sub(/:.+@/, ':xxx@')
"mysql://asdf:fdsa@asdf/asdf".sub(/:.+?@/, ':xxx@')
"mysql://asdf:fdsa@asdf/asdf".sub(/.+?@/, 'xxx@')
"mysql://asdf:fdsa@asdf/asdf".sub(/:.+?@/, ':xxx@')
"mysql://asdf:fdsa@asdf/asdf".sub(/:.+@/, ':xxx@')
"mysql://asdf:fdsa@asdf/asdf".sub(/:[^:]+@/, ':xxx@')
require 'crust'
ls
require './lib/crust'
require 'fleetctl'
require 'sinatra'
get '/ping' { "pong" } 
get('/ping') { "pong" } 
run Sinatra::Application
run!
run Sinatra::Application
require 'sinatra/base'
my_app = Sinatra.new { get('/') { "hi" } }
my_app = Sinatra.new { get('/ping') { "pong" } }
my_app.run!
require 'json'
x
load 'payload'
load './payload'
load './payload.rb'
x
Integrity::DiligenceBranch.all(order: :id.desc, active: true)
Integrity::DiligenceBranch.all(order: :id.desc, active: false)
pp Integrity::DiligenceBranch.all(order: :id.desc, active: false)
ap Integrity::DiligenceBranch.all(order: :id.desc, active: false)
Integrity::DiligenceBranch.all(order: :id.desc, active: false).each do |x|
  puts x
end
Integrity::DiligenceBranch.all(order: :id.desc, active: false).each { |x| puts x.inspect } ;nil
ENV
ENV.to_s
ENV.inspect
CGI
CGI.escape "hi hi"
CGI.escape "{"id": "e5b4b670bb7cb311", "auth": "5784ace43006d0bf"}"
CGI.escape '{"id": "e5b4b670bb7cb311", "auth": "5784ace43006d0bf"}'
puts "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>RequestTimeTooSkewed</Code><Message>The difference between the request time and the current time is too large.</Message><RequestTime>Wed, 11 Mar 2015 10:40:59 +0000</RequestTime><ServerTime>2015-03-11T18:43:06Z</ServerTime><MaxAllowedSkewMilliseconds>900000</MaxAllowedSkewMilliseconds><RequestId>011C0E9EF253386B</RequestId><HostId>1SBUpL6BGY6x0aQyaSnGiuIfL1v3853J60zgaZZJUEJcG1JrfjLDlNAqQm+QSHLbHlzNypO2Y0Q=</HostId></Error>", :headers=>{"x-amz-request-id"=>"011C0E9EF253386B", "x-amz-id-2"=>"1SBUpL6BGY6x0aQyaSnGiuIfL1v3853J60zgaZZJUEJcG1JrfjLDlNAqQm+QSHLbHlzNypO2Y0Q=", "Content-Type"=>"application/xml", "Transfer-Encoding"=>"chunked", "Date"=>"Wed, 11 Mar 2015 18:43:05 GMT", "Connection"=>"close", "Server"=>"AmazonS3"}, :status=>403, :remote_ip=>"54.231.16.200"}, @body="<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>RequestTimeTooSkewed</Code><Message>The difference between the request time and the current time is too large.</Message><RequestTime>Wed, 11 Mar 2015 10:40:59 +0000</RequestTime><ServerTime>2015-03-11T18:43:06Z</ServerTime><MaxAllowedSkewMilliseconds>900000</MaxAllowedSkewMilliseconds><RequestId>011C0E9EF253386B</RequestId><HostId>1SBUpL6BGY6x0aQyaSnGiuIfL1v3853J60zgaZZJUEJcG1JrfjLDlNAqQm+QSHLbHlzNypO2Y0Q=</HostId></Error>"
puts "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>RequestTimeTooSkewed</Code><Message>The difference between the request time and the current time is too large.</Message><RequestTime>Wed, 11 Mar 2015 10:40:59 +0000</RequestTime><ServerTime>2015-03-11T18:43:06Z</ServerTime><MaxAllowedSkewMilliseconds>900000</MaxAllowedSkewMilliseconds><RequestId>011C0E9EF253386B</RequestId><HostId>1SBUpL6BGY6x0aQyaSnGiuIfL1v3853J60zgaZZJUEJcG1JrfjLDlNAqQm+QSHLbHlzNypO2Y0Q=</HostId></Error>"
def x ; puts 'printing x' ; return 'x' ; end
def y ; @y ||= x ; end
y
"Alex Bird <alexebird@gmail.com>"
"Alex Bird <alexebird@gmail.com>".gsub(/ <.+>/, '')
"Alex Bird <alexebird@gmail.com>".gsub(/<.+>/, '').strip
x = {x: 1}
x = {x: 1}.merge(y: 2)
x = {x: 1}.merge(y: 2, x:3)
%i|asdf|
@y = {y: 1} ; x = lambda { puts @y.inspect } ; x.call()
@y = {y: 1} ; x = lambda { puts @y.inspect } ; @y[:j] = :k ;  x.call()
x = lambda { puts @y.inspect } ; @y = {x: 1 } ;  x.call()
@y
@yx
@yxasdf
class X
  private
  def self.y
    puts 'hi'
  end
end
X.y
x = proc { puts @y.inspect } ; @y = {x: 1 } ;  x.call()
x.respond_to?(:call)
4.respond_to?(:call)
x = { mysql: { ports: [3306] }, jarvis: { ports: [1, 2, 3] }}
x.to_a.map{|e| e[:ports] }.reduce(&:+)
x.to_a.map{|k,v| v[:ports] }.reduce(&:+)
x.to_a.map{|k,v| v[:ports] || [] }.reduce(&:+)
x = { mysql: { ports: [3306] }, jarvis: { ports: [1, 2, 3] }, bob: { prawns: []}}
x.to_a.map{|k,v| v[:ports] || [] }.reduce(&:+)
x.to_a.map{|k,v| v[:ports]  }.reduce(&:+)
x.to_a.map{|k,v| v[:ports] || [] }.reduce(&:+)
x
x.fetch(:asdf)
x[:asdf]
def z(a) ; "z" + a.to_s ; end
x, y, b = "x", "y", z(x)
x = { mysql: { ports: [3306] }, jarvis: { ports: [1, 2, 3] }, bob: { prawns: []}}
x.each {|k,v| puts k}
x.each {|k,v| puts k} ; nil
client.repositories
client.repositories.count
client.orgs
client.orgs.repositories
client.orgs.first.repositories
client.orgs.first
client.orgs
client.orgs.first.class
Octokit::Repository.from_url("https://github.com/consultingmd/consultingmd")
Octokit::Repository.from_url("https://github.com/consultingmd/consultingmd").path
Octokit.repository_evsnts
Octokit.repository_events
Octokit.repository_events('consultingmd/consultingmd')
require 'json'
JSON.load(File.read('events.json'))
xx = JSON.load(File.read('events.json'))
xx.first
xx.map{|x| x['created_at'] }
pushes = xx.select{|x| x['type'] == 'PushEvent' }
pushes[0]
pushes[1]
pushes[2]
pushes[3]
pushes.map{|x| x['payload']['ref'] }
Octokit.repository_events('consultingmd/consultingmd')
xx = Octokit.repository_events('consultingmd/consultingmd')
xx
xx.select{|x| x[:type] == 'PushEvent' }
xx.select{|x| x[:type] == 'PushEvent' }.count
xx.select{|x| x[:type] == 'PushEvent' && x[:payload][:ref] =~ /pancakes/ }
xx = Octokit.repository_events('consultingmd/consultingmd')
xx.select{|x| x[:type] == 'PushEvent' && x[:payload][:ref] =~ /pancakes/ }
[["hi", {x:1}], ["ho", {y: 2}]].reduce({}){|m,v| puts v }
[["hi", {x:1}], ["ho", {y: 2}]].reduce({}){|m,v| puts v.inspect }
[["hi", {x:1}], ["ho", {y: 2}]].reduce({}){|m,b,info| puts b.inspect }
[["hi", {x:1}], ["ho", {y: 2}]].reduce({}){|m,(b,info)| puts b.inspect }
[["hi", {x:1}], ["ho", {y: 2}]].reduce({}){|m,(b,info)| puts b.inspect, info.inspect }
"a=#{nil}"
"a=#{nil.inspect}"
{x: 1, y: 2}.map
{x: 1, y: 2}.map{|x|puts x }
{x: 1, y: 2}.map{|k,v| puts k,v }
{jarvis: 'master', frick: 'asdf'}
{jarvis: 'master', frick: 'asdf'}.to_a
{jarvis: 'master', frick: 'asdf'}.to_a.map{|x| x.join(':')}
{jarvis: 'master', frick: 'asdf'}.to_a.map{|x| x.join(':')}.sort
{jarvis: 'master', frick: 'asdf'}.to_a.map{|x| x.join(':')}.sort.join('+')
Time.now
Time.now.to_s
Time.now.10.minutes.ago
10.minutes
"mas"["mas"]
%w[x y z].delete("x")
x = %w[x y z]
x.delete("y")
x
require 'fleet'
Fleet.new
require 'fleet'
Fleet.new
cc = Fleet.new
cc.machines
cc.list_machines
cc.list
def x(one: 1, **rest) ; puts one ; puts rest.inspect ; end
x(one: 1111)
x(one: 1111, {a: 1, b: 2})
x(one: 1111, **{a: 1, b: 2})
x(one: 1111, *{a: 1, b: 2})
x(one: 1111, **{a: 1, b: 2})
ec2 = Aws::EC2::Client.new(region:'us-east-2')
ec2.instances
ec2.describe_instances
resp = ec2.run_instances
ENV['USER_DATA']
resp = ec2.run_instances(dry_run: true, image_id: 'ami-323b195a', instance_type: 't2.micro', min_count: 1, max_count: 1, key_name: 'aws-grnds-test', security_group_ids: %w[sg-749d7310], subnet_id: 'subnet-1e29f747', user_data: ENV['USER_DATA'])
resp = ec2.run_instances(image_id: 'ami-323b195a', instance_type: 't2.micro', min_count: 1, max_count: 1, key_name: 'aws-grnds-test', security_group_ids: %w[sg-749d7310], subnet_id: 'subnet-1e29f747', user_data: ENV['USER_DATA'])
ec2.create_tags(resources: ['i-cf5cd433'], tags: [{key: 'Name', value: 'worker-ec2-1-dtest'}])
[{key: 'Name', value: name}]
[{key: 'Name', value: 'namexxxxx'}]
{ 'Name' => 'namexxxxx'}
{ 'Name' => 'namexxxxx'}.to_a
{ 'Name' => 'namexxxxx'}.to_a.map{|k,v| {key: k, value: v}}
{ 'Name' => 'namexxxxx'}.to_a.map{|k,v| {key: k, value: v}} == [{key: 'Name', value: 'namexxxxx'}]
ec2 = Aws::EC2::Client.new(region:'us-east-1')
ec2 = AWS::EC2::Client.new(region:'us-east-1')
require 'aws-sdk'
ec2 = AWS::EC2::Client.new(region:'us-east-1')
ec2.describe_instances
require 'aws-sdk'
ec2 = AWS::EC2::Client.new(region:'us-east-1')
ec2.describe_instances
pp ec2.describe_instances
pp ec2.describe_instances ; ni
pp ec2.describe_instances ; nil
version
Pancakes::Stack.etcd_inspect
Pancakes::Stack.all
xx = Pancakes::Stack.all.first
xx.etcd_get_stack
xx
xx = Pancakes::Stack.all.first
xx.etcd_get_stack
xx = Pancakes::Stack.all.first
xx.etcd_get_stack
self.class
self.class.methods
xx = Pancakes::Stack.all.first
xx.etcd_get_stack
xx = Pancakes::Stack.all.first
xx.save
xx = Pancakes::Stack.all.first
xx.save
xx = Pancakes::Stack.all.first
xx.save
xx = Pancakes::Stack.all.first
xx.logger.info "hi"
xx = Pancakes::Stack.all.first
xx.logger.info "hi"
xx = Pancakes::Stack.all.first
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.save
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.tcp_info
xx = Pancakes::Stack.all.first
xx.tcp_info
xx.tcp_info(:mysql)
xx.state_of_my_units
xx = Pancakes::Stack.all.first
xx.state_of_my_units
xx.state_of_the_pancakes
xx = Pancakes::Stack.all.first
xx.all_services_up?
xx = Pancakes::Stack.all.first
xx.all_services_up?
{}.ai
{x: 3}.ai
{x: 3}.ap
{x: 3}
Pancakes.logger.info {x: 3}.ai
Pancakes.logger.info({x: 3}.ai)
Pancakes.logger.info("\n" + {x: 3}.ai)
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
unit
unit.name
Templating.gen_service_file_name(unit)
Templating.detect_and_make_template(unit)
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
frick
frick.name
start_unit(frick)
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
unit
unit.name
c
continue
help
quit
xx = Pancakes::Stack.all.first
xx.bring_up
unit.name
gen_container_name(unit)
quit
gen_container_name(unit)
quit
xx = Pancakes::Stack.all.first
xx.bring_up
quit
unit
unit.name
quit
unit.name
quit
unit
quit
unit
quit
unit
quit
unit
quit
unit
quit
unit
backtrace
help
pry-backtrace
xx = Pancakes::Stack.all.first
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_up
unit.stack.unit(conf[:machineof])
conf[:machineof]
conf
xx = Pancakes::Stack.all.first
xx.bring_up
quit
xx.bring_up
unit.stack.unit(conf[:machineof])
conf[:machineof]
conf
conf[:machineof] ? gen_service_file_name(unit.stack.unit(conf[:machineof])) : nil
xx = Pancakes::Stack.all.first
Pancakes::Stack.all.first.bring_up
quit
conf
quit
Pancakes::Stack.all.first.bring_up
quit
unit.stack.unit(conf[:machineof])
gen_service_file_name(unit.stack.unit(conf[:machineof]))
unit.stack.networking_unit
gen_container_name(unit.stack.networking_unit)
quit
Pancakes::Stack.all.first.bring_up
quit
gen_container_name(unit)
conf
smart_requires(conf[:requires])
:x.symbol?
:x.sym?
Pancakes::Stack.all.first.bring_up
quit
Pancakes::Stack.all.first.bring_up
quit
Pancakes::Stack.all.first.bring_up
%w[a b c]]
%w[a b c]
%w[a b c].inspect
%i{a}
xx = Pancakes::Stack.all.first
xx.bring_down
xx = Pancakes::Stack.all.first
xx.bring_down
xx = Pancakes::Stack.all.first
xx.units
xx = Pancakes::Stack.all.first
xx.bring_down
xx.bring_up
xx.bring_down
xx.bring_up
xx = Pancakes::Stack.all.first
xx.bring_down
Pancakes.cluster.reset_failed
Pancakes.cluster.reset_failed('10.0.80.10')
Net::SSH.start(host, 'core'){|ssh| }
Net::SSH.start(host, 'core'){|ssh| ssh.exec("hostname") }
Net::SSH.start(host, 'core'){|ssh| puts ssh.exec("hostname") }
Net::SSH.start(host, 'core'){|ssh| puts ssh.exec("sudo ls") }
Net::SSH.start(host, 'core'){|ssh| puts ssh.exec!("sudo ls") }
Net::SSH.start(host, 'core'){|ssh| puts ssh.exec!("sudo mkdir /home/core/test") }
Net::SSH.start(host, 'core'){|ssh| puts ssh.exec!("sudo echo hi") }
Pancakes.cluster.reset_failed('10.0.80.10')
Pancakes.cluster.machine_ip
Pancakes.cluster.machine_ips
Pancakes
Pancakes::Stack.all.first
require 'fleet'
Docker
Pancakes::Stack.all.first
env
Pancakes::Stack.all.first
Pancakes::Stack.all
xx = Etcd.new
xx = Etcd.client
ENV['ETCD_HOSTNAME'] = 'etcd-dtest'
xx = Etcd.client
Etcd.client(host: ENV['ETCD_HOSTNAME'])
xx = Etcd.client(host: ENV['ETCD_HOSTNAME'])
xx.ls
xx.get
xx.get('/')
xx.delete('/bird')
xx.delete('/pancakes')
xx.delete('/pancakes', recursive: true)
xx.delete('/bird', recursive: true)
Pancakes::Stack.all
Pancakes::Stack.new_with_branch_names(jarvis: 'master', frick: 'master', tim: 'master')
Pancakes::Github.commits_from_branch_names(branch_names)
branch_nams
branch_names
Pancakes::Github.commits_from_branch_names(branch_names)
Pancakes::Github.commits_from_branch_names(jarvis: 'master', frick: 'master', tim: 'master')
branch_names
Pancakes::Github.commits_from_branch_names(jarvis: 'master', frick: 'master', tim: 'master')
codebase
branch_name
Pancakes.repo_name(codebase)
Pancakes::Commit.new_from_sha(repo: Pancakes.repo_name(:jarvis), sha: '2719cd6df04d7f0fda4262c4cbb25cd2a571b981')
xx = Pancakes::Commit.new_from_sha(repo: Pancakes.repo_name(:jarvis), sha: '2719cd6df04d7f0fda4262c4cbb25cd2a571b981')
xx.date_as_date
xx = Pancakes::Commit.new_from_sha(repo: Pancakes.repo_name(:jarvis), sha: '2719cd6df04d7f0fda4262c4cbb25cd2a571b981')
xx.date_as_date
xx.save!
{x: 1, y: 2}.map {|k,v| [k,v]}
{x: 1, y: 2}.map {|k,v| [k,v]}.to_h
[{n: 2}, {n: 3}, {n: 6}].detect{|p| p[:n] == 5 }
[{n: 2}, {n: 3}, {n: 6}].detect{|p| p[:n] == 2 }
reload
Pancakes::Unit.new_with_service_named(:mysql)
Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
"2719cd6df04d7f0fda4262c4cbb25cd2a571b981:master" 
"2719cd6df04d7f0fda4262c4cbb25cd2a571b981:master".match(/(?<sha>[0-9a-f]{40})/i)
"2719cd6df04d7f0fda4262c4cbb25cd2a571b981:master".match(/(?<sha>[0-9a-f]{40})(?::(?<branch>.+))?/i)
"master".match(/(?<sha>[0-9a-f]{40})(?::(?<branch>.+))?/i)
Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.units
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.units
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.units
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
doc
self.service_def && self.stack
self.service_def 
self.stack
exit
exit-all
exit!
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.units
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
_ex_
wtf?
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.units
xx.commits
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
me
me.commits
git_info
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
ci
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
ci
me.commits
me.commits = ci
me.commits
quit
xx
xx.save!
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
quit
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.save!
xx.errors
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.save!
xx.units.errors
xx.units.
xx.units
xx.units.map(&:errors)
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.save!
xx.units.map(&:errors)
xx.units.first.state
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.save!
Pancakes::Stack.all
Pancakes::Stack.first
Pancakes::Stack.first.commits
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.save!
xx.reload
xx.commits
Pancakes::Services.all_names
Pancakes::Services.all_names.to_set
Pancakes::Services.all_names.to_set.to_a
Pancakes::Stack.all
Pancakes::Stack.all.size
Pancakes::Stack.all.destroy
Pancakes::Stack.all.size
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.units.size
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.units.size
Pancakes::Services.all_names.to_set.to_a.size
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.units.size
xx.commits
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.commits
ci
x
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
ci
me
me.commits
me.commits = []
me.commits
me.units
me
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
ci
me
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
me .units
me.commits
me.commits = {}
me.commits
me.commits = []
me.commits
me.commits = []
me.save!
me.commits
ci
ci.first
me.commits.class
me
me.units
me.commits
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.commits
xx.save!
Pancakes::Commit.where
Pancakes::Commit.where(branch: 'master')
Pancakes::Commit.where(branch: 'master').size
Pancakes::Stack.commits.where(branch: 'master').size
Pancakes::Stack.where(branch: 'master')
Pancakes::Stack.where(branch: 'master').size
Pancakes::Stack.where("commits.branch": 'master').size
Pancakes::Stack.where("commits.branch": 'master')
ap Pancakes::Stack.where("commits.branch": 'master')
ap Pancakes::Stack.where("commits.branch": 'master').to_h
ap Pancakes::Stack.where("commits.branch": 'master').to_hash
ap Pancakes::Stack.where("commits.branch": 'master')
ap Pancakes::Stack.where("commits.branch": 'master').first
Digest::SHA1.hexdigest(Time.now.to_i)
Digest::SHA1.hexdigest(Time.now.to_i.to_s)
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.save!
Pancakes::Stack.all.map(&:sha)
Pancakes::Stack.all.map(&:sha)[0..7]
Digest::SHA1.hexdigest(Time.now.to_i.to_s)[0..7]
Digest::SHA1.hexdigest(Time.now.to_i.to_s)[0..6]
Pancakes::Stack.all.destroy
Pancakes::Stack.all.size
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
Pancakes.cluster.all_units
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.save!
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx.units.where()
xx.units.where('service_def.name' => 'mysql')
xx.units.where('service_def.name' => 'mysql').size
xx = Pancakes::Stack.first
xx.units.with_name(:mysql)
xx.units.with_name(:mysql).size
xx = Pancakes::Stack.first
xx.units.with_name(:mysql).size
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx.logger
xx = Pancakes::Stack.first
xx.logger
xx = Pancakes::Stack.first
xx.logger
xx.name
xx = Pancakes::Stack.first
xx.logger
xx.bring_up
wtf?
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx.commits
xx.units.first
xx.commits
xx.commits.first
xx = Pancakes::Stack.first
xx.commits.first
xx.units.first.service_def
wtf?
xx
xx[:desired_state]
xx = Pancakes::Stack.first
xx.units.first.service_def
xx.units.first.service_def.name
xx = Pancakes::Stack.first
xx.units.first.service_def
xx = Pancakes::Stack.first
xx.units.first.service_def
xx = Pancakes::Stack.first
xx.units.first.service_def
xx.units.last.service_def
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
wtf?
uu = xx.units.with_name(:mailcatcher)
uu.stack
uu
uu.first
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx
xx.commits
xx.commits.map{|j| [j.repo, j.branch].join(':') }
xx.commits.map{|j| [j.repo, j.branch].join('->') }
xx.commits.map{|j| [j.repo, j.branch].join('->') }.sort
xx.commits.map{|j| [j.repo, j.branch].join('->') }.rsort
xx.commits.map{|j| [j.repo, j.branch].join('->') }.revrse
xx.commits.map{|j| [j.repo, j.branch].join('->') }.reverse
xx.commits.map{|j| [j.repo, j.branch].join('->') }.reverse.sort
xx.commits.map{|j| [j.repo, j.branch].join('->') }.sort
xx.commits.map{|j| [j.repo, j.branch].join('->') }.sort.join('|')
xx.commits.map{|j| [j.repo, j.branch].join(':') }.sort.join('|')
xx = Pancakes::Stack.first
xx.branch_sha
xx = Pancakes::Stack.first
xx.bring_up
wtf?
xx = Pancakes::Stack.first
xx.bring_up
wtf?
xx = Pancakes::Stack.first
xx.bring_up
xx
xx.commits.where(repo: Pancakes.repo_name(:jarvis))
xx.commits.where(repo: Pancakes.repo_name(:jarvis)).first
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
container_name
[net_str, env_string(environment), ports_str(ports), image, cmd].flatten.map(&:to_s).map(&:strip).join(' ')
[net_str, env_string(environment), ports_str(ports), image, cmd].flatten.map(&:to_s).map(&:strip).join(' ').strip
env_string(environment)
["asdf", nil, 'asdf'].join(' ')
["asdf", nil, 'asdf'].flatten.join(' ')
["asdf", nil, 'asdf'].compact.join(' ')
exit!
xx = Pancakes::Stack.first
xx.bring_up
exit!
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
Pancakes.cluster
Pancakes.cluster.client
Pancakes.cluster.client.list
Pancakes.cluster.client.machines
Pancakes.cluster.client.list_machines
Pancakes.cluster
Pancakes.cluster.client.machines
Pancakes.cluster.client.list_machines
xx = Pancakes::Stack.first
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.save!
xx
Pancakes.cluster.client.list_machines
Pancakes::Stack.all
Pancakes::Stack.last
xx = Pancakes::Stack.last
xx.bring_up
Pancakes.cluster.client.list_machines
xx = Pancakes::Stack.last
xx.bring_up
Pancakes.cluster.client.list_machines
xx.bring_up
xx = Pancakes::Stack.last
Pancakes.cluster.client.list_machines
xx.bring_up
Pancakes.cluster.client.list_machines
xx = Pancakes::Stack.last
xx.bring_up
ap ENV
xx = Pancakes::Stack.last
xx.bring_down
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx.class
xx.class.to_s
xx.class.to_s + "[]"
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
Pancakes.cluster.all_units
Pancakes.cluster.all_units.group_by{|x| x[:name].match(/pancakes_[0-9a-f]+/)  }
Pancakes.cluster.all_units.group_by{|x| x[:name].match(/pancakes_[0-9a-f]+/)[0]  }
xx = Pancakes::Stack.last
Pancakes.cluster.stack_units(xx.name)
Pancakes.cluster.stack_units(xx)
xx = Pancakes::Stack.last
xx.my_units
xx = Pancakes::Stack.last
xx.services
xx = Pancakes::Stack.last
xx.units_state
xx = Pancakes::Stack.last
xx.units_state
xx = Pancakes::Stack.last
xx.units_state
unit_names
uu.map(&:name)
expected
expected.reduce(true){|memo,val| memo && !!unit_names.detect{|name| name.include?(val) } }
expected.reduce(true){|memo,val| memo && !!unit_names.detect{|name| name.include?(val.to_s) } }
Pancakes::Stack.last.unit_state
Pancakes::Stack.last.units_state
unit_names
Pancakes::Stack.last.units_state
expected
units
_units
_units.first.name
_units.first.name.match(/\/?(?<stack_name>pancakes_[0-9a-f]+)/)
_units.first.name.match(/\/?(?<stack_name>pancakes_[0-9a-f]+_(?<service_name>\w+))(?:\.service)?/)
_units.first.name.match(/\/?(?<stack_name>pancakes_[0-9a-f]+_(?<service_name>[a-z0-9-]+))(?:\.service)?/i)
Pancakes::Stack.last.units_state
_units
Pancakes::Stack.last.units_state
_units
Pancakes::Stack.last.units_state
expected.reduce(true){|memo,service_name| memo && !!_units.detect{|name| name.to_s.include?(service_name.to_s) } }
_units.map(&:name)
_units.map(&:service_name)
_units.map(&:service_name).to_set
_units.map(&:service_name).to_set == expected.to_set
Pancakes::Stack.last.units_state
all_active
all_active == [true]
all_active == [true].to_set
all_active.size
all_active.first
%w[failed active active]
%w[failed active active].detect{|x| x == 'failed' }
%w[failed active active].detect{|x| x == 'failedx' }
%w[failed active active].detect{|x| x == 'failed' }
!!%w[failed active active].detect{|x| x == 'failed' }
!!%w[failedx active active].detect{|x| x == 'failed' }
Pancakes::Stack.last.units_state
Pancakes::Stack.all.destroy
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.save!
xx.bring_up
Pancakes::Stack.last.units_state
Pancakes::Stack.last.all_services_up?
Pancakes::Stack.last.pingable_units
Pancakes::Stack.last.my_units
Pancakes::Stack.last.units.where('service_def.ping_url' => true)
Pancakes::Stack.last.units.where('service_def.ping_url' => true).size
Pancakes::Stack.last.units.where('service_def.ping_url' => true).exists
Pancakes::Stack.last.units.where('service_def.ping_url' => true).exists.size
Pancakes::Stack.last.units.
all
Pancakes::Stack.last.units.to_a
Pancakes::Stack.last.units.where('service_def.ping_url' => true).exists.size
Pancakes::Stack.last.units.exists('service_def.ping_url' => true).size
Pancakes::Stack.last.units.pingable
Pancakes::Stack.last.units.pingable.to_a
Pancakes::Stack.last.units.pingable.to_a.first
Pancakes::Stack.last.units.pingable.to_a.first.ping_url
Pancakes::Stack.last.my_units
Pancakes::Stack.last.units.where('service_def.name' => :mysql).first
Pancakes::Stack.last.units.where('service_def.name' => 'mysql').first
Pancakes::Stack.last.units.count
xx
xx = Pancakes::Stack.last
xx.units.to_a
xx.units.map()
xx.units.map(&:state)
xx.units.map(:state)
xx.units.map(&:state)
xx.refresh_unit_states
quit
xx.units.map(&:state)
xx.reload
xx.units.map(&:state)
xx = Pancakes::Stack.last
xx.units.map(&:state)
xx = Pancakes::Stack.last
xx.refresh_unit_states
cluster_units
xx = Pancakes::Stack.last
xx.refresh_unit_states
xx = Pancakes::Stack.last
xx.refresh_unit_states
cluster_units
xx = Pancakes::Stack.last
xx.refresh_unit_states
cluster_units
xx = Pancakes::Stack.last
xx.refresh_unit_states
xx = Pancakes::Stack.last
xx.refresh_unit_states
cluster_unit
unit.service_def.name
cluster_units
xx.services
xx = Pancakes::Stack.last
xx.refresh_unit_states
cluster_unit
xx = Pancakes::Stack.last
xx.units_state
xx = Pancakes::Stack.last
xx.units_state
uu.map{|u| Unit.parse_unit_name(u.name) }
xx = Pancakes::Stack.last
xx.units_state
uu.map{|u| Unit.parse_unit_name(u.name) }
self.units
xx = Pancakes::Stack.last
xx.units_state
uu
uu.first.service_name
any_off = uu.map(&:state)
any_off = uu.map{|u| u.state == :off }
any_off = uu.map{|u| u.state == :off }.to_set
any_off = uu.map{|u| u.state == :off }.to_set.first
[true, false].to_set
[true, false].to_set.first
[false, true].to_set.first
[false, true].sort.to_set.first
[false, true, true ,false].to_set
Array(:x)
Array([:x])
xx = Pancakes::Stack.last
xx.class.has_allowed_states(%i[x x z], :x)
xx.class.has_allowed_states(%i[x x z], :x, :z)
xx.class.has_allowed_states(%i[x x z], :x, :y)
xx.class.has_allowed_states(%i[x x z], :x)
xx.class.has_allowed_states(%i[x x z])
xx.class.detect_states(%i[x x z], :x)
xx.class.detect_states(%i[x x z], :y)
xx.class.detect_states(%i[x x z], :z)
xx.class.detect_states(%i[x x z], :x, :z)
xx.class.detect_states(%i[x x z], :x, :y)
xx = Pancakes::Stack.last
xx.units_state
xx
xx.as_json
xx.as_json(except: :commits)
xx.units_state
/a/ =~ 'asdf'
!!(/a/ =~ 'asdf')
!!(/a/ =~ 'sdf')
!!(/a/ =~ 'asdf')
Pancakes.docker
Pancakes.docker.all
Pancakes.docker.all_containers
Pancakes.docker.get_container(/mong/)
Pancakes.docker.get_container('mongo')
Pancakes.docker.get_container('mongoo')
Pancakes.cluster.all_units
Pancakes::Stack.last
Pancakes::Stack.last.bring_up
[].map{|| {} }
[].map{|| {} }.reduce(&:merge)
Pancakes::Stack.last.bring_up
wtf?
Pancakes::Stack.last.bring_up
Pancakes::Stack.last.bring_down
Pancakes::Stack.last.bring_up
Pancakes.cluster.all_units
Pancakes::Stack.last.bring_up
Pancakes::Stack.last.units_state
Pancakes::Stack.last.units
Pancakes::Stack.last.name
Pancakes::Stack.last.units.first.name
Pancakes.docker.all_containers
Pancakes.docker.all_containers.info
Pancakes.docker.all_containers.first.info
Pancakes.docker.set_machine_ip('198.199.97.120')
Pancakes.docker.all_containers.first
Pancakes.docker.all_containers
Pancakes::Stack.last.bring_up
Pancakes::Stack.last.bring_down
Pancakes::Stack.last.bring_up
Pancakes.docker.get_container(unit.name, ip)
Pancakes.docker.get_container(/mysql/, ip)
Pancakes.docker.get_container(/mysql/)
Pancakes.docker.get_container(/mysql/, '198.199.97.120')
Pancakes::Stack.last.bring_up
container_name
ports
Pancakes::Stack.last.bring_up
service_def.netmaster
unit.name
unit.service_def
Pancakes::Stack.all.destroy
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.save!
Pancakes::Stack.last
Pancakes::Stack.last.units
Pancakes::Stack.last.bring_up
service_def.netmaster
quit
Pancakes::Stack.last.bring_up
xx
Pancakes::Stack.last
Pancakes::Stack.last.units
Pancakes::Stack.last.refresh_unit_states
ip
cluster_unit
Pancakes::Stack.last.refresh_unit_states
Pancakes::Stack.last.units
Pancakes::Stack.last
Pancakes::Stack.last.commits
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master')
xx.units
xx.units.where()
xx.units.exists('service_def.ping_url').size
xx.units.exists('service_def.ping_url' => true).size
xx.save!
xx.units.exists('service_def.ping_url' => true).size
xx.reload
xx.units.exists('service_def.ping_url' => true).size
xx.units.first.ping_url
xx.units.last.ping_url
xx.units[1].ping_url
xx.units[2].ping_url
xx.units[3].ping_url
xx.units[4].ping_url
xx.units[5].ping_url
xx.units[11].ping_url
xx.units.first.service_def
xx.units[1].service_def
xx.units[2].service_def
xx.units[2].service_def.ping_url
xx.units[2].ip
xx.refresh_unit_states
xx.units[2].ip
xx.reload
xx.units[2].ip
Pancakes::Stack.all.destroy
xx = Pancakes::Stack.new_with_git_info(jarvis: 'master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx = Pancakes::Stack.create_with_git_info(jarvis: 'master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx
xx.save!
Pancakes::Stack.all.size
Pancakes::Stack.last.bring_up
Pancakes::Stack.last.bring_down
Pancakes::Stack.last.bring_up
Pancakes::Stack.last
Pancakes::Stack.last.pingable
Pancakes::Stack.last.units.pingable
Pancakes::Stack.last.units.pingable.size
Pancakes::Stack.last.state
Pancakes::Stack.last.state_from_cluster!
Pancakes::Stack.last.bring_down
Pancakes::Stack.last.state_from_cluster!
Pancakes::Stack.last.bring_up
Pancakes::Stack.last.state_from_cluster!
Pancakes.docker.all_containers
Pancakes::Stack.last.units.last.ping_url
Pancakes::Stack.last.units.last.port
Pancakes::Stack.last.state_from_cluster!
port
IPAddr
Pancakes::Stack.last.units.last
Pancakes::Stack.last.units.last.ip
IPAddr.ipv4? Pancakes::Stack.last.units.last.ip
IPAddr.new(Pancakes::Stack.last.units.last.ip).ipv4?
Pancakes::Stack.last.units.last.ip
Pancakes::Stack.last.state_from_cluster!
ports
unit.service_def.ports
ports
ports.info
ports.info['Ports']
ports.info['Ports'].map{|p| { p['PrivatePort'] => p['PublicPort'] } }
ports.info['Ports'].map{|p| { p['PrivatePort'] => p['PublicPort'] } }.reduce(&:merge)
Pancakes::Stack.last.state_from_cluster!
ports
unit
unit.service_def.ports
unit.service_def.ports.first
Pancakes::Stack.last.state_from_cluster!
unit.pingable?
unit.name
Pancakes::Stack.last.state_from_cluster!
unit.service_name
unit.name
unit.pingable?
unit.port
unit.ip
unit.service_def.ping_url
Pancakes::Stack.last.state_from_cluster!
units
units.reload
units.first
units.first.reload
Pancakes::Stack.last.state_from_cluster!
port
port_map
unit.primary_private_port
unit.service_def
unit.service_def.ports
unit.service_def.ports.first
Pancakes::Stack.last.state_from_cluster!
service_def
service_def.ports
service_def.ports.first
Pancakes::Stack.last.state_from_cluster!
xx = Time.now
Time.now - xx
Pancakes::Stack.last.state_from_cluster!
Pancakes::Stack.all.destroy
xx = Pancakes::Stack.create_with_git_info(jarvis: 'master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx.reload
xx.state_from_cluster!
xx = Pancakes::Stack.last
xx.state_from_cluster!
Pancakes.docker.all_containers
xx.state_from_cluster!
xx = Pancakes::Stack.last
xx.bring_down
xx = Pancakes::Stack.last
xx.bring_down
xx.bring_up
xx.bring_down
xx.bring_up
Pancakes.cluster.list_machines
Pancakes.cluster.client.list_machines
Pancakes.cluster.client.list_machines['machines']
Pancakes.cluster.client.list_machines['machines'].map{|m| m['primaryIP'] }
xx.state
xx = Pancakes::Stack.last
xx.state
xx = Pancakes::Stack.last
xx.state!
xx.bring_down
Pancakes.docker.logger.appenders
Pancakes.docker.logger
xx = Pancakes::Stack.last
xx.bring_down
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_down
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_down
Pancakes.cluster.reset_failed_all
Pancakes.docker.all_containers
Pancakes.cluster.reset_failed_all
xx = Pancakes::Stack.last
xx.state!
Pancakes.cluster.reset_failed_all
xx = Pancakes::Stack.last
Pancakes.cluster.reset_failed_all
xx.state!
xx.bring_up
xx.bring_down
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
quit
xx = Pancakes::Stack.last
xx.bring_up
quit
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_down
Logging
Logging.logger
Logging.logger.all
Logging.logger.methods - Object.methods
Logging.logger.instance_variables
Logging.logger[].instance_variables
Logging.logger['Pancakes::Docker']
Logging.logger['Pancakes::Docker'].methods
Logging.logger['Pancakes::Docker'].methods - Object.methods
Logging.logger['Pancakes::Docker'].appenders
Logging.logger['Pancakes::Docker'].appenders.methods - Object.methods
Logging.logger['Pancakes::Docker'].appenders.first.methods - Object.methods
Logging.logger.root.appenders.first.methods - Object.methods
Logging.logger.root.appenders
xx = Pancakes::Stack.last
xx.bring_up
logger.info "hi"
logger.appenders
logger
logger.appenders
logger.appenders.first.methods - Object.methods
Logging.show_configuration
xx = Pancakes::Stack.last
xx.bring_up
xx.bring_down
logger.info "hi"
quit
Pancakes::Stack.all.destroy
xx = Pancakes::Stack.create_with_git_info(jarvis: 'master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.name
xx.bring_up
xx.units.first.name
xx.netmaster_unit.name
xx.netmaster_unit
xx.units
Services.netmaster
Pancakes::Services.netmaster
xx.units.with_service_name("mysql")
xx.units.with_service_name("frick-server")
xx.units.with_service_name(:"frick-server")
xx
xx.save!
xx.reload
xx.units.with_service_name(:"frick-server")
Pancakes::Stack.all.destroy
xx = Pancakes::Stack.create_with_git_info(jarvis: 'master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.netmaster_unit
xx = Pancakes::Stack.last
xx.netmaster_unit
Pancakes::Stack.all.destroy
xx = Pancakes::Stack.create_with_git_info(jarvis: 'master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.bring_up
xx.bring_down
xx = Pancakes::Stack.last
xx = Pancakes::Stack.all.size
xx = Pancakes::Stack.last
xx.bring_down
xx.desired_state
xx.bring_down
Pancakes.logger.debug "hi"
Pancakes.cluster.logger.debug "hi"
xx.desired_state
xx.bring_down
xx = Pancakes::Stack.last
xx.bring_down
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_down
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx.logger.name
xx.logger.name=
xx.logger.name= 'asdf'
xx.logger.methods
xx.logger.methods - Object.methods
xx = Pancakes::Stack.last
xx.bring_down
xx.bring_up
Pancakes::Stack.all.destroy
xx = Pancakes::Stack.create_with_git_info(jarvis: 'master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.name
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx.units.where('service_def.name' => 'mysql')
xx.units.where('service_def.name' => 'mysql').size
xx.units.ne('service_def.name' => 'mysql').size
xx.units.ne('service_def.name' => 'mysql').first
Pancakes.docker.get_container(xx.units.netmaster.name)
xx
xx.units
xx = Pancakes::Stack.last
Pancakes.docker.get_container(xx.units.netmaster.name)
xx = Pancakes::Stack.last
xx.netmaster
xx.netmaster.get_container
xx.netmaster
xx.netmaster.name
xx = Pancakes::Stack.last
xx.bring_up
xx.units
xx
xx.name
xx.state!
Pancakes.docker.get_container(xx.units.first.name, xx.units.first.ip)
xx = Pancakes::Stack.last
xx.state!
Pancakes.docker.get_container(xx.units.first.name, xx.units.first.ip)
xx.state!
xx = Pancakes::Stack.last
xx.state!
ip
container
xx = Pancakes::Stack.last
xx.state!
xx = Pancakes::Stack.last
xx.state!
xx.bring_down
xx = Pancakes::Stack.last
xx.bring_up
xx.bring_down
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx.bring_down
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_up
xx.state
xx.state!
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx.sha
xx.reload
xx
xx.sha
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx.sha
xx.reload
xx.sha
xx = Pancakes::Stack.last
xx.as_json
xx.as_json only: :commits
xx.as_json
xx.commits
Pancakes::Stack.parse_git_info('50945ac456a0e8ffb166dd3351eec0ddf0ed6475')
Pancakes::Stack.parse_git_info('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:master')
Pancakes::Stack.parse_git_info('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:L+master')
Pancakes::Stack.parse_git_info('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:master')
Pancakes::Stack.parse_git_info('master')
Pancakes::Stack.parse_git_info('L+master')
Pancakes::Stack.parse_git_info('L+')
Pancakes::Stack.parse_git_info('L+asdf')
Pancakes::Stack.parse_commit_string('L+asdf')
Pancakes::Stack.parse_commit_string('L-asdf')
Pancakes::Stack.parse_commit_string('L+asdf')
Pancakes::Stack.parse_commit_string('asdf')
Pancakes::Stack.parse_git_info('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:master')
Pancakes::Stack.parse_commit_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:master')
Pancakes::Stack.parse_commit_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:L+master')
Pancakes::Stack.parse_commit_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:L-master')
Pancakes::Stack.parse_commit_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:L+master')
Pancakes::Stack.parse_commit_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:master')
Pancakes::Stack.parse_commit_string('master')
Pancakes::Stack.parse_commit_string('L+master')
Pancakes::Stack.parse_commit_string('L-master')
xx = Pancakes::Stack.last
xx.commits.first.commit_string
xx = Pancakes::Stack.last
xx.commits.first.commit_string
xx = Pancakes::Stack.last
xx.commits.first.commit_string
xx.commits.first.branch_string
xx.commits.first.branch_string(include_sha: true)
xx = Pancakes::Stack.last
xx.restart_with_branch_heads
ci
ci.branch_string
Pancakes::Stack.parse_git_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:master')
Pancakes::Commit.parse_git_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:master')
Pancakes::Commit.parse_git_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:L-master')
Pancakes::Commit.parse_git_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:L+master')
Pancakes::Commit.parse_git_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475')
Pancakes::Commit.parse_git_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:')
Pancakes::Commit.parse_git_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:asdf')
Pancakes::Commit.parse_git_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:')
Pancakes::Commit.parse_git_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475')
Pancakes::Commit.parse_git_string('50945ac456a0e8ffb166dd3351eec0ddf0ed6475:L+master')
Pancakes::Commit.parse_git_string('L+master')
Pancakes::Commit.parse_git_string('L-master')
Pancakes::Commit.parse_git_string('master')
xx = Pancakes::Stack.last
xx.commits.first.git_string
xx = Pancakes::Stack.last
xx.commits.first.git_string
xx = Pancakes::Stack.last
xx.commits.first.git_string
xx.commits.locked
xx.commits.first.locked
xx.commits.first.locked = true
xx.commits.first.git_string
xx.commits.first.branch = nil
xx.commits.first.git_string
Pancakes::Stack.all.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'd1f856b066db19ed89bd03e2a9cc1fa58842e8b3:master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.as_json
xx.commits.first
xx.commits.first.as_json
xx.commits.first.refresh_at_head
xx.commits.first.as_json
xx.commits.last.as_json
xx.commits.last.refresh_at_head
xx.commits.last.as_json
xx.commits.last.refresh_at_head
xx.commits.last.save!
xx.commits.last.as_json
xx.reload
xx.commits.last.as_json
xx = Pancakes::Stack.new_with_git_strings(jarvis: 'd1f856b066db19ed89bd03e2a9cc1fa58842e8b3:master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.commits.as_json
xx.save!
xx.reload
xx.commits.as_json
xx.commits.first
xx.commits.first.as_json
xx.commits.first.refresh_at_head
xx.commits.first.as_json
xx.reload
xx.commits.as_json
xx.refresh_commits_to_head
xx.commits.as_json
xx.commits.first
Pancakes::Stack.all.destroy
xx = Pancakes::Stack.new_with_git_strings(jarvis: 'd1f856b066db19ed89bd03e2a9cc1fa58842e8b3:master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.commits.as_json
xx.refresh_commits_to_head
xx.commits.as_json
xx.commits.first.toggle_lock
xx.commits.as_json
xx.commits.first.toggle_lock
xx.commits.first.locked
xx.commits.first.toggle_lock
xx.commits.first.locked
Pancakes::Stack.all.size
Pancakes::Stack.all.destroy
Pancakes::Stack.all.size
xx = Pancakes::Stack.new_with_git_strings(jarvis: 'd1f856b066db19ed89bd03e2a9cc1fa58842e8b3:master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
Pancakes::Stack.all.size
xx.commits
xx.commits.first
xx.commits.first.locked
xx.commits.first.locked = false
xx.commits.first.locked
xx.commits.first.toggle_locked
Pancakes::Stack.all.size
xx = Pancakes::Stack.new_with_git_strings(jarvis: 'd1f856b066db19ed89bd03e2a9cc1fa58842e8b3:master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.commits.first.locked
xx = Pancakes::Stack.new_with_git_strings(jarvis: 'd1f856b066db19ed89bd03e2a9cc1fa58842e8b3:master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.commits.first.locked?
xx.commits.first.toggle_locked
xx.commits.first.locked?
xx.commits.as_json
Pancakes::Stack.all.size
xx = Pancakes::Stack.new_with_git_strings(jarvis: 'd1f856b066db19ed89bd03e2a9cc1fa58842e8b3:master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.commits.first.toggle_locked
xx.commits.as_json
xx.refresh_commits_to_head
xx.commits.as_json
Pancakes::Stack.all.size
xx = Pancakes::Stack.new_with_git_strings(jarvis: 'd1f856b066db19ed89bd03e2a9cc1fa58842e8b3:master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx.commits.as_json
xx.commits.first.toggle_locked
Pancakes::Stack.all.size
xx.commits.as_json
xx.refresh_commits_to_head
xx.commits.as_json
Pancakes::Stack.all.size
xx = Pancakes::Stack.new_with_git_strings(jarvis: 'd1f856b066db19ed89bd03e2a9cc1fa58842e8b3:master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx
xx.commits.as_json
xx.refresh_commits_to_head
xx.commits.as_json
Pancakes.docker
Pancakes::Stack.all.size
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'master', frick: 'master', tim: 'master', services: %i[mysql frick-server])
xx = Pancakes::Stack.create_with_git_strings(frick: 'master' services: %i[mysql frick-server])
xx = Pancakes::Stack.create_with_git_strings(frick: 'master', services: %i[mysql frick-server])
xx.commits
Pancakes.docker.get_container(xx.units.first.name, xx.units.first.ip)
xx.bring_up
Pancakes.docker.get_container(xx.units.first.name, xx.units.first.ip)
cc = Pancakes.docker.get_container(xx.units.first.name, xx.units.first.ip)
cc
cc.info
cc.info['Status']
xx = Pancakes::Stack.first
Pancakes.docker.get_running_container(xx.units.first.name, xx.units.first.ip)
xx = Pancakes::Stack.first
Pancakes.docker.get_running_container(xx.units.first.name, xx.units.first.ip)
quit
xx = Pancakes::Stack.first
Pancakes.docker.get_running_container(xx.units.first.name, xx.units.first.ip)
container
name
xx.name
xx
xx.name
xx = Pancakes::Stack.all.size
xx = Pancakes::Stack.last
xx.name
Pancakes.docker.get_running_container(xx.units.first.name, xx.units.first.ip)
quit
xx = Pancakes::Stack.last
Pancakes.docker.get_running_container(xx.units.first.name, xx.units.first.ip)
Pancakes.docker.set_machine_ip(xx.units.first.ip)
Pancakes.docker.block_until_container_running(name: xx.units.first.name, timeout: 10)
Docker.class
Docker.class.name
Docker::Container.class.name
::Docker::Container.class.name
::Docker::Container.class
::Docker::Container.class == Class
Xx = 1
Xx.class
Xx = 'asdf'
Xx.class
xx = Pancakes::Stack.last
Pancakes.docker.block_until_container_running(name: xx.units.first.name, timeout: 10)
Pancakes.docker.set_machine_ip(xx.units.first.ip)
Pancakes.docker.block_until_container_running(name: xx.units.first.name, timeout: 10)
"asdf".class == "asdfasdf".class
xx = Pancakes::Stack.last
Pancakes.docker.set_machine_ip(xx.units.first.ip)
Pancakes.docker.block_until_container_running(name: xx.units.first.name, timeout: 10)
Pancakes.docker.block_until_container_running(name: xx.units.first.name, timeout: 0)
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'master', frick: 'master', tim: 'master')
Pancakes::Stack.all.to_a
Pancakes::Stack.first
Pancakes::Stack.first.bring_down
Pancakes::Stack.first.desired_state
Pancakes::Stack.first.bring_down
Pancakes::Stack.first.bring_down(force: true)
Pancakes::Stack.first.destroy
Pancakes::Stack.all.to_a
Pancakes::Stack.first
Pancakes::Stack.first.state!
Pancakes::Stack.first.bring_down
Pancakes::Stack.first.state!
Pancakes::Stack.first.destroy
Pancakes::Stack.all.size
xx = Pancakes::Stack.first
xx
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx = Pancakes::Stack.first
xx.bring_up
xx.destroy
xx = Pancakes::Stack.all.size
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'master', frick: 'master', tim: 'master')
xx = Pancakes::Stack.first
xx.bring_up
wtf?
xx = Pancakes::Stack.first
xx.bring_down
xx.bring_down!
xx = Pancakes::Stack.first
xx.bring_down!
xx.bring_up
xx = Pancakes::Stack.first
xx.restart_with_same_commits
xx = Pancakes::Stack.first
xx.restart_with_same_commits
self.units
name
nm
nm.reload
nm
refresh_unit_states
nm
xx = Pancakes::Stack.first
xx.bring_down!
Pancakes.block_until_values(values: true) {|| true }
Pancakes.block_until_values(values: true, timeout: 10) {|| true }
Pancakes.block_until_values(values: true, timeout: 10) { true }
xx = Pancakes::Stack.first
xx.netmaster
xx.netmaster.ip
xx.netmaster.ip.nil?
xx = Pancakes::Stack.first
xx.state!
xx.state
xx.state!
xx.bring_up
xx = Pancakes::Stack.first
xx.state!
xx.restart_with_same_commits
xx.state!
xx.units_state
xx = Pancakes::Stack.first
xx.units_state
statess
states
list
list-
list+
list=
xx = Pancakes::Stack.first
xx.units_state
xx = Pancakes::Stack.first
xx.units_state
quit
xx = Pancakes::Stack.first
xx.units_state
xx = Pancakes::Stack.first
xx.units_state
xx.restart_with_same_commits
xx = Pancakes::Stack.first
xx.units_state
cluster_unit
quit
cluster_unit
quit
cluster_unit
quit
cluster_unit
quit
cluster_unit
quit
cluster_unit
unit.service_name
cluster_units
xx = Pancakes::Stack.first
xx.units_state
exit!
xx = Pancakes::Stack.first
xx.units_state
xx.state!
xx = Pancakes::Stack.first
xx.state!
xx.bring_up
xx.state!
"%.0f" % 3.433
xx.state!
Pancakes::Stack.all
Pancakes::Stack.first.as_json
Pancakes::Stack.first.state!
Pancakes::Stack.first.as_json(except: [:commits, :units])
Pancakes::Stack.first.as_json(except: [:commits, :units, :history])
Pancakes::Stack.first.started_at
Time.now - Pancakes::Stack.first.started_at
Pancakes::Stack.first.units.first.as_json
Pancakes::Stack.first.commits.first.as_json
Pancakes::Stack.first.commits.first.codebase
Pancakes::Stack.first.units
Pancakes::Stack.first.units.with_service_name(:mailcatcher)
xx = Pancakes::Stack.first.units.with_service_name(:mailcatcher)
xx.service_def.ports
xx.service_def.ports = [80,25]
xx.save!
xx.reload
xx.service_def
xx
xx.ports
xx.service_def
ss = xx.service_def
xx
xx.service_def = ss
ss.save!
xx.save!
xx.reload
xx = Pancakes::Stack.first
xx.units.with_service_name(:mailcatcher)
ss
ss.delete('save')
ss
xx.units.with_service_name(:mailcatcher)
xx.units.with_service_name(:mailcatcher).service_def = ss
ss.save!
xx.units.with_service_name(:mailcatcher).service_def = ss
ss.delete('save')
xx.units.with_service_name(:mailcatcher).service_def = ss
xx.save!
xx.reload
ss
xx = Pancakes::Stack.first
yy = xx.units.with_service_name(:mailcatcher)
yy
yy.service_def
dd = yy.service_def
dd["ports"] = [80,25]
yy.service_def = dd
yy.save!
xx.reload
require 'byebue'
require 'byebug'
Pancakes::Stack.first.state!
Pancakes::Stack.all
Pancakes::Stack.find('commits.branch' => 'master')
Pancakes::Stack.where('commits.branch' => 'master')
Pancakes::Stack.where('commits.branch' => 'master').size
Pancakes::Stack.and('commits.branch' => 'master', 'commits.repo).size
Pancakes::Stack.and('commits.branch' => 'master', 'commits.repo' => 'consultingmd/consultingmd').size
Pancakes::Stack.and('commits.branch' => 'master', 'commits.repo' => 'consultingmd/consultingmd').and().to_s
Pancakes::Stack.and('commits.branch' => 'master', 'commits.repo' => 'consultingmd/consultingmd').and().inspect
Pancakes::Stack.and('commits.branch' => 'master', 'commits.repo' => 'consultingmd/consultingmd').and('commits.branch' => 'master', 'commits.repo' => 'consultingmd/frick').inspect
Pancakes::Stack.and('commits.branch' => 'master', 'commits.repo' => 'consultingmd/consultingmd').and('commits.branch' => 'master', 'commits.repo' => 'consultingmd/frick').size
Pancakes::Stack.and('commits.branch' => 'master', 'commits.repo' => 'consultingmd/consultingmd').and('commits.branch' => 'master', 'commits.repo' => 'consultingmd/frick').and('commits.branch' => 'master', 'commits.repo' => 'consultingmd/frick').size
Mongoid::Criteria.new
Mongoid::Criteria.new()
Pancakes::Stack.with_branches(jarvis: 'master')
Pancakes::Stack.with_branches(jarvis: 'master').size
Pancakes::Stack.exists
Pancakes::Stack.exists.size
Pancakes::Stack.exists('commits.branch' => 'master').size
Pancakes::Stack.elem_match('commits.branch' => 'master').size
Pancakes::Stack.elem_match('commits.branch' => 'master')
Pancakes::Stack.elem_match('commits.branch' => 'master').size
Pancakes::Stack.elem_match(commits: { branch: => 'master' }).size
Pancakes::Stack.elem_match(commits: { branch: => 'master' }}).size
Pancakes::Stack.elem_match(commits: { branch: => 'master' }).size
Pancakes::Stack.elem_match(commits: { branch: 'master' }).size
Pancakes::Stack.elem_match(commits: { repo: 'consultingmd/consultingmd' branch: 'master' }).size
Pancakes::Stack.elem_match(commits: { repo: 'consultingmd/consultingmd', branch: 'master' }).size
Pancakes::Stack.elem_match(commits: { repo: 'consultingmd/consultingmd', branch: 'master' }).elem_match(commits: { repo: 'consultingmd/frick', branch: 'master' }).size
Pancakes::Stack.with_branches(jarvis: 'master').size
Pancakes::Stack.with_branches(jarvis: 'master', frick: 'master').size
Pancakes::Stack.with_branches(jarvis: 'master', frick: 'master', tim: 'master').size
xx = Pancakes::Stack.with_branches(jarvis: 'master', frick: 'master', tim: 'master').size
xx = Pancakes::Stack.with_branches(jarvis: 'master', frick: 'master', tim: 'master').units.first
xx = Pancakes::Stack.with_branches(jarvis: 'master', frick: 'master', tim: 'master')first..units.first
xx = Pancakes::Stack.with_branches(jarvis: 'master', frick: 'master', tim: 'master')first.units.first
xx = Pancakes::Stack.with_branches(jarvis: 'master', frick: 'master', tim: 'master').first.units.first
xx = Pancakes::Stack.with_branches(jarvis: 'master', frick: 'master', tim: 'master').first.units.first.tcp_info
xx = Pancakes::Stack.with_branches(jarvis: 'master', frick: 'master', tim: 'master').first.units.first.tcp_info_string
xx = Pancakes::Stack.with_branches(jarvis: 'master', frick: 'master', tim: 'master').first.units.first.tcp_str
xx = Pancakes::Stack.create_with_git_strings(services: %i[mysql frick-server], frick: 'master')
xx.bring_up
xx.state!
xx = Pancakes::Stack.with_branches(jarvis: 'master', frick: 'master', tim: 'master').first.units.first.tcp_str
xx = Pancakes::Stack.with_branches(frick: 'master').first.units.first.tcp_str
xx = Pancakes::Stack.with_branches(frick: 'master').first.units.first
xx = Pancakes::Stack.with_branches(frick: 'master').last.units.first
xx = Pancakes::Stack.with_branches(frick: 'master').last.units.first.tcp_str
Pancakes::Stack.all
Pancakes::Stack.all.size
Pancakes::Stack.al
Pancakes::Stack.all
xx = Pancakes::Stack.create_with_git_strings(services: %i[mysql frick-server], frick: 'master')
Pancakes::Workers::StackWorker.perform_async(xx.sha, :bring_up)
$LOAD_PATH
xx = Pancakes::Stack.all
xx = Pancakes::Stack.all.size
xx = Pancakes::Stack.all.last
xx = Pancakes::Stack.find('55248f4c6269723fd5000000')
xx
xx = Pancakes::Stack.find('55248f4c6269723fd5000000')
Pancakes::Workers::BringUpWorker.perform_async(xx.id)
xx = Pancakes::Stack.find('55248f4c6269723fd5000000')
Pancakes::Workers::BringUpWorker.perform_async(xx.id)
xx.id
xx._id
xx._id.to_s
Pancakes::Workers::BringUpWorker.perform_async(xx.id.to_s)
xx = Pancakes::Stack.find('55248f4c6269723fd5000000')
xx.id.to_s
Pancakes.cluster.machine_ips
@client.list_machines['machines']
Pancakes.cluster.client.list_machines
Pancakes.cluster.client.list_machines.select{|m| m['metadata']['role'] == 'worker' }.map{|m| m['primaryIP'] }
Pancakes.cluster.client.list_machines['machines'].select{|m| m['metadata']['role'] == 'worker' }.map{|m| m['primaryIP'] }
Pancakes::Stack.all
Pancakes::Stack.all.size
ENV['FLEETCTL_ENDPOINT']
ENV['FLEETCTL_ENDPOINT'].sub(/http:\/\//, '')
ENV['FLEETCTL_ENDPOINT'].sub(/http:\/\//, '').split(':)
ENV['FLEETCTL_ENDPOINT'].sub(/http:\/\//, '').split(':')
ENV['FLEETCTL_ENDPOINT'].sub(/http:\/\//, '').split(':').first
ENV['FLEETCTL_ENDPOINT'].sub(/https?:\/\//, '').split(':').first
ENV['FLEETCTL_ENDPOINT'].sub(/https?:\/\//, '').split(':')[0]
Pancakes::Stack.with_branches(jarvis: 'master')
Pancakes::Stack.with_branches(jarvis: 'master').size
Pancakes::Stack.with_branches(frick: 'master').size
Sinatra::Application.routes
Integrity::App.routes
Integrity::App.routes["]
Integrity::App.routes['GET']
RACK_ENV
env
app.environment
app
Pancakes::Stack.all
Pancakes::Stack.first.state!
Pancakes.cluster
Pancakes.cluster.client.list
Delayed::Job.logger
Delayed::Worker.logger
Delayed::Worker.logger.info "hi"
Delayed::Job.all
Delayed::Job.queues
Delayed::Job.all
Pancakes::Stack.all.destroy
def hi(**o) ; puts o.inspect ; end
hi name: hi
hi name: 'hi'
CloudConfig::Config.new
fleet
CloudConfig::Config.new
fleet
CloudConfig::Config.new
fleet
metadata(role: 'worker')
[fleet, etcd, metadata(role: 'worker')]
CloudConfig::Config.new
[fleet, etcd('foo'), metadata(role: 'worker')]
%w[a s c]
%w[a s c] << 'x'
%w[a s c].shift('x')
%w[a s c].unshift('x')
%w[a s c].concat('x')
%w[a s c].concat(Array('x'))
CloudConfig::Config.new
[fleet, etcd('foo'), metadata(role: 'worker')]
[fleet, etcd('foo'), metadata(role: 'worker')].reduce{|config,partial| config_merge(config, partial) }
CloudConfig::Config.new
CloudConfig::EtcdConfig.new(discovery_url: 'foo')
xx = CloudConfig::EtcdConfig.new(discovery_url: 'foo')
xx.to_yaml
xx = CloudConfig::EtcdConfig.new(discovery_url: 'foo')
xx.to_yaml
puts xx.to_yaml
xx.to_yaml
xx = CloudConfig::EtcdConfig.new(discovery_url: 'foo')
puts xx.to_yaml
puts xx
puts xx.cloud_config
pp xx.cloud_config
puts xx.to_yaml
xx = CloudConfig::EtcdConfig.new(discovery_url: 'foo')
puts xx.to_yaml
gf
xx = CloudConfig::EtcdConfig.new(discovery_url: 'foo')
puts xx.to_yaml
xx = CloudConfig::EtcdConfig.new(discovery_url: 'foo')
puts xx.to_yaml
opts
@hostname
xx = CloudConfig::EtcdConfig.new(discovery_url: 'foo', hostname: 'bar')
@hostname
xx = CloudConfig::EtcdConfig.new(discovery_url: 'foo', hostname: 'bar')
puts xx.to_yaml
xx = CloudConfig::EtcdConfig.new(discovery_url: 'foo', hostname: 'bar')
puts xx.to_yaml
xx = CloudConfig::EtcdConfig.new(discovery_url: 'foo', hostname: 'bar')
puts xx.to_yaml\
puts xx.to_yaml
CloudConfig::WorkerConfig.new(fleet_etcd_servers: %w[http://10.0.0.101:4001 http://10.0.0.102:4001 http://10.0.0.103:4001], hostname: 'bar-worker-1').to_yaml
puts CloudConfig::WorkerConfig.new(fleet_etcd_servers: %w[http://10.0.0.101:4001 http://10.0.0.102:4001 http://10.0.0.103:4001], hostname: 'bar-worker-1').to_yaml
CloudConfig::EtcdCluster.new(discovery_url: `curl -s https://discovery.etcd.io/new`, hostname: 'bar-worker-%d', host_count: 2)
CloudConfig::EtcdCluster.new(discovery_url: `curl -s https://discovery.etcd.io/new`, hostname_pattern: 'bar-worker-%d', host_count: 2)
i
xx = CloudConfig::EtcdCluster.new(discovery_url: `curl -s https://discovery.etcd.io/new`, hostname_pattern: 'bar-worker-%d', host_count: 2)
xx.configs.first
puts xx.configs.first
puts xx.configs.first.to_yaml
puts xx.configs[1].to_yaml
xx = CloudConfig::EtcdCluster.new(discovery_url: `curl -s https://discovery.etcd.io/new`, hostname_pattern: 'etcd-%d-asdf', host_count: 2)
xx = CloudConfig::EtcdCluster.new(hostname_pattern: 'etcd-%d-asdf', host_count: 2)
puts xx.configs[1].to_yaml
puts xx.configs[0].to_yaml
options
options[:hostname_nums]
options[:hostname_nums].last
options
cluster
cluster.configs
cluster.configs.first
puts cluster.configs.first.to_yaml
puts cluster.configs.last.to_yaml
FileUtils.pwd()
FileUtils.pwd
v
File.expand_path(v)
File.expand_path('/' +v)
ARGV
vowels = 'aeiou'.split
vowels = 'aeiou'.split(//)
vowels = 'aeiou'.to_a
vowels = 'aeiou'.chars
conses = 'bcdfghjklmnpqrstvwxyz'.chars
10.times.map {|i| i.to_s}
vowels.sample
word = lambda {|len| len.times.map {|i| i.even? ? conses.sample : vowels.sample }  }.join
word = lambda {|len| len.times.map {|i| i.even? ? conses.sample : vowels.sample }.join  }
word.call(3)
word.call(4)
word = lambda {|len| len.times.map {|i| i.even? ? 'bcdfghjklmnpqrstvwxz'.chars.sample : 'aeiouy'.chars..sample }.join  }
word.call(10)
word = lambda {|len| len.times.map {|i| i.even? ? 'bcdfghjklmnpqrstvwxz'.chars.sample : 'aeiouy'.chars.sample }.join  }
word.call(10)
word.call(4)
word.call(5)
word.call(4)
word = lambda {|len| len.times.map {|i| i.even? ? 'bcdfghjklmnpqrstvwxz'.chars.sample : 'aeiouy'.chars.sample }.join }
%w[a b c].detect{|x| x == 'a' }
%w[a b c].detect{|x| x =~ /a/ }
%w[abc b c].detect{|x| x =~ /a/ }
require_relative 'cloud_config/partials'
require_relative 'lib/cloud_config/partials'
exit-all
ROOT
__FILE__
ROOt
ROOT
"234-adsf_asdf.pwid".gsub(/[^0-9a-z]/i, '')
"234-adsf_asdf.pwid".gsub(/[^0-9a-zA-Z]/, '')
"12345678"
x = "12345678"
x
x[4] = '-'
x
x = "12345678"
x.insert(4) = '-'
x.insert(4, '-')
x
CloudConfig::Cluster.silly_name
Integrity.crust.get_service_status
services
services.select{|s| s[:name].scan(/_/).count == 3}
services.select{|s| s['name'].scan(/_/).count == 3}
services['units'].select{|s| s['name'].scan(/_/).count == 3}
Integrity.crust.get_service_status
help
Integrity.crust.get_service_status
c
Integrity.crust.get_service_status
c
Integrity.crust.get_service_status
n
c
ss = Integrity.crust.get_service_status
ss.each {|id,status| puts id, status }
ss.each {|id,status| puts id, status } ; nil
ss.each {|id,status| puts id, status[:active] } ; nil
ss.each {|id,status| puts id, status['active'] } ; nil
ss.each {|id,status| puts id, status['currentState'] } ; nil
ss.each {|id,status| puts id, status.class } ; nil
ss.each {|id,status| puts id, status..detect{|s| s[:type] == 'app' } } ; nil
ss.each {|id,status| puts id, status.detect{|s| s[:type] == 'app' } } ; nil
ss = Integrity.crust.fleet_client
::Fleet.new.list
::Fleet.new.list_units
::Fleet.new.list
ss = Integrity.crust.fleet_client
ss = Integrity.crust.get_service_status
ss.each {|id,status| puts id, status.detect{|s| s[:type] == 'app' } } ; nil
ss.each {|id,status| puts id, status.detect{|s| s[:type] == 'app' }[:active_state] } ; nil
ss.each {|id,status| puts id, status.detect{|s| s[:type] == 'app' }[:machine_ip] } ; nil
Integrity.start_services
so = Integrity::Build.last
so = Integrity::Build.last.service_opts
so = Integrity::Build.last.service_options
Integrity.crust.destroy_build(so)
wtf?
so = Integrity::Build.last.service_options
Integrity.crust.destroy_build(so)
so = Integrity::Build.last.service_options
Integrity.crust.start_build(so)
unit_file
unit_file.lines.map(&:chomp).map{|line| line.empty? ? nil : line }.compact.slice_before(section_header_re).to_a.map{|section| [ section.first.match(section_header_re)[1], section.drop(1).map{|conf| conf.split('=', 2)}.to_h ] }.to_h
unit_file.lines.map(&:chomp).map{|line| line.empty? ? nil : line }.compact.slice_before(section_header_re).to_a
unit_file.lines.map(&:chomp).map{|line| line.empty? ? nil : line }.compact.slice_before(section_header_re).to_a.map{|section| [ section.first.match(section_header_re)[1], section.drop(1).map{|conf| conf.split('=', 2)}.to_h ] }
unit_file.lines.map(&:chomp).map{|line| line.empty? ? nil : line }.compact.slice_before(section_header_re).to_a.map{|section| [ section.first.match(section_header_re)[1], section.drop(1).map{|conf| conf.split('=', 2)} ] }
unit_file.lines.map(&:chomp).map{|line| line.empty? ? nil : line }.compact.slice_before(section_header_re).to_a.map{|section| [ section.first.match(section_header_re)[1], section.drop(1).map{|conf| name,val = conf.split('=', 2) ; [name, [val]]} ] }
ap unit_file.lines.map(&:chomp).map{|line| line.empty? ? nil : line }.compact.slice_before(section_header_re).to_a.map{|section| [ section.first.match(section_header_re)[1], section.drop(1).map{|conf| name,val = conf.split('=', 2) ; [name, [val]]} ] }, index: false
ap unit_file.lines.map(&:chomp).map{|line| line.empty? ? nil : line }.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1], section.drop(1).map{|conf| name,val = conf.split('=', 2) ; [name, [val]]}] }, index: false
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2);[name, [val]]}] }, index: false
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2);[name, [val]]}] }, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2);[name, [val]]}].to_h }, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2);[name, [val]].to_h}] }, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2);[name, val].to_h}] }, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2);[name, [val]]}] }, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2);[name, val]}] }, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2);[name, val].to_h}] }, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); [name, val].to_h }] }, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); [name, val].class }] }, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); collapse_values([name, val]) }] }, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); collapse_values.call([name, val]) }] }, index: false, indent: 2
collapse_values = lambda {|n,v| { name => v }  }
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); collapse_values.call([name, val]) }] }, index: false, indent: 2
collapse_values = lambda {|(n,v)| { name => v }  }
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); collapse_values.call([name, val]) }] }, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); [name, val] }] }, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); [name, val] }]}, index: false, indent: 2
xx = unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); [name, val] }]}
xx
yy = xx[1][1]
yy
yy.reduce({}){|hsh,val|   ; hsh }
yy.reduce({}){|hsh,val| puts val  ; hsh }
yy.reduce({}){|hsh,(k,v)| puts val  ; hsh }
yy.reduce({}){|hsh,(k,v)| puts v  ; hsh }
yy.reduce({}){|hsh,(k,v)| puts k,v  ; hsh }
yy.reduce({}){|hsh,(k,v)| puts k,v ; hsh[k] = hsh[k] ? hsh[k] << v : v ; hsh }
yy.reduce({}){|hsh,(k,v)| puts k,v ; hsh[k] = hsh[k] ? hsh[k] << v : [v] ; hsh }
yy.reduce({}){|hsh,(k,v)| hsh[k] = hsh[k] ? hsh[k] << v : [v] ; hsh }
yy.reduce({}){|hsh,(k,v)| hsh[k] = hsh[k] ? hsh[k] << v : [v] ; hsh }.to_a
ap yy.reduce({}){|hsh,(k,v)| hsh[k] = hsh[k] ? hsh[k] << v : [v] ; hsh }.to_a, index: false, indent: 2
ap yy.reduce({}){|hsh,(k,v)| hsh[k] = hsh[k] ? Array(hsh[k]) << v : v ; hsh }.to_a, index: false, indent: 2
ap yy.reduce({}){|hsh,(k,v)| hsh[k] = hsh[k] ? Array(hsh[k]) << v : v ; hsh }.to_a.to_h, index: false, indent: 2
xx = unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); [name, val] }.reduce({}){|hsh,(k,v)| hsh[k] = hsh[k] ? Array(hsh[k]) << v : v ; hsh }]}
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); [name, val] }.reduce({}){|hsh,(k,v)| hsh[k] = hsh[k] ? Array(hsh[k]) << v : v ; hsh }]}, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); [name, val] }.reduce({}){|hsh,(k,v)| hsh[k] = hsh[k] ? Array(hsh[k]) << v : v ; hsh }]}.to_h, index: false, indent: 2
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); [name, val] }.reduce({}){|hsh,(k,v)| hsh[k] = hsh[k] ? Array(hsh[k]) << v : v ; hsh }]}.to_h, index: false, indent: 6
ap unit_file.lines.map(&:chomp).map{|line|line.empty? ? nil : line}.compact.slice_before(section_header_re).to_a.map{|section| [section.first.match(section_header_re)[1],section.drop(1).map{|conf| name,val=conf.split('=',2); [name, val] }.reduce({}){|hsh,(k,v)| hsh[k] = hsh[k] ? Array(hsh[k]) << v : v ; hsh }]}.to_h, index: false, indent: 10
so = Integrity::Build.last.service_options
Integrity.crust.start_build(so)
xx
ap xx, indent: 10
Time.now.strftime
Time.now.strftime('%y')
Time.now.strftime('%Y')
Time.now.strftime('%Y%m')
Time.now.strftime('%Y%M')
Time.now.strftime('%Y%m%d')
Time.now.strftime('%Y%m%d%H')
Time.now.strftime('%Y%m%d%h')
Time.now.strftime('%Y%m%d-%H')
Time.now.strftime('%Y%m%d-%P')
Time.now.strftime('%Y%m%d-%H')
Time.now.strftime('%Y%m%d-%H%M')
Time.now.strftime('%Y%m%d-%H:%M')
Time.now.strftime('%Y%m%d-%H:%M:%s')
Time.now.strftime('%Y%m%d-%H:%M:%S')
Time.now.strftime('%Y%m%d-%H-%M-%S')
Time.now.strftime('%Y%m%d-%H%M%S')
Pancakes::Services.services
Pancakes::Services.all_names
Pancakes::Services.all_names - 'mailcatcher'
Pancakes::Services.all_names - ['mailcatcher']
xx = Pancakes::Stack.new_with_git_strings(jarvis: 'L-epic/1773564/expert-payment2', frick: 'L+master', tim: 'L+master', services: Pancakes::Services.all_names - ['mailcatcher'])
xx.save!
xx.bring_up
xx.id
xx.units
xx.bring_up
wtf?
xx.netmaster
xx.units.netmaster
xx.reload
xx.units.netmaster
xx.bring_up
xx = Pancakes::Stack.new_with_git_strings(jarvis: 'L-master', frick: 'L+master', tim: 'L+master', services: Pancakes::Services.all_names - ['mailcatcher'])
xx.save!
xx.reload
xx.bring_up
"  ".empty?
"  ".chomp.empty?
"  ".chomp!.empty?
"  ".chomp
"  ".chop
"  \n".chomp
Delayed::Job.last
Delayed::Job.first
xx = Pancakes::Stack.last
Pancakes::Stack.count
Pancakes::Stack.last
Pancakes::Stack.last.state!
Pancakes::Stack.last.all_services_up?
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
xx.bring_up
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx.bring_down
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx.bring_down
xx.bring_up
Delayed::Job.enqueue(Integrity::StatsJob.new)
Delayed::Job.all
Delayed::Job.delete_all
Delayed::Job.all
Delayed::Job.count
DB
DB.run('select * from integrity_builds limit 2')
DB.run('show tables')
DB
DB.schema
DB << 'select * from integ'
DB << 'select * from integrity_builds limit 10'
DB.get{server_version{}}
DB.get{1}
DB.schema(:integrity_builds)
Pancakes::Stack.new
Pancakes::Stack.all
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
desired_state
self.desired_state
exit-all
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
c
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
wtf?
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
services
n
me.services
n
me
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
n
me
services
n
me.units
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
c
Pancakes::Stack.count
Pancakes::Stack.destroy_all
Pancakes::Stack.destroy
Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
Pancakes::Stack.db
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
git_strings
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
git_strings
me.services
me.units
me.units.service_def
me.units.last.service_def
me.units.last.service_def.deep_symbolize_keys
Hashie::Mash.new me.units.last.service_def.deep_symbolize_keys
Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
wtf?
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
sd
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
c
git_strings
me.units
me.units.last.service_def
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
c
me.units.last.service_def
me.units.last.service_def.codebase
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
sd
n
c
me.units.last.service_def
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
git_strings
git_strings.keys.select{ |str|  }
git_strings.to_a.select{ |k,v| true }
git_strings.select{ |k,v| true }
me.services
cbs = me.services.map{|s| Services.parse_service_name(s) }
cbs = me.services.map{|s| Services.parse_service_name(s)[1] }
cbs = me.services.map{|s| Services.parse_service_name(s).last }
cbs = me.services.map{|s| Services.parse_service_name(s) }
cbs = me.services.map{|s| Services.parse_service_name(s) }.select{|s| s.size == \2 }
cbs = me.services.map{|s| Services.parse_service_name(s) }.select{|s| s.size == 2 }
cbs = me.services.map{|s| Services.parse_service_name(s) }.select{|s| s.size == 2 }.map(:last)
cbs = me.services.map{|s| Services.parse_service_name(s) }.select{|s| s.size == 2 }.map(&:last)
cbs = me.services.map{|s| Services.parse_service_name(s) }.select{|s| s.size == 2 }.map(&:first)
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
git_strings
codebases
help
exit-program
q
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
git_strings
q
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
git_strings
git_strings.select! {|cb| codebases.include?(cb.to_s) }
git_strings
Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
xx.bring_up
xx = Pancakes::Stack.last
xx
xx.bring_up
xx
xx.units.filter(netmaster: true)
xx = Pancakes::Stack.last
xx = Pancakes::Stack.last.units.last
xx = Pancakes::Stack.last.units.last.service_def
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
xx
xx = Pancakes::Stack.last
xx.units.where
Pancakes::Unit.where(stack_id: xx.id)
Pancakes::Unit.where(stack_id: xx.id, service_name: Services.netmaster)
Pancakes::Unit.where(stack_id: xx.id, service_name: Pancakes::Services.netmaster)
Pancakes::Unit.where(stack_id: xx.id, service_name: Pancakes::Services.netmaster).first
Pancakes::Unit.where(stack_id: xx.id+1, service_name: Pancakes::Services.netmaster).first
Pancakes::Unit.where(stack_id: xx.id).exclude(ping_url: nil)
Pancakes::Unit.where(stack_id: xx.id).exclude(ping_url: nil).first
xx = Pancakes::Stack.last
xx.pingable_units
xx = Pancakes::Stack.last
xx.pingable_units
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
xx
xx.pingable_units
xx = Pancakes::Stack.last
xx.pingable_units
xx = Pancakes::Stack.last
xx.non_netmaster_units
xx = Pancakes::Stack.last
xx.non_netmaster_units
xx.netmaster_unit
xx.pingable_units
xx.unit_with_service_name(:mysql)
xx.commits.dataset
xx.commits_dataset
Pancakes::Stack.all
Pancakes::Stack.all.eager
Pancakes::Stack.eager
Pancakes::Stack.eager(:units)
Pancakes::Stack.eager(:units).all
Pancakes::Stack.eager(:units).all.to_json
Pancakes::Stack.eager(:units).all.as_json
Pancakes::Stack.eager(:units).all.as_json(include: :units)
Pancakes::Stack.eager(:units).all.to_json(include: :units)
Pancakes::Stack.eager(:units).all.to_json(include: [:units])
puts Pancakes::Stack.eager(:units).all.to_json(include: [:units])
puts Pancakes::Stack.eager(:units).all.as_json(include: [:units])
Pancakes::Stack.eager(:units).all.as_json(include: [:units])
Pancakes::Stack.first.eager(:units).all.as_json(include: [:units])
Pancakes::Stack.eager.all.as_json(include: [:units])
Pancakes::Stack.first.as_json
Pancakes::Stack.first.as_json(include: :units)
Pancakes::Stack.first.as_json(include: :commits)
Pancakes::Stack.eager.first.as_json(include: :commits)
Pancakes::Stack.eager(:commits).first.as_json(include: :commits)
Pancakes::Stack.eager(:commits).first
Pancakes::Stack.eager(:commits)
Pancakes::Stack.eager(:commits).first
Pancakes::Stack.eager(:commits).first.units
Pancakes::Stack.eager(:commits).first.units.as_json
Pancakes::Stack.eager(:commits).first.units
Pancakes::Stack.first.units
Pancakes::Stack.first.as_json
Pancakes::Stack.first.as_json(include: :units)
Pancakes::Stack.first.as_json(include: :unit)
Pancakes::Stack.first.as_json(include: :unitasdf)
Pancakes::Stack.first.as_json(include: :unit)
require 'json'
Pancakes::Stack.first.as_json(include: :unit)
Pancakes::Stack.first.as_json(include: :unit, except: :sha)
Pancakes::Stack.first.as_json(except: :sha)
Pancakes::Stack.first.as_json(except: [:sha])
Pancakes::Stack.first.to_json(except: [:sha])
puts Pancakes::Stack.first.to_json(except: [:sha])
JSON.load Pancakes::Stack.first.to_json(except: [:sha])
JSON.load Pancakes::Stack.first.to_json(except: [:sha], include: [:units])
JSON.load Pancakes::Stack.first.to_json(except: [:sha], include: [:units, :commits])
JSON.load Pancakes::Stack.to_json(except: [:sha], include: [:units, :commits])
Pancakes::Stack
Pancakes::Stack.all.map(&:commits)
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server jarvis-server tim-server])
Pancakes::Stack.all.map(&:commits)
Pancakes::Commit.where(branch: 'master')
Pancakes::Commit.where(branch: 'master').group(:stack_id)
Pancakes::Commit.select(:stack_id).where(branch: 'master').group(:stack_id)
Pancakes::Commit.select(:stack_id).where(branch: 'master').group(:stack_id).having{count(:*) = n}
Pancakes::Commit.select(:stack_id).where(branch: 'master').group(:stack_id).having{count(:*) == n}
branches = {repo: 'consultingmd/consultingmd', branch: 'master'}
Pancakes::Commit.select(:stack_id).where(branch: 'master').group(:stack_id).having{count(:*) == branches.size}
branches.size
Pancakes::Commit.select(:stack_id).where(branch: 'master').group(:stack_id).having{count(:*) == 2}
Pancakes::Commit.select(:stack_id).where(branch: 'master').group(:stack_id).having{count(:*){} == 2}
Pancakes::Commit.select(:stack_id).where(**branches).group(:stack_id).having{count(:*){} == 2}
Pancakes::Commit.select(:stack_id).where(**branches).group(:stack_id).having{count(:stack_id){} == 2}
Pancakes::Commit.select(:stack_id).where(**branches).group_and_count(:stack_id).having{count(:stack_id){} == 2}
Pancakes::Commit.select(:stack_id).where(**branches).group(:stack_id).having{count(:stack_id){} == 2}
Pancakes::Commit.select(:stack_id).where(**branches).group(:stack_id).having("count(stack_id) == 1")
Pancakes::Commit.select(:stack_id).where(**branches).group(:stack_id).having("count(stack_id) = 1")
branches
branches.size
branches = [branches]
branches.size
Pancakes::Commit.select(:stack_id).where(**branches).group(:stack_id).having("count(stack_id) = ?", branches.size)
Pancakes::Commit.select(:stack_id).where(branches).group(:stack_id).having("count(stack_id) = ?", branches.size)
Pancakes::Commit.select(:stack_id).where(branches).where(branches).group(:stack_id).having("count(stack_id) = ?", branches.size)
Pancakes::Commit.select(:stack_id).where(branches).where(branches).invert.group(:stack_id).having("count(stack_id) = ?", branches.size)
Pancakes::Commit.select(:stack_id).where(branches).where(branches).group(:stack_id).having("count(stack_id) = ?", branches.size)
Pancakes::Commit.select(:stack_id).where(branches).where(branches).group(:stack_id).having{count(:stack_id){} =~ branches.size }
Pancakes::Commit.select(:stack_id).where(branches).where(branches).group(:stack_id).having{count(:*){} =~ branches.size }
Pancakes::Commit.select(:stack_id).where(branches).where(branches).group(:stack_id).having{count('stack_id'){} =~ branches.size }
Pancakes::Commit.select(:stack_id).where(branches).where(branches).group(:stack_id).having{count(stack_id){} =~ branches.size }
Pancakes::Commit.select(:stack_id).where(branches).where(branches).group(:stack_id).having{count(){stack_id} =~ branches.size }
Pancakes::Commit.select(:stack_id).where(branches).where(branches).group(:stack_id).having{count(){:stack_id} =~ branches.size }
Pancakes::Commit.select(:stack_id).where(branches).where(branches).group(:stack_id).having{count{:stack_id} =~ branches.size }
Pancakes::Commit.select(:stack_id).where(branches).where(branches).group(:stack_id).having{count{} =~ branches.size }
Pancakes::Commit.select(:stack_id).where(branches).where(branches).group(:stack_id).having{count(:stack_id) =~ branches.size }
Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size }
Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size }.sql
Pancakes::Stack.where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } )
Pancakes::Stack.where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } ).sql
Pancakes::Stack.eager.where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } ).sql
Pancakes::Stack.eager.where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } )
Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } )
Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } ).first
Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } ).all
Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } ).first
Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } ).first.to_json(include: [:units, :commits])
JSON.parse Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } ).first.to_json(include: [:units, :commits])
Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } ).all
Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } )
Pancakes::Stack.eager(:units, :commits).or(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } )
Pancakes::Stack.eager(:units, :commits).or(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } ).sql
Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } ).sql
{x: 1, y: 2}.take(1)
{x: 1, y: 2}.drop(1)
Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches.take(1)).or(branches).group(:stack_id).having{count(:stack_id) =~ branches.size } ).sql
Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches.take(1)).or(branches.drop(1)).group(:stack_id).having{count(:stack_id) =~ branches.size } ).sql
branches.take(1)
branches.take(1).to_a
branches.take(1).first.to_a
Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches.take(1).to_a).or(branches.drop(1)).group(:stack_id).having{count(:stack_id) =~ branches.size } ).sql
Pancakes::Stack.eager(:units, :commits).where(id: Pancakes::Commit.select(:stack_id).where(branches.take(1).to_a).or(branches.drop(1).to_a).group(:stack_id).having{count(:stack_id) =~ branches.size } ).sql
[:x].drop(1)
Pancakes::Stack.with_branches jarvis: 'mastster'
Pancakes::Stack.with_branches jarvis: 'master'
Pancakes::Stack.with_branches frick: 'master'
Pancakes::Stack.with_branches(frick: 'master').all
Pancakes::Stack.with_branches(frick: 'master').eager(:units).all
Pancakes::Stack.with_branches(frick: 'master').all.to_a
Pancakes::Stack.with_branches(frick: 'master')
Pancakes::Stack.with_branches(frick: 'master').all
Pancakes::Stack.with_branches(frick: 'master')
Pancakes::Stack.with_branches(frick: 'master').all
Pancakes::Stack.with_branches(frick: 'master')
Pancakes::Stack.with_branches(frick: 'master', jarvis: 'master')
Pancakes::Stack.with_branches(frick: 'master', jarvis: 'master', tim: 'master')
Pancakes::Stack.last.commits
Pancakes::Stack.with_branches(frick: 'master', jarvis: 'master', tim: 'master')
Pancakes::Stack.with_branches(frick: 'master', jarvis: 'master', tim: 'master').sql
Pancakes::Stack.with_branches(frick: 'master', jarvis: 'master', tim: 'master')
:x
Pancakes::Stack.dataset.destroy
Pancakes::Stack.count
Pancakes::Commit.count
Pancakes::Unit.count
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
xx.bring_up
xx.units.first
xx.units.first.name
Pancakes::Stack.all
xx =Pancakes::Stack.last
xx = Pancakes::Stack.last
xx.restart_with_same_commits
wtf?
xx = Pancakes::Stack.last
xx.restart_with_same_commits
wtf?
xx.units
xx.units.cluster_info
xx.units.last.cluster_info
uu = xx.units.last
uu
uu.cluster_info
uu.cluster_info = nil
xx = Pancakes::Stack.last
xx.restart_with_same_commits
uu = xx.units.last
uu.cluster_info = {}
Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
uu = xx.units.last
uu.cluster_info
xx.restart_with_same_commits
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx.state!
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx.state!
wtf?
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx = Pancakes::Stack.last
xx.restart_with_same_commits
wtf?
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx = Pancakes::Stack.last
xx.restart_with_same_commits
nm
nm.refresh
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx = Pancakes::Stack.last
xx.restart_with_same_commits
xx.restart_with_branch_heads
xx.restart_with_same_commits
xx = Pancakes::Stack.last
xx.restart_with_branch_heads
stack
codebase
stack.commit_for_codebase(codebase)
stack.commits
xx.restart_with_branch_heads
xx = Pancakes::Stack.last
xx.restart_with_branch_heads
commits
self
self.commits
remove_all_commits
xx = Pancakes::Stack.last
xx.restart_with_branch_heads
remove_all_commits
Pancakes::Commit.count
Pancakes::Stack.count
Pancakes::Stack.first
Pancakes::Stack.first.commits
xx = Pancakes::Stack.last
xx.delete
Pancakes::Stack.count
Pancakes::Unit.count
Pancakes::Commit.count
Pancakes::Commit.delete_all
Pancakes::Commit.all.delete_all
Pancakes::Commit.all.delete
Pancakes::Commit.destroy_all
Pancakes::Commit.destroy
Pancakes::Commit.all
Pancakes::Commit.all.destroy
Pancakes::Commit.all.destroy_all
Pancakes::Commit.destroy_all
Pancakes::Commit.dataset.destroy
Pancakes::Unit.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
xx.commits
xx.send(:refresh_commits_to_head)
commits
c
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
xx.send(:refresh_commits_to_head)
c
Pancakes::Commit.dataset.destroy
Pancakes::Unit.dataset.destroy
Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
xx.send(:refresh_commits_to_head)
xx.save
xx
Pancakes::Commit.get_model_fields_from_git_string('consultingmd/tim', 'master')
sha
ci[:sha]
branch
Pancakes::Commit.get_model_fields_from_git_string('consultingmd/tim', 'master')
sha
c
Pancakes::Commit.get_model_fields_from_git_string('consultingmd/tim', 'master')
Pancakes::Commit.get_model_fields_from_git_string('consultingmd/tim', 'L+master')
Pancakes::Commit.get_model_fields_from_git_string('consultingmd/tim', 'L-master')
Pancakes::Commit.get_model_fields_from_git_string('consultingmd/tim', '174603eeb7d1ccd11d44047e73f9fc55efbd7403:L-master')
Pancakes::Commit.get_model_fields_from_git_string('consultingmd/tim', '174603eeb7d1ccd11d44047e73f9fc55efbd7403:L+master')
Pancakes::Commit.get_model_fields_from_git_string('consultingmd/tim', '174603eeb7d1ccd11d44047e73f9fc55efbd7403')
Pancakes::Stack.dataset.destroy
Pancakes::Commit.dataset.destroy
Pancakes::Unit.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: %w[mysql frick-server])
xx.commits
xx.units
xx.commits.first
xx.commits.first.refresh_myself
xx.commits.first.date
xx.commits.first.date.class
xx = Pancakes::Stack.last
xx.comits
xx.commits
xx.commits.first.toggle_lock
xx.commits.first.refresh_myself
xx = Pancakes::Stack.last
xx.commits.first.refresh_myself
xx = Pancakes::Stack.last
xx.commits.first.refresh_myself
xx = Pancakes::Stack.last
xx.restart_with_branch_heads
unit
stack
stack.commit_for_codebase(codebase)
xx.state!
xx.running?
so = Integrity::Build.last.service_options
Integrity.crust.start_build(so)
unit_file
puts unit_file
xx
[:x, :y].take(1)
[:x, :y].drop(1)
[:x, :y].take(2)
[:x, :y].take(1)
[:x, :y][0]
[:x, :y].take(1)
Dir.mkdir
Dir.mkdir('/tmp/payloads') unless File.directory?('/tmp/payloads')
Pancakes::Stack.where(sha: 'add919de1787c972d22477bf6155b423a9daa62d')
Pancakes::Stack.first(sha: 'add919de1787c972d22477bf6155b423a9daa62d')
xx = Pancakes::Stack.first(sha: 'add919de1787c972d22477bf6155b423a9daa62d')
xx.commits_dataset.first(repo: 'consultingmd/frick')
xx = Pancakes::Stack.find(1)
xx = Pancakes::Stack[1]
xx = Pancakes::Stack[2]
xx = Pancakes::Stack[3]
xx = Pancakes::Stack[11]
params
params[:branches]
params[:branches][:jarvis]
params[:branches]['jarvis']
c
"".split(',')
pocs
c
me.set_state(:off)
me.save
"asdf".capitalize
xx = Pancakes::Stack.first
xx.bring_up
xx.state!
xx.bring_up
requires_unit
c
requires_unit
c
requires_unit
c
requires_unit
c
requires_unit
c
requires_unit
c
requires_unit
c
requires_unit
c
requires_unit
c
xx
xx.bring_down
xx = Pancakes::Stack.size
Pancakes::Stack
Pancakes::Stack.count
Pancakes::Stack.size
xx = Pancakes::Stack.first_with_branches(jarvis: 'master')
xx.sha
xx = Hashie::Mash.new
xx.hi = 'hi'
xx.hi
xx.hi.hi.foo = 'hi'
xx = Hashie::Clash.new
xx.hi.hi.foo = 'hi'
xx = Hashie::Mash.new(foo: :bar)
xx.foo
Pancakes.logger
Pancakes.logger.debug 'hi'
Pancakes.logger.error 'hi'
Pancakes.logger
c
Pancakes.logger.appenders
logger.info 'hi
'
logger.info 'hi'
logger.appenders
Logging.logger.root.appenders
logger.info 'hi'
Pancakes::Stack.all
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master')
(pocs || "").split(',').map(&:strip)
(pocs || "").split(',')
(pocs || "")
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master')
xx.bring_up
xx.restart_with_branch_heads
xx = Pancakes::Stack[19]
xx.restart_with_branch_heads
xx.bring_down
xx.restart_with_branch_heads
xx.human_state!
xx = Pancakes::Stack[19]
xx.only_units(:mysql)
names
self.units.select{|unit| names.include?(unit.name) }
xx = Pancakes::Stack[19]
xx.only_units(:mysql)
xx.only_units('mysql')
names
xx = Pancakes::Stack[19]
xx.only_units('mysql')
names
self.units.select{|unit| names.include?(unit.name) }
self.units.select{|unit| names.include?(unit.service_name) }
self.units.map(&:service_name)
self
self.units_dataset.where(state: 'running')
self.units_dataset.first
self.units_dataset.where(state: 'active')
self.units_dataset.where(state: 'active').map(&:service_name)
xx = Pancakes::Stack[19]
xx.restart_with_branch_heads
xx.running_unit_names
xx.running_unit_names == ["mysql]
xx.running_unit_names == ["mysql"]
xx.running_unit_names == ["mysql"].to_set
xx.running_unit_names.to_set == ["mysql"].to_set
xx = Pancakes::Stack[19]
xx.restart_with_branch_heads
xx = Pancakes::Stack[19]
xx.restart_with_branch_heads
xx.bring_down
['mysql']
['mysql'].to_set
puts ['mysql'].to_set
puts ['mysql'].to_set.inspect
xx = Pancakes::Stack[19]
uu = xx.units.last
Pancakes::Fleet::Templating.gen_service_file_name(uu)
Pancakes::Fleet::Templating.detect_and_make_template(uu)
xx = Pancakes::Stack[19] ; uu = xx.units.last
Pancakes::Fleet::Templating.detect_and_make_template(uu)
xx = Pancakes::Stack[19] ; uu = xx.units.last
Pancakes::Fleet::Templating.detect_and_make_template(uu)
ap Pancakes::Fleet::Templating.detect_and_make_template(uu), index: false
ap Pancakes::Fleet::Templating.detect_and_make_template(uu), index: false, indent: 10
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('mysql')), index: false
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('mysql')), index: false, indent: 10
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
partials_list
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
c
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
c
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
c
{x: 1}.merge(nil)
{x: 1}.merge(nil || {})
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
xx.units.last.name
xx.units.last.service_name
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-orchestra')), index: false, indent: 10
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('jarvis-server')), index: false, indent: 10
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('frick-server')), index: false, indent: 10
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('mailcatcher')), index: false, indent: 10
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('mailcatcher')), index: false, indent: 10
xx = Pancakes::Stack[19]
xx.restart_with_branch_heads
xx = Pancakes::Stack[19]
xx.restart_with_branch_heads
xx = Pancakes::Stack[19]
xx.restart_with_branch_heads
xx = Pancakes::Stack[19]
xx.restart_with_branch_heads
xx = Pancakes::Stack[19]
xx.add_freezer_unit
xx = Pancakes::Stack[19]
xx.add_freezer_unit
xx.remove_freezer_unit
xx.unit_names
xx.all_unit_names
xx.add_freezer_unit
xx.all_unit_names
xx.remove_freezer_unit
xx.all_unit_names
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('freezer')), index: false, indent: 10
ff = xx.units.last
ff.name
ff.serivice_name
ff.service_name
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.unit_with_service_name('freezer')), index: false, indent: 10
xx.units
xx.unit_with_service_name('freezer')
xx.reload
xx.unit_with_service_name('freezer')
xx.units
xx.add_freezer_unit
xx.units\
xx.units
xx.reload
xx.units
Pancakes::Unit.all
xx = Pancakes::Stack[19]
xx.freezer
xx = Pancakes::Stack[19]
xx.freezer_unit
Pancakes::Unit.all
Pancakes::Unit[97]
Pancakes::Unit[97].delete
Pancakes::Unit.all
xx = Pancakes::Stack[19]
xx.freezer_unit
xx.add_freezer_unit
xx = Pancakes::Stack[19]
xx.add_freezer_unit
xx.remove_freezer_unit
Pancakes::Unit.all
xx.add_freezer_unit
Pancakes::Unit.all
xx.add_freezer_unit
xx = Pancakes::Stack[19]
xx.freezer_unit
Pancakes::Unit.all
xx.remove_freezer_unit
Pancakes::Unit.all
xx.freezer_unit
Pancakes::Unit.all
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.detect_and_make_template(xx.freezer_unit), index: false, indent: 10
[:a, :b, *[1, 2]]
xx
xx.units.first
xx.units.first.machine_ip
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.freezer_template(xx.freezer_unit), index: false, indent: 10
ap Pancakes::Fleet::Templating.freezer_template(freezer_unit: xx.freezer_unit, mysql_unit: xx.unit_with_service_name('mysql')), index: false, indent: 10
ap Pancakes::Fleet::Templating.freezer_template(freezer_unit: xx.freezer_unit, mysql_unit: xx.unit_with_service_name('mysql'), mysql_dbs: ['foo', 'bar']), index: false, indent: 10
xx = Pancakes::Stack[19]
ap Pancakes::Fleet::Templating.freezer_template(freezer_unit: xx.freezer_unit, mysql_unit: xx.unit_with_service_name('mysql'), mysql_dbs: ['foo', 'bar']), index: false, indent: 10
ap Pancakes::Fleet::Templating.freezer_template(freezer_unit: xx.freezer_unit, mysql_unit: xx.unit_with_service_name('mysql'), mysql_dbs: ['foo', 'bar'])
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master')
xx = Pancakes::Stack[20]
xx.units.last
xx.units.last.dbs
xx.my_dbs
xx = Pancakes::Stack[20]
xx.my_dbs
xx = Pancakes::Stack[20]
xx.ports
xx = Pancakes::Stack[20]
xx.ports
xx = Pancakes::Stack[20]
temp = Templating.freezer_template( freezer_unit: freezer_unit, mysql_unit:   mysql_unit, mysql_dbs:    dbs,)
temp = Pancakes::Fleet::Templating.freezer_template(freezer_unit: xx.freezer_unit, mysql_unit: xx.mysql_unit, mysql_dbs: xx.dbs)
xx = Pancakes::Stack[20]
temp = Pancakes::Fleet::Templating.freezer_template(freezer_unit: xx.freezer_unit, mysql_unit: xx.mysql_unit, mysql_dbs: xx.dbs)
xx = Pancakes::Stack[20]
temp = Pancakes::Fleet::Templating.freezer_template(freezer_unit: xx.freezer_unit, mysql_unit: xx.mysql_unit, mysql_dbs: xx.dbs)
Pancakes.cluser.start_template(xx.freezer_unit.service_file, temp)
Pancakes.cluster.start_template(xx.freezer_unit.service_file, temp)
Pancakes.cluster.start_template(xx.freezer_unit.service_file_name, temp)
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: ['mysql', 'frick-server'])
xx.bring_up
xx.bring_down
xx = Pancakes::Stack[19]
xx.bring_down
xx.destroy
xx
xx.reload
xx
xx = Pancakes::Stack[21]
xx.bring_down
xx = Pancakes::Stack[21]
xx.bring_up
xx.bring_down
self.mysql_unit
xx = Pancakes::Stack[21]
xx.bring_up
xx.bring_down
c
wtf?
help
wtf?
xx = Pancakes::Stack[21]
xx.bring_up
c
xx = Pancakes::Stack[21]
xx.bring_down
self.units
self.units.reload
self.units
self.units.all
self.units_dataset.all
self.units.refresh
c
xx = Pancakes::Stack[21]
xx.bring_up
xx.refresh
xx.bring_down
c
xx = Pancakes::Stack[21]
xx.bring_up
xx.bring_down
xx = Pancakes::Stack[21]
s3 = Aws::S3::Client.new
ENV
s3 = Aws::S3::Client.new(credentials: Aws::Credentials.new(ENV['DEV_AWS_ACCESS_KEY_ID'], ENV['DEV_AWS_SECRET_ACCESS_KEY']),)
s3.list_buckets
s3.get_object(key: "ObjectKey", bucket: 'grnds-development-coreos', reponse_target: '/freezer')
s3.get_object(key: "ObjectKey", bucket: 'grnds-development-coreos', response_target: '/freezer')
s3.get_object(key: "/freezer", bucket: 'grnds-development-coreos')
s3.get_object(key: "/freezer/frozen_stack_21.sql.gz", bucket: 'grnds-development-coreos')
s3.get_object(key: "freezer/frozen_stack_21.sql.gz", bucket: 'grnds-development-coreos')
s3.get_head(key: "freezer/frozen_stack_21.sql.gz", bucket: 'grnds-development-coreos')
s3.head_object(key: "freezer/frozen_stack_21.sql.gz", bucket: 'grnds-development-coreos')
s3.head_object(key: "xfreezer/frozen_stack_21.sql.gz", bucket: 'grnds-development-coreos')
xx = Pancakes::Stack[21]
Pancakes::Freezer.stack_exists?(xx)
xx = Pancakes::Stack[21]
xx.restart_with_same_commits
xx = Pancakes::Stack[21]
xx.restart_with_same_commits
xx = Pancakes::Stack[21]
xx.restart_with_same_commits
xx = Pancakes::Stack[21]
xx.bring_down
xx.bring_down(force: true)
xx = Pancakes::Stack[21]
xx.bring_up(thaw_dbs: true)
xx = Pancakes::Stack[21]
s3 = Aws::S3::Client.new(credentials: Aws::Credentials.new(ENV['DEV_AWS_ACCESS_KEY_ID'], ENV['DEV_AWS_SECRET_ACCESS_KEY']),)
s3.delete_object(key: "freezer/frozen_stack_21.sql.gz", bucket: 'grnds-development-coreos')
s3.delete_object(key: "freezer/frozen_stack_21.sql.gzxxxxxxxx", bucket: 'grnds-development-coreos')
xx = Pancakes::Stack[21]
xx.restart_with_branch_heads
xx = Pancakes::Stack[21]
xx.state!
xx.run_freezer
ENV
ENV.grep(/FLEET/)
ENV.to_a.grep(/FLEET/)
ENV.to_a.map(&:first)grep(/FLEET/)
ENV.to_a.map(&:first).grep(/FLEET/)
ENV['FLEETCTL_ENDPOINT']
xxx = ENV['FLEETCTL_ENDPOINT']
URI.parse(xxx)
URI.parse(xxx)[:host]
URI.parse(xxx).host
@client
@client.fleet_api_url
URI.parse(@client.fleet_api_url).host
Pancakes.cluster.etcd
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.get_unit_state(xx.units.first.service_file_name)
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.get_unit_state(xx.units.first.service_file_name)
service_fname
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.get_unit_state(xx.units.first.service_file_name)
c
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.get_unit_state(xx.units.first.service_file_name)
Pancakes.cluster.etcd.get_unit_active_state(xx.units.first.service_file_name)
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.get_unit_active_state(xx.units.first.service_file_name)
resp
resp.index
resp
resp.node
resp.node.modified_index
resp.node.modifiedIndex
node
resp
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.get_unit_active_state(xx.units.first.service_file_name)
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.get_unit_active_state(xx.units.first.service_file_name)
Pancakes.cluster.etcd.watch_unit(xx.units.first.service_file_name)
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
states_resp
states_resp.node
states_resp.node.etcd_index
states_resp.etcd_index
c
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
Pancakes.cluster.etcd.watch_states
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.watch_states
resp
Pancakes.cluster.etcd.watch_states
resp
resp.key
Pancakes.cluster.etcd.watch_states
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.watch_unit(xx.freezer_unit)
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.watch_unit(xx.freezer_unit)
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.watch_unit(xx.freezer_unit)
Pancakes.cluster.etcd.watch_unit(xx.freezer_unit.service_fname)
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.watch_unit(xx.freezer_unit.service_fname)
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.watch_unit(xx.freezer_unit.service_fname)
xx = Pancakes::Stack[21]
Pancakes.cluster.etcd.watch_unit(xx.freezer_unit.service_fname)
Time.now
then = Time.now
x = Time.now
Time.now - x
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
wtf?
help
$!
$@
xx.run_freezer
Integrity.base_url
RConfig.application
Pancakes.config[:freezer]
Pancakes.config
[].all?
[nil, :foo].all?
[:nil, :foo].all?
[:foo, :bar].reduce({foo: {bar: 'baz'}}){|memo,key| memo[key] }
[:foo, :barx].reduce({foo: {bar: 'baz'}}){|memo,key| memo[key] }
[:foox, :bar].reduce({foo: {bar: 'baz'}}){|memo,key| memo[key] }
[:foox, :bar].reduce({foo: {bar: 'baz'}}){|memo,key| memo.has_key?(key) && memo[key] }
[:foox, :bar].reduce({foo: {bar: 'baz'}}){|memo,key| memo && memo[key] }
[:foo, :bar].reduce({foo: {bar: 'baz'}}){|memo,key| memo && memo[key] }
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
Pancakes.config
Pancakes.config.freezer.namespace
xx = Pancakes::Stack[21]
xx.run_freezer
"freezer/#{Pancakes.config.freezer.namespace}/frozen_stack_#{stack_id}.sql.gz"
c
xx.run_freezer
c
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
c
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx = Pancakes::Stack.all
xx = Pancakes::Stack[21]
xx.started_at
xx = Pancakes::Stack[21]
xx.run_freezer
xx = Pancakes::Stack[21]
xx.state
Pancakes::Stack.all
xx = Pancakes::Stack.all
xx = Pancakes::Stack.all.first
xx
xx.units
xx = Pancakes::Stack.all
raise
$!
$@
x = Error.new
x
x = StandardError.new
x
x.backtrace
x.backtracex
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: ['mysql', 'mailcatcher', 'frick-server'])
xx.bring_up
unit
c
unit
unit.service_name
%w[mysql mailcatcher].include?(unit.service_name)
c
unit
c
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master', frick: 'L+master', tim: 'L+master', services: ['mysql', 'mailcatcher', 'frick-server'])
xx.units
xx.bring_up
c
xx = Pancakes::Stack.all
xx = Pancakes::Stack.first
xx.units
xx.units[1]
xx.units[2]
xx.non_netmaster_units
xx.non_netmaster_units.first
xx.non_netmaster_units[1]
xx.non_netmaster_units[2]
xx.non_netmaster_units[0]
xx.non_netmaster_units
xx.non_netmaster_units.to_a
xx.non_netmaster_units.to_a[1]
xx.non_netmaster_units.to_a[0]
xx.non_netmaster_units.to_a[1]
xx.non_netmaster_units.to_a[1].commit
xx.non_netmaster_units.to_a[2].commit
xx.non_netmaster_units.to_a[6].commit
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master')
git_strings
services
services.select{|x| x['-'] }
services.map{|x| x.split('-') }
services.map{|x| x.split('-') }.select{|x| x.size == 2 }
services.map{|x| x.split('-') }.select{|x| x.size == 2 }.map(&:first)
services.map{|x| x.split('-') }.select{|x| x.size == 2 }.map(&:first).to_set
git_strings
services.map{|x| x.split('-') }.select{|x| x.size == 2 }.map(&:first).map(&:to_sym).to_set
xx = services.map{|x| x.split('-') }.select{|x| x.size == 2 }.map(&:first).map(&:to_sym).to_set
xx
xx - git_strings.keys
mm = xx - git_strings.keys
mm
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master')
xx.commits
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master')
git_strings
xx.commits
xx = Pancakes::Stack[49]
xx.commits
xx = Pancakes::Stack.create_with_git_strings('jarvis' => 'L+master')
xx = Pancakes::Stack.create_with_git_strings(branches: {'jarvis' => 'L+master'})
Autoscaling.ec2
require 'nokogiri'
require 'open-uri'
doc = Nokogiri::HTML(open('https://coreos.com/docs/running-coreos/cloud-providers/ec2/#stable'))
doc
doc.css('#stable')
doc.css('#stable tbody')
doc.css('#stable tbody').children
doc.css('#stable tbody a')each {||}
doc.css('#stable tbody a')
doc.css('#stable tbody a[href=]')
doc.css('#stable tbody a[href*="region=us-east-1"]')
doc.css('#stable tbody a[href*="region=us-east-1"]').size
doc.css('#stable tbody a[href*="region=us-east-1"]')
ap doc.css('#stable tbody a[href*="region=us-east-1"]')
pp doc.css('#stable tbody a[href*="region=us-east-1"]')
pp doc.css('#stable tbody a[href*="region=us-east-1"]').to_h
pp doc.css('#stable tbody a[href*="region=us-east-1"]').to_hash
pp doc.css('#stable tbody a[href*="region=us-east-1"]')
doc.css('#stable tbody a[href*="region=us-east-1"]')
doc.css('#stable tbody tr')
doc.css('#stable tbody tr').select{|tr| tr.text }
doc.css('#stable tbody tr').select{|tr| tr.text =~ /HVM/ }
doc.css('#stable tbody tr').select{|tr| tr.text =~ /HVM/ && tr.text =~ /region=us-east-1/ }
doc.css('#stable tbody tr').select{|tr| puts tr.text ; tr.text =~ /HVM/ && tr.text =~ /region=us-east-1/ }
doc.css('#stable tbody tr').select{|tr| puts tr.text ; tr.text =~ /HVM/ && tr.text =~ /us-east-1/ }
doc.css('#stable tbody tr').select{|tr| tr.text =~ /HVM/ && tr.text =~ /us-east-1/ }
doc.css('#stable tbody tr').slice_before{|tr| tr.text =~ /us-east-1/ }
doc.css('#stable tbody tr').slice_before{|tr| tr.text =~ /us-east-1/ }.to_a
doc.css('#stable tbody tr').slice_after{|tr| tr.text =~ /us-east-1/ }.last.first
doc.css('#stable tbody tr').to_a.slice_after{|tr| tr.text =~ /us-east-1/ }.last.first
VERSION
__version__
doc.css('#stable tbody tr').slice_before{|tr| tr.text =~ /us-east-1/ }
doc.css('#stable tbody tr').slice_after{|tr| tr.text =~ /us-east-1/ }
doc.css('#stable tbody tr').slice_before{|tr| tr.text =~ /us-east-1/ }
doc.css('#stable tbody tr').slice_before{|tr| tr.text =~ /us-east-1/ }.last
doc.css('#stable tbody tr').slice_before{|tr| tr.text =~ /us-east-1/ }.to_a
doc.css('#stable tbody tr').slice_before{|tr| tr.text =~ /us-east-1/ }.to_a.last
doc.css('#stable tbody tr').slice_before{|tr| tr.text =~ /us-east-1/ }.to_a.last.first
doc.css('#stable tbody tr').slice_before{|tr| tr.text =~ /us-east-1/ }.to_a.last.first.text
doc.css('#stable tbody tr').slice_before{|tr| tr.text =~ /us-east-1/ }.to_a.last[1].text
doc.css('#stable tbody tr').drop_while{|tr| !tr.text.match(/us-east-1/) }
doc.css('#stable tbody tr').drop_while{|tr| !tr.text.match(/us-east-1/) }[1]
doc.css('#stable tbody tr').drop_while{|tr| !tr.text.match(/us-east-1/) }[1].text
doc.css('#stable tbody tr').drop_while{|tr| !tr.text.match(/us-east-1/) }[1]
doc.css('#stable tbody tr').drop_while{|tr| !tr.text.match(/us-east-1/) }[1].css
doc.css('#stable tbody tr').drop_while{|tr| !tr.text.match(/us-east-1/) }[1].css('a')
doc.css('#stable tbody tr').drop_while{|tr| !tr.text.match(/us-east-1/) }[1].css('a').text
doc.css('#stable tbody tr').drop_while{|tr| !tr.text.match(/us-east-1/) }[1].css('a:last-child').text
doc.css('#stable tbody tr').drop_while{|tr| !tr.text.match(/us-east-1/) }[1].css('a').last.text
doc.css('#stable tbody tr').drop_while{|tr| !tr.text.match(/us-east-1/) }[1].css('a')[1].text
doc.css('#stable tbody tr').drop_while{|tr| !tr.text['us-east-1'] }[1].css('a')[1].text
Autoscaling.instances
Autoscaling.instances.to_a
Autoscaling.latest_coreos_ami
Pancakes.cluster.cluster_name
Pancakes.cluster.name
Autoscaling.worker_cloud_config(num: 3)
xx = Autoscaling.worker_cloud_config(num: 3)
xx.to_yaml
puts xx.to_yaml
xx = Autoscaling.worker_cloud_config(num: 3)
puts xx.to_yaml
xx = Autoscaling.worker_cloud_config(num: 3)
puts xx.to_yaml
Pancakes.cluster.list_machines
Pancakes.cluster.ssh_public_keys
worker_ip
keys = `ssh core@#{worker_ip} cat .ssh/authorized_keys`
keys.lines
keys
Pancakes.cluster.ssh_public_keys
keys
Pancakes.cluster.ssh_public_keys
Pancakes.cluster.fleet_etcd_servers
xx = Autoscaling.worker_cloud_config(num: 3, fleet_etcd_servers: Pancakes.cluster.fleet_etcd_servers, ssh_public_keys: Pancakes.cluster.ssh_authorized_keys)
xx = Autoscaling.worker_cloud_config(num: 3, fleet_etcd_servers: Pancakes.cluster.fleet_etcd_servers, ssh_public_keys: Pancakes.cluster.ssh_public_keys)
puts xx.to_yaml
xx = Pancakes::Stack.last
xx.units.last
yy = xx.units.last
yy.service_name
yy.name
Pancakes::Fleet::Templating.retirement_sentinel_template(machine_id: 'foo')
xx = Pancakes::Stack.last
yy = xx.units.last
Pancakes::Fleet::Templating.detect_and_make_template(yy)
Pancakes::Fleet::Templating.retirement_sentinel_template(machine_id: 'foo')
xx = Pancakes::Stack.last
yy = xx.units.last
Pancakes::Fleet::Templating.detect_and_make_template(yy)
Pancakes.cluster.worker_machines
Pancakes.cluster.all_units
Pancakes.cluster.client.list
xx = Autoscaling.worker_cloud_config(num: 3, fleet_etcd_servers: Pancakes.cluster.fleet_etcd_servers, ssh_public_keys: Pancakes.cluster.ssh_public_keys)
xx = Autoscaling.worker_cloud_config(num: 3)
puts xx.to_yaml
xx = Autoscaling.worker_cloud_config(num: 3)
puts xx.to_yaml
Pancakes.cluster.aws_worker_machines
Pancakes.cluster.worker_machines
ww = Pancakes.cluster.worker_machines
uu = Pancakes.cluster.all_units
uu
uu.group_by {|unit| unit["name"] }
uu.group_by {|unit| unit["machine_id"] }
ww
uu = Pancakes.cluster.waitress
uu = Pancakes.cluster.retired_machines
au = all_units.group_by {|unit| unit["machine_id"] }
uu = Pancakes.cluster.retired_machines
uu = Pancakes.cluster.all_units
uu
uu.select{|u| u['name'] == 'retirement-sentinel.service' }
uu = Pancakes.cluster.retired_machines
uu = Pancakes.cluster.retired_machine_ids
ls
puts 3
uu = Pancakes.cluster.next_worker_num
worker_machines
worker_machines.size
xx = Pancakes::Stack.create_with_git_strings(tp: 'L+master', services: ['mysql', 'tp-server'])
xx.commits
xx.units
xx.bring_up
xx = Pancakes::Stack.create_with_git_strings(tp: 'L+master', services: ['mysql', 'tp-server'])
xx.bring_up
xx = Pancakes::Stack[54]
xx.destroy
Pancakes::Stack.where(state: 'running')
Pancakes::Stack.count(state: 'running')
Pancakes::Stack.where(state: 'running').first.netmaster_unit
xx = Pancakes::Stack.where(state: 'running')
xx.count
xx = Pancakes::Stack.where(state: 'running').map{|ss| ss.netmaster_unit.machine_ip }
xx = Pancakes::Stack.where(state: 'running').map{|ss| ss.netmaster_unit.machine_ip }.count
xx = Pancakes::Stack.where(state: 'running').map{|ss| ss.netmaster_unit.machine_ip }
Hash.new(0)
yy = Hash.new(0)
yy[:foo]
yy[:foo] += 1
yy]
yy
xx = Pancakes::Stack.where(state: 'running').map{|ss| ss.netmaster_unit.machine_ip }.reduce(Hash.new(0)){|memo,ip| memo[ip] += 1 ; memo }
xx = Pancakes::Stack.where(state: 'off')
Services.jarvis
Pancakes::Services.jarvis
Pancakes::Stack.last
xx = Pancakes::Stack.last
xx.bring_down
xx.destroy
xx = Pancakes::Stack.create_with_git_strings(tp: 'L+master', services: Pancakes::Services.tp_services)
xx.commits
xx.bring_up
xx
xx.ports
xx.units
services
params
services
Pancakes.cluster.next_worker_num
Pancakes.cluster.machine_thread
$0 = 'foo'
Pancakes.cluster.machine_thread
{ foo: :bar, bar: :foo } == { bar: :foo, foo: :bar }
{ boo:{hubs: 'hi'},foo: :bar, bar: :foo } == { bar: :foo, foo: :bar, boo: { hugs: 'hi' }}
{ boo:{hugs: 'hi'},foo: :bar, bar: :foo } == { bar: :foo, foo: :bar, boo: { hugs: 'hi' }}
Pancakes.cluster.machine_thread
xx = Pancakes.cluster.machine_thread
xx
xx = Pancakes.cluster.machine_thread.join
Pancakes.cluster.machine_thread
Pancakes.cluster.machine_thread.state
Pancakes.cluster.machine_thread.status
Pancakes.cluster.machine_thread
Pancakes.cluster.machine_thread.join
Pancakes.cluster.machine_thread.kill
Pancakes.cluster.machine_thread.status
Pancakes.cluster.etcd
Pancakes.cluster.etcd.get(Pancakes::Fleet::Etcd::MACHINES_KEY)
Pancakes.cluster.etcd.client.get(Pancakes::Fleet::Etcd::MACHINES_KEY)
Pancakes.cluster.etcd.client.get(Pancakes::Fleet::Etcd::MACHINES_KEY).node
xx = Pancakes.cluster.etcd.client.get(Pancakes::Fleet::Etcd::MACHINES_KEY)
xx.node
xx = Pancakes.cluster.etcd.client.get(Pancakes::Fleet::Etcd::MACHINES_KEY, recursive: true)
xx.node
xx = Pancakes.cluster.list_machines
Pancakes.cluster.etcd.all_machines
xx = Pancakes.cluster.etcd.client.get(Pancakes::Fleet::Etcd::MACHINES_KEY, recursive: true)
xx.dir?
xx.dir
xx
xx.node.dir
xx.node.dir?
xx.node.children
xx.node.children.first
Pancakes.cluster.etcd.all_machines
Pancakes.cluster.etcd.all_machines.first.values.last
Pancakes.cluster.etcd.all_machines
JSON.parse("foo")
JSON.parse("{")
Pancakes.cluster.etcd.all_machines
Pancakes.cluster.etcd.all_units
Pancakes.cluster.start_threads!
[/metrics/].reduce(false){|memo,re| memo || re.match('foo') }
[/metrics/].reduce(false){|memo,re| memo || !!re.match('foo') }
[/metrics/].reduce(false){|memo,re| memo || !!re.match('metrics.serv') }
[/metrics/].reduce(false){|memo,re| memo || re =~ 'foo' }
Pancakes.cluster.start_threads!
Pancakes.cluster.etcd.instance_varible_get
Pancakes.cluster.start_threads!
Pancakes.cluster.etcd.cache
Pancakes.cluster.etcd.cache.first
Pancakes.cluster.etcd.cache[1]
Pancakes.cluster.etcd.cache.keys
Pancakes.cluster.start_threads!
Pancakes.cluster.etcd.cache
Pancakes.cluster.etcd.all_machines
Pancakes.cluster.etcd.cache
Pancakes.cluster.start_threads!
Pancakes.cluster.machine_thread
Pancakes::Fleet::Machine
Pancakes::Fleet::Machine.store
Pancakes.cluster.units_thread
Pancakes.cluster.start_threads!
Pancakes::Fleet::Machine.store
Pancakes.cluster.start_threads!
Pancakes::Fleet::Machine.store
ap Pancakes::Fleet::Machine.store
Pancakes.cluster.start_threads!
Pancakes.cluster
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.start_threads!
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
[] + [12000]
Pancakes.cluster.etcd.cluster_name
Pancakes.cluster.start_threads!
Sequel.sqlite
Pancakes::Fleet::Mirror.db
Pancakes::Fleet::Mirror.db.tables
Pancakes::Fleet::Mirror.mkdb
Pancakes::Fleet::Mirror.db.tables
Pancakes::Fleet::Mirror.db.tables.first
Pancakes::Fleet::Mirror.db.schema
Pancakes::Fleet::Mirror.db.schema(:machines)
Pancakes::Fleet::Mirror.mkdb
Pancakes::Fleet::Mirror.db.schema(:machines)
Pancakes::Fleet::Mirror.db.schema(:units)
Pancakes::Fleet::Mirror::Machine.all
Pancakes::Fleet::Mirror::Machine.db
Pancakes::Fleet::Mirror::Unit.db
Pancakes::Fleet::Mirror.db.schema(:units)
Pancakes::Fleet::Mirror.db.indexes
Pancakes::Fleet::Mirror.db.indexes(:units)
Pancakes.cluster.watch_machines
event.value
Machine
Machine.upsert(event.value)
c
wtf?
Machine.all
Pancakes::Fleet::Mirror::Machine.all
Pancakes.cluster.watch_machines
c
Pancakes.cluster.watch_machines
hsh
inst
inst.metadata
inst.attributes
inst.columns
inst.metadata
Pancakes.cluster.watch_machines
c
Pancakes.cluster.watch_machines
inst
Pancakes::Fleet::Mirror::Machine.all
inst.columns
inst
inst.machine_metadata
inst.machine_metadata = :foo
Pancakes.cluster.watch_machines
c
Pancakes.cluster.watch_machines
inst
c
Pancakes.cluster.watch_machines
Pancakes::Fleet::Mirror.in_memory_db
Pancakes::Fleet::Mirror::Unit.db
Pancakes.cluster.watch_machines
c
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes::Fleet::Mirror::Machine.all
Pancakes::Fleet::Mirror::Machine.first.units
Pancakes.cluster.watch_units
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes::Fleet::Mirror::Machine.all
Pancakes.cluster.watch_units
Pancakes::Fleet::Mirror::Machine.all
Pancakes.cluster.watch_units
Pancakes::Fleet::Mirror::Machine.all
Pancakes::Fleet::Mirror::Machine.all.destroy
Pancakes::Fleet::Mirror::Machine.destroy
Pancakes::Fleet::Mirror::Machine.dataset.destroy
Pancakes::Fleet::Mirror::Machine.all.destroy
Pancakes::Fleet::Mirror::Machine.all
Pancakes.cluster.watch_machines
Pancakes::Fleet::Mirror::Machine.all
Pancakes.cluster.watch_machines
Pancakes::Fleet::Mirror::Machine.all
Pancakes.cluster.watch_units
Pancakes::Fleet::Mirror::Machine.all.last.units
Pancakes.cluster.watch_units
Pancakes::Fleet::Mirror::Machine.all
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes::Fleet::Mirror::Machine.all
Pancakes::Fleet::Mirror::Machine.all.first.units
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
wtf?
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes::Fleet::Mirror::Machine.all.first.units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes::Fleet::Mirror::Machine.last
.units
Pancakes::Fleet::Mirror::Machine.last.units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes::Fleet::Mirror::Machine.last.units
Pancakes::Fleet::Mirror::Machine.last
Pancakes::Fleet::Mirror::Machine.all
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes::Fleet::Mirror::Machine.all
Pancakes.cluster.watch_units
Pancakes::Fleet::Mirror::Machine.last.units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
event
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes::Fleet::Mirror::Machine.last.units
Pancakes::Fleet::Mirror::Machine.last
Pancakes::Fleet::Mirror::Machine.last.units
Pancakes.cluster.watch_machines
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.watch_units
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
unit
unit.machine
c
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes::Fleet::Mirror::Machine.last.units
Pancakes::Fleet::Mirror::Machine.last
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.watch_machines
Pancakes::Fleet::Mirror::Machine.last
Pancakes::Fleet::Mirror::Machines
Pancakes::Fleet::Mirror::Machine.all
Pancakes::Fleet::Mirror::Machine.all[2]
Pancakes::Fleet::Mirror::Machine.all[2].units
Pancakes.cluster.watch_units
Pancakes::Fleet::Mirror::Machine.all[2]
Pancakes::Fleet::Mirror::Machine.all[2].units
Pancakes::Fleet::Mirror::Machine.all[1].units
Pancakes::Fleet::Mirror::Machine.all[1]
Pancakes::Fleet::Mirror::Machine.all[2]
Pancakes::Fleet::Mirror::Machine.all[1]
Pancakes::Fleet::Mirror::Machine.all[1].units
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
unit
unit.machine
unit.machine.remove_unit(unit)
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes::Fleet::Mirror::Machine.all
Pancakes::Fleet::Mirror::Machine.all[1]
Pancakes::Fleet::Mirror::Machine.all[1].units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes::Fleet::Mirror::Machine.all[1].units
Pancakes::Fleet::Mirror::Machine.all
Pancakes::Fleet::Mirror::Machine.all[3]
Pancakes::Fleet::Mirror::Machine.all[3].units
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.unretire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Pancakes.cluster.retire_machine(machine_id: 'ab6dc9641ada4a18be57346374acd257')
Time.utc
Time.utc.now
Time.zone(:utc).ow
Time.zone(:utc).now
Time.zone(:utc)
Time.zone
Time.now.utc
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_units
Pancakes::Fleet::Templating.suicide_template(machine_id: 'foo')
puts Pancakes::Fleet::Templating.suicide_template(machine_id: 'foo')['Service']['ExecStart']
Autoscaling.scale_up
wtf?
Autoscaling.scale_up
Pancakes.cluster.suicide_machine('11c92151330e47f48c753fa6f32a2735')
wtf?
Pancakes::Fleet::Templating.suicide_template(machine_id: 'foo')
Pancakes.cluster.suicide_machine('11c92151330e47f48c753fa6f32a2735')
Pancakes.cluster.retire_machine('11c92151330e47f48c753fa6f32a2735')
wtf
Pancakes.cluster.retire_machine('11c92151330e47f48c753fa6f32a2735')
Pancakes.cluster.dummy_unit(machine_id: '11c92151330e47f48c753fa6f32a2735', name: 'pancakes_dummy')
Pancakes.cluster.start_dummy_unit(machine_id: '11c92151330e47f48c753fa6f32a2735', name: 'pancakes_dummy')
Pancakes.cluster.retire_machine('11c92151330e47f48c753fa6f32a2735')
Pancakes.cluster.start_dummy_unit(machine_id: '11c92151330e47f48c753fa6f32a2735', name: 'pancakes_dummy')
Pancakes.cluster.retire_machine('11c92151330e47f48c753fa6f32a2735')
Pancakes.cluster.start_dummy_unit(machine_id: '11c92151330e47f48c753fa6f32a2735', name: 'pancakes_dummy')
Pancakes.cluster.retire_machine('11c92151330e47f48c753fa6f32a2735')
Pancakes.cluster.start_dummy_unit(machine_id: '11c92151330e47f48c753fa6f32a2735', name: 'pancakes_dummy')
Pancakes.cluster.retire_machine('11c92151330e47f48c753fa6f32a2735')
Autoscaling.scale_up
cc = Autoscaling.worker_cloud_config(num: 4)
cc
cc.cloud_config
cc.cloud_config.fleet
cc.cloud_config['fleet']
cc.cloud_config[:fleet]
cc.cloud_config.keys
cc.cloud_config['coreos']
cc.cloud_config['coreos']['fleet']
cc.cloud_config['coreos']['fleet']['metadata']
cc.cloud_config['coreos']['fleet']['metadaxta']
cc.cloud_config['coreos']['fleext']['metadaxta']
cc.metadata
Pancakes.cluster.retire_machine('62d816d14e2b43e5a856145c2044752d')
cc = Autoscaling.worker_cloud_config(num: 4, metadata: { foo: 'bar' })
puts cc.to_yaml
_, _, foo = "grnds_honesty_foo".split('_')
foo
_
a, x, foo = "grnds_honesty_foo".split('_')
z
a
x
foo
"asdf".include?('as')
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+master')
xx.bring_up
xx.netmaster_unit
xx.netmaster_unit.name
Pancakes::Unit.parse_unit_name(xx.netmaster_unit.name)
xx.refresh_unit_states
xx = Pancakes::Stack.last
xx.refresh_unit_states
cluster_units
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.refresh_unit_states
cluster_units
list
list-
list+
help
whereami
whereami 100
whereami 10
w
help
w
w 10
Pancakes.cluster.all_units
c
xx = Pancakes::Stack.last
xx.refresh_unit_states
xx.bring_down
xx.bring_up
Integrity.start_service('metrics')
unit_file
puts unit_file
hsh
Integrity.start_service('metrics')
Pancakes.cluster.watch_metrics
event
event.value
JSON.parse event.value
JSON.parse event.value.strip
puts event.value
Integrity.start_service('metrics')
c
Pancakes.cluster.watch_metrics
event.value
Integrity.start_service('metrics')
Pancakes.cluster.watch_metrics
c
Pancakes.cluster.watch_metrics
event.value
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_metrics
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_metrics
wf
wtf
Pancakes.cluster.watch_metrics
wtf
Pancakes::Fleet::Mirror::Machine.db.schema(:machines)
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_metrics
Pancakes::Fleet::Mirror::Machine.last.metrics
Pancakes::Fleet::Mirror::Machine.last.memfree
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_metrics
Pancakes.cluster.event_hub_thread
Pancakes.cluster.watch_metrics
Integrity.start_service('metrics')
%i[x ]
Pancakes::Fleet::Mirror::Machine.all
Pancakes::Etcd::Client.cache
Pancakes.cluster.etcd.cache
Pancakes.cluster.event_hub_thread
Pancakes.cluster.watch_machines
Pancakes.cluster.watch_metrics
machines
machines.select{|m| !!m.memfree }
machines.select{|m| !!m.memfree }.map{|m| m.values }
machines.select{|m| !!m.memfree }.map{|m| %i[memfree memtotal ncpu load].reduce({}){|hsh,key| hsh[key] = m[key ; hsh } }
machines.select{|m| !!m.memfree }.map{|m| %i[memfree memtotal ncpu load].reduce({}){|hsh,key| hsh[key] = m[key] ; hsh } }
xx = machines.select{|m| !!m.memfree }.map{|m| %i[memfree memtotal ncpu load].reduce({}){|hsh,key| hsh[key] = m[key] ; hsh } }
xx.keys
xx.first.keys
xx.first.keys.reduce({}) {|hsh,key| hsh[key] +=}
Pancakes.cluster.watch_machines
Pancakes.cluster.event_hub_thread
Pancakes.cluster.watch_machines
Pancakes.cluster.event_hub_thread
Pancakes.cluster.watch_metrics
machines
Pancakes.cluster.watch_machines
Pancakes.cluster.event_hub_thread
Pancakes.cluster.watch_metrics
Pancakes.cluster.watch_machines
Pancakes.cluster.event_hub_thread
Pancakes.cluster.watch_metrics
Pancakes.cluster.watch_machines
Pancakes.cluster.event_hub_thread
Pancakes.cluster.watch_metrics
Pancakes.cluster.metrics
Pancakes.cluster.watch_machines
Pancakes.cluster.event_hub_thread
Pancakes.cluster.watch_metrics
Pancakes.cluster.metrics
xx = Pancakes::Stack.create_with_git_strings(services: Pancakes::Services.banyan_services)
xx.commits
xx = Pancakes::Stack.create_with_git_strings(services: Pancakes::Services.jarvis_services)
x.bring_up
xx.bring_up
xx.bring_down
xx = Pancakes::Stack.last
xx.commits
xx.bring_up
xx.bring_down
xx.bring_up
xx.bring_down
xx.bring_up
xx.bring_down
xx.bring_up
xx.bring_down
xx.bring_up
xx.bring_down
xx.bring_up
xx.bring_down
xx = Pancakes::Stack.last
xx.refresh_commits
xx = Pancakes::Stack.last
xx.refresh_commits
xx.refresh_commits(force: true)
xx.bring_down
xx.bring_up
xx.refresh_commits(force: true)
xx.bring_down
xx.refresh_commits(force: true)
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+fix-sample-data-webmock', services: Pancakes::Services.jarvis_services)
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_down
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+fix-rake-jarvis-dev', services: Pancakes::Services.jarvis_services)
xx.bring_up
xx.commits
xx = Pancakes::Stack.last
Pancakes::Stack.all
Pancakes::Stack.all.destroy
Pancakes::Stack.destroy
Pancakes::Stack.destroy_all
Pancakes::Stack.all.destroy
Pancakes::Stack.all.destroy_al;
Pancakes::Stack.all.destroy_all
Pancakes::Stack.dataset.destroy_all
Pancakes::Stack.dataset
Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+fix-rake-jarvis-dev', services: Pancakes::Services.jarvis_services)
xx.bring_up
Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+fix-rake-jarvis-dev', services: Pancakes::Services.jarvis_services)
xx.bring_up
xx.commit_for_codebase('frick')
xx = Pancakes::Stack.last
xx.bring_down
xx.bring_up
unit
unit.commit
xx
unit.stack
unit.stack.commits
unit.commit
unit.stack.commit_for_codebase('frick')
unit.codebase
xx = Pancakes::Stack.last
xx.bring_down
xx.bring_up
xx = Pancakes::Stack.last
Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+fix-rake-jarvis-dev', services: Pancakes::Services.jarvis_services)
Pancakes::Stack.dataset.destroy
Integrity.start_services
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+fix-rake-jarvis-dev', services: Pancakes::Services.jarvis_services)
xx.bring_up
Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(services: Pancakes::Services.banyan_services)
xx.commits
xx.bring_up
xx.bring_down
xx.bring_up
xx.bring_down
xx = Pancakes::Stack.last
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_down
xx = Pancakes::Stack.create_with_git_strings(services: %w[frick-server])
xx.bring_up
tt = Pancakes::Fleet::Templating.detect_and_make_template(xx.units.first)
xx = Pancakes::Stack.create_with_git_strings(services: %w[frick-server mysql postgres])
xx.bring_up
xx.bring_down
xx = Pancakes::Stack.create_with_git_strings(services: %w[frick-server mysql postgres banyan-server])
xx.bring_up
xx = Pancakes::Fleet::MachinesWatcher.new
xx.watch
xx = Pancakes::Fleet::MachinesWatcher.new
xx.async.watch
xx = Pancakes::Fleet::MachinesWatcher.new
xx.async.watch
xx = Pancakes::Fleet::WatcherSupervisionGroup.run
xx = Pancakes::Fleet::WatcherSupervisionGroup.run!
xx.actors
xx = Pancakes::Fleet::WatcherSupervisionGroup.run!
xx
xx.actors
xx = Pancakes::Fleet::WatcherSupervisionGroup.run!
xx
Pancakes::Fleet::WatcherSupervisionGroup.run!
puts 'foo'
xx
p Celluloid::Actor.all.size
p Celluloid::Actor.all
Celluloid::Actor.all
Pancakes::Fleet::WatcherSupervisionGroup.run!
Pancakes::Fleet::ClusterSupervisionGroup.run!
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx
xx.actors
xx[:machines_watcher]
Pancakes::Fleet::ClusterSupervisionGroup[:machines_watcher]
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.class
xx
xx[]
xx[:foo]
xx[:watcher_sg]
xx[:watcher_sg][:machines_watcher]
xx[:watcher_sg]
xx[:watcher_sg].class
xx[:watcher_sg].blocks
xx[:watcher_sg]
xx[:watcher_sg].actors
xx.actors
xx.actors.size
xx.actors.first
xx.actors.first[:machines_watcher]
xx.actors.first.registry
Celluloid::Registry
Celluloid::Registry.names
xx.actors.first
xx.actors.first.methods
xx.actors
xx.actors.first
xx.actors.first.registry
xx.actors.first
yy = xx.actors.first
yy
yy.class
yy.alive?
xx.actors.first[:machines_watcher]
xx[:watcher_sg].actors
xx[:watcher_sg]
xx = Pancakes::Fleet::ClusterSupervisionGroup.run
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx[:metrics_watcher]
xx[:metrics_watcher].async.watch
xx[:machines_watcher].async.watch
xx[:metrics_watcher].async.watch
xx[:event_hub].async.wa
xx[:event_hub].async.go
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx[:machines_watcher].async.watch
xx[:metrics_watcher].async.watch
xx[:event_hub].async.go
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx[:machines_watcher].async.watch
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.actors
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes::Fleet::Mirror::Machine.all
Pancakes::Fleet::Mirror::Machine.all.map(&:metrics)
Pancakes::Fleet::Mirror::Machine.all.map(&:metrics).compact
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Celluloid.stack_dump
puts 'hi'
xx.running?
xx
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx[:machines_watcher].async.watch
xx[:metrics_watcher].async.watch
xx
puts xx
xx.actors
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx[:machines_watcher].async.watch
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx[:machines_watcher].async.watch
xx[:metrics_watcher].async.watch
xx
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx
xx.actors
xx[:machines_watcher].async.watch
xx
Celluloid.stack_dump
Celluloid.task_class = Celluloid::TaskThread
Celluloid.stack_dump
xx[:machines_watcher].async.watch
Celluloid.stack_dump
xx[:metrics_watcher].async.watch
Celluloid.stack_dump
xx[:event_hub].async.watch
xx[:event_hub]
xx[:event_hub].running?
xx
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.actors
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx = Pancakes::Fleet::ClusterSupervisionGroup.run
Pancakes::Fleet::ClusterSupervisionGroup.run!
xx = Pancakes::Fleet::Machine.new
xx = Pancakes::Fleet::MachineWatcher.new
xx = Pancakes::Fleet::MachinesWatcher.new
xx.async.watch
xx = Pancakes::Fleet::MachinesWatcher.new
xx = Pancakes::Fleet::MachinesWatcher.supervise
xx = Pancakes::Fleet::MachinesWatcher.new
xx.async.watch
xx = Pancakes::Fleet::MachinesWatcher.new
xx.async.watch
xx = Pancakes::Fleet::MetricsWatcher.new
xx.async.watch
xx = Pancakes::Fleet::EventHub.new
ls
puts "hi
"
puts "hi"
Celluloid.actors
Pancakes.cluster.event_hub
xx = Pancakes::Fleet::MachinesWatcher.new
xx = Pancakes::Fleet::MetricsWatcher.new
Pancakes::Fleet::MachinesWatcher.new.async.watch
Pancakes::Fleet::MetricsWatcher.new.async.watch
Pancakes::Fleet::MachinesWatcher.new.async.watch
Pancakes::Fleet::MetricsWatcher.new.async.watch
Pancakes::Fleet::MachinesWatcher.new.async.watch
Pancakes::Fleet::MetricsWatcher.new.async.watch
Pancakes::Fleet::MachinesWatcher.new.async.watch
Pancakes::Fleet::MetricsWatcher.new.async.watch
Pancakes::Fleet::MachinesWatcher.new
Pancakes::Fleet::MetricsWatcher.new
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Celluloid::Actor[:event_hub]
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
7.0/8..0
7.0/8.0
(7.0/8.0)*100
(7.0/8.0)
((8.0 - 7.0) / 8.0)
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Pancakes::Fleet::Machine.all
Pancakes::Fleet::Machine.dataset
Pancakes::Fleet::Machine.dataset.destroy
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Pancakes::Fleet::Machine.all_metrics
Pancakes::Fleet::Machine.first.latest_metrics
Pancakes::Fleet::Machine.first.latest_metrics!
Pancakes::Fleet::Machine.last.latest_metrics!
Pancakes::Fleet::Machine.all
Pancakes::Fleet::Machine.all[1].latest_metrics!
JSON.parse Pancakes::Fleet::Machine.all[1].latest_metrics!
Pancakes::Fleet::Machine.all
JSON.parse Pancakes::Fleet::Machine.all[1].latest_metrics!
wtf?
Pancakes::Fleet::Machine.all[1].latest_metrics!
Pancakes::Fleet::Machine.all
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Pancakes::Fleet::Machine.all
x = 4294967294433
xx
x
Pancakes::Fleet::Machine.all
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Pancakes::Fleet::Machine.all
Pancakes.cluster.metrics
Celluloid.actors
Celluloid::Actor
Celluloid::Actor.all
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Pancakes::Fleet::Machine.all
Pancakes::Fleet::Machine.order(:memfree.asc)
Pancakes::Fleet::Machine.order(:memfree)
Pancakes::Fleet::Machine.reverse(:memfree)
Pancakes::Fleet::Machine.reverse(:load1)
Pancakes::Fleet::Machine.exists(:load1).reverse(:load1)
Pancakes::Fleet::Machine.exist(:load1).reverse(:load1)
Pancakes::Fleet::Machine.exclude(load1: nil).reverse(:load1)
Pancakes::Fleet::Machine.exclude(load1: nil).order(:load1)
Pancakes.cluster.retirement_candiates
Autoscaling.scale_up
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Pancakes.cluster.retirement_candiates
Pancakes::Fleet::Machine.all
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Pancakes::Fleet::Machine.all
Pancakes.cluster.retirement_candiates
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Pancakes.cluster.metrics
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Pancakes::Fleet::Machine.all
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Autoscaling.scale_down
xx = Pancakes::Stack.create_with_git_strings(services: %w[frick-server mysql postgres banyan-server])
xx.bring_up
xx.bring_down
xx.bring_up
xx.bring_down
xx.bring_up
xx = Pancakes::Stack.create_with_git_strings(banyan: 'epic/1686236/banyan-visit-task-details', services: %w[frick-server mysql postgres banyan-server])
xx.bring_up
xx.bring_down
xx = Pancakes::Stack[72]
xx.bring_down
xx = Pancakes::Stack[73]
xx.commits
xx.bring_up
xx.units
xx.units.last
xx.units.last.tcp_info
xx.units.last.tcp_str
xx = Pancakes::Stack.all
xx = Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.all
def foo(x=Hash.new) ; puts x.inspect ; end
foo
def foo(x=Hash.new(3) ; puts x.inspect ; end
def foo(x=Hash.new(3)) ; puts x.inspect ; end
def foo(x=Hash.new(3)) ; puts x[:y].inspect ; end
foo
def foo(x=Hash.new(3), y={}) ; puts x[:y].inspect ; end
foo
Pancakes::Stack.all
xx = Pancakes::Stack.last
xx
x
xx
xx.units.lats
xx.units
xx.units[10]
uu = xx.units[10]
uu.machine_of
xx.units[9].machine_of
client
client.get('/grnds')
client.set('/test/dir0/leaf0')
client.set('/test/dir0/leaf0', value: 'foo')
w
w10
w 10
w 100
w 20
client.get(key)
client.get(key, verbose: true)
client.watch(key)
last_index(key)
resp
watch_resp
change_index
watched_index
watched_indexc
c
change_index
watched_index
Pancakes::Fleet::Machine.all
Pancakes.cluster.etcd
Pancakes.cluster.fleet_client
Pancakes::Fleet::Machine.all
xx = Pancakes::Fleet::MachinesWatcher.new
Pancakes::Fleet::Machine.all
xx = Pancakes::Fleet::MachinesWatcher.new
yy = etcd.leaves(MACHINES_KEY)
yy = etcd.get_leaves(MACHINES_KEY)
xx = Pancakes::Fleet::MachinesWatcher.new
Pancakes::Fleet::Machine.all
xx = Pancakes::Fleet::MachinesWatcher.new
etcd_hash
xx = Pancakes::Fleet::MachinesWatcher.new
Pancakes::Fleet::Machine.all
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx = Pancakes::Fleet::ClusterSupervisionGroup.run
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes::Fleet::Machine.all
Pancakes::Fleet::Machine.dataset.delete
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
event
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
event
c
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
hsh
etcd_hsh
etcd_hash
etcd_hash.delete 'Version'
etcd_hash
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
hsh
c
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes::Fleet::Unit.all
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
hsh
Pancakes::Fleet::Machine.all
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes::Fleet::Machine.all
Pancakes::Fleet::Machine.all[2]
Pancakes::Fleet::Machine.all[2].units
Pancakes::Fleet::Machine.all[3].units
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes::Fleet::Machine.all[3].units
Pancakes::Fleet::Machine.all
Pancakes::Fleet::Machine.all[1].units
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes::Fleet::Machine.all[1].units
Pancakes::Fleet::Machine.all[2].units
Pancakes::Fleet::Machine.all[3].units
Pancakes::Fleet::Machine.all[3].units.last.machine
Pancakes::Fleet::Machine.all[3].units.last.machine.units
Pancakes::Fleet::Machine.all[3].units
c
Pancakes::Fleet::Machine.all
Pancakes.cluster.start_dummy_unit(machine_id: 'ab6dc9641ada4a18be57346374acd257', name: 'foo')
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes.cluster.start_dummy_unit(machine_id: 'ab6dc9641ada4a18be57346374acd257', name: 'foo2')
Pancakes::Fleet::Machine.all
Pancakes::Fleet::Machine.all[2]
Pancakes::Fleet::Machine.all[2].units
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes.cluster.start_dummy_unit(machine_id: 'ab6dc9641ada4a18be57346374acd257', name: 'foo')
Pancakes::Fleet::Machine.all
Pancakes::Fleet::Machine.all[2].unit
Pancakes::Fleet::Machine.all[2].units
Pancakes.cluster.start_dummy_unit(machine_id: 'ab6dc9641ada4a18be57346374acd257', name: 'foo')
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Autoscaling.scale_up
Pancakes::Fleet::Machine.all
Pancakes::Fleet::Machine.all[5]
Pancakes::Fleet::Machine.all[5].units
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Autoscaling.scale_up
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes::Fleet::Machine.all
Pancakes::Fleet::Machine.all[3].units
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes::Fleet::Unit.all
Pancakes::Fleet::Unit.exclude(machine_id: nil)
Pancakes::Fleet::Unit.where(machine_id: nil)
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes::Fleet::Unit.find_by_hash('db88c88d740a58b0805ee2dd5c3e6e0bd9b3e486')
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
c
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
c
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
c
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
c
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
c
etcd_hash
etcd_hash[etcd_hash_field]
c
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
c
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
c
etcd_hash[etcd_hash_field]
etcd_hash
c
etcd_hash
c
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
c
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
c
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
machine_state
event.value.merge(name: event.key_components.last, machine_hash: machine_state['ID'])
s
f
model_hash(etcd_hash)
Pancakes::Fleet::Unit.where(unit_hash: 'db88c88d740a58b0805ee2dd5c3e6e0bd9b3e486')
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes::Fleet::Unit.count
Pancakes::Fleet::Machine.count
Pancakes::Fleet::Unit.last
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
machine_state
event.value
n
unit
c
n
unit.machine_id
unit.machine
hsh = model_hash(etcd_hash)
hsh = model_hash(event.value.merge(name: event.key_components.last, machine_hash: machine_state['ID']))
hsh = Pancakes::Fleet::Unit.model_hash(event.value.merge(name: event.key_components.last, machine_hash: machine_state['ID']))
Pancakes::Fleet::Unit.find_by_hash(hsh[Pancakes::Fleet::Unit.etcd_hash_field])
hsh[Pancakes::Fleet::Unit.etcd_hash_field]
Pancakes::Fleet::Unit.etcd_hash_field
event.value.merge(name: event.key_components.last, machine_hash: machine_state['ID'])
hsh = model_hash(event.value.merge(name: event.key_components.last, machine_hash: machine_state['ID']))
hsh = Pancakes::Fleet::Unit.model_hash(event.value.merge(name: event.key_components.last, machine_hash: machine_state['ID']))
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Pancakes.cluster.start_dummy_unit(machine_id: 'ab6dc9641ada4a18be57346374acd257', name: 'foo')
Autoscaling.scale_up
Pancakes::Fleet::Machine.all
Pancakes::Fleet::Machine.all[5].units
mm = Pancakes::Fleet::Machine.all[5]
mm
uu = Pancakes::Fleet::Unit.where(machine_id: nil)
uu = Pancakes::Fleet::Unit.where(machine_hash: mm.machine_id)
mm.machine_id
uu = Pancakes::Fleet::Unit.where(name: 'metrics.service')
uu = Pancakes::Fleet::Unit.where(name: 'logspout.service')
Pancakes.cluster.start_dummy_unit(machine_id: 'cd58062cf84144a9a9960b15a91a7944', name: 'foo')
mm.units
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
event
event.key_components
event.key_components.last
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Autoscaling.scale_up
Pancakes.cluster.start_dummy_unit(machine_id: '80434e2d6e804180ab5f0b652053098d', name: 'foo')
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.start
Autoscaling.scale_up
Pancakes.cluster.start_dummy_unit(machine_id: '5c02a7efd823456bb5c3a2f5898c983a', name: 'foo')
Pancakes.cluster.etcd.get('etcd:///grnds/keys/SEND_WITH_US_API_KEY')
Pancakes::Fleet::Etcd::URL_SCHEME
Pancakes::Stack.count
Pancakes::Stack.first
xx = Pancakes::Stack.first
xx.refresh_commits
xx.bring_up
xx.bring_down
xx.bring_up
xx.ports
xx.units
xx.units.first.tcp_str
xx.units.
xx.units
xx.units.select{|u| !!u.port }
xx.units.select{|u| !!u.ping_url }
xx.units.first.service_name
xx.units.last.service_name
xx.units[3].service_name
xx.units[3].app
xx.units[3].codebase
xx = Pancakes::Stack.first
xx.units[3].codebase
xx.units[3].service_addr
xx.units[4].service_addr
xx.units[2].service_addr
xx = Pancakes::Stack.first
xx.service_addrs
xx.units
xx.units.last.tcp_str
xx.units.first.tcp_str
xx = Pancakes::Stack.first
xx.service_addrs
xx.bring_down
xx.state!
xx.service_addrs
xx = Pancakes::Stack.first
xx.service_addrs
xx = Pancakes::Stack.first
xx.bring_up
xx.bring_down
xx.bring_up
Configuration
Configuration.methods
Configuration.methods - Object.methods
help
show-source Configuration
xx = Configuration.new
xx.foo = 'foo'
xx.bar = 'foo'
xx.foo
xx.bar
xx.api_user = 'asdf'
xx.api_user 
xx.bar
xx.foo
Integrity::Build.last.service_opts
Integrity::Build.last.service_options
Integrity.cluster.start_build(Integrity::Build.last.service_options)
template_name
template
c
services
services.ai(indexes: false)
puts services.ai(indexes: false)
puts services.ai(index: false)
w
Integrity.cluster.start_build(Integrity::Build.last.service_options)
services
Integrity.cluster.start_build(Integrity::Build.last.service_options)
w
w 100
w 10
service_type
service_name
requires_service_name
service_fname
c
partials
unify partials
Integrity.cluster.start_build(Integrity::Build.last.service_options)
c
unify(partials)
Integrity.cluster.start_build(Integrity::Build.last.service_options)
template_hash
Integrity.cluster.start_build(Integrity::Build.last.service_options)
template_hash
c
template_hash
Integrity.cluster.start_build(Integrity::Build.last.service_options)
unit_fname
unit_hash
@dry_run
@dry_run = true
c
unit_fname
unit_hash
@dry_run
c
lines = File.readlines('/tmp/foo')
lines = File.readlines('/tmp/foo').map(&:chomp)
URI('adsf')
URI('asdfasdfkjdfasjklasdfkjl;fasd')
URI(nil)
lines = File.readlines('/tmp/foo').map(&:chomp)
lines = File.readlines('/tmp/foo').map{|l| l.split('!') }
lines = File.readlines('/tmp/foo').map{|l| [l['!'], l] }
lines = File.readlines('/tmp/foo').map{|l| [!!l['!'], l] }
File.readlines('/tmp/foo').map{|l| [!l['!'], l] }.reduce(true){ |m,line| m && "branch1" }
File.readlines('/tmp/foo').map{|l| [!l['!'], l] }.reduce(true){ |m,no,line| m && "branch1".include?(line) }
File.readlines('/tmp/foo').map{|l| [!l['!'], l] }.reduce(true){ |m,line| m && "branch1".include?(line[1]) }
File.readlines('/tmp/foo').map{|l| [!l['!'], l] }.reduce(true){ |m,line| m && line[0] && "branch1".include?(line[1]) }
File.readlines('/tmp/foo').map{|l| [!l['!'], l] }
File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.reduce(true){ |m,line| m && line[0] && "branch1".include?(line[1]) }
File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }
branch = 'branch1' ; File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line) }
branch = 'branch1' ; File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }
branch = 'branch1' ; File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first)
branch = 'branch1' ; File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'xbranch1' ; File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'xbrch1' ; File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'xbrch1' ; File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last || false
[].last
branch = 'xbrch1' ; !!File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'brach1' ; !!File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'branch1' ; !!File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'branch' ; !!File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'branch33333' ; !!File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'branch222' ; !!File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'branch1222' ; !!File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
URI('http://')
URI('http://asdfasdfasdf')
URI('http:asdfasdfasdf')
URI('httpasdfasdfasdf')
URI('http://///////////asdfasdfasdf')
branch = '' ; !!File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'b' ; !!File.readlines('/tmp/foo').map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'foo' ; !!['branch1', 'branch2', '!branch3'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'foo' ; !!['', 'branch1', 'branch2', '!branch3'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'foo' ; !!['x', 'branch1', 'branch2', '!branch3'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'foo' ; !!['!', 'branch1', 'branch2', '!branch3'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'foo' ; !!['!'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'foo' ; !![''].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'foo' ; !!['!', 'foo'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'foo' ; !!['!', 'foxo'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'foo' ; !!['', '!dtest'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'foo/dtest' ; !!['', '!dtest'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'foodtest' ; !!['', '!dtest'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'foo/dtest' ; !!['', '!dtest'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'dtest' ; !!['', '!dtest'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'dtest' ; !!['!dtest'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'dtest' ; !!['', '!dtest'].map{|l| [!l['!'], l.chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'dtest' ; !!['', '!dtest'].map{|l| [!l['!'], l.chomp] }.select{|line| puts line ; branch.include?(line[1]) }.map(&:first).last
branch = 'dtest' ; !!['', '!dtest'].map{|l| [!l['!'], l.chomp] }
branch = 'dtest' ; ['', '!dtest'].map{|l| [!l['!'], l.chomp] }
branch = 'dtest' ; ['', '!dtest'].map{|l| puts l ; [!l['!'], l.chomp] }
branch = 'dtest' ; ['', "!dtest\n"].map{|l| puts l ; [!l['!'], l.chomp] }
branch = 'dtest' ; ['', "!dtest\n"].map{|l| puts l ; [!l['!'], l.sub('!', '').chomp] }
branch = 'dtest' ; !!['', '!dtest'].map{|l| [!l['!'], l.sub('!', '').chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'dtest' ; !!['', '!dtesxt'].map{|l| [!l['!'], l.sub('!', '').chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'dtest' ; !!['f', '!dtesxt'].map{|l| [!l['!'], l.sub('!', '').chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
branch = 'dtest' ; !!['', '!dtesxt'].map{|l| [!l['!'], l.sub('!', '').chomp] }.select{|line| branch.include?(line[1]) }.map(&:first).last
"".chomp
"\n".chomp
Integrity::Build.last.service_options
puts 'hi'
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx = Pancakes::Fleet::ClusterSupervisionGroup.run
Pancakes.cluster.etcd.dios_add_machine
resp = @client.get(MACHINES_KEY, recursive: true)
resp = @client.get(Pancakes::Fleet::MachinesWatcher::MACHINES_KEY, recursive: true)
leaves(resp.node)
xx = leaves(resp.node)
xx.detect{|e| e[1]['PublicIP'] =~ /10\.0\.\90\.\d+/ }
xx.detect{|e| e[1]['PublicIP'] =~ /\A10\.0\.\90\.\d+\z/ }
xx.select{|e| e[1]['PublicIP'] =~ /\A10\.0\.\90\.\d+\z/ }
xx.select{|e| e[1]['PublicIP'] =~ /\A10\.0\.\80\.\d+\z/ }
xx.detect{|e| e[1]['PublicIP'] =~ /\A10\.0\.\80\.\d+\z/ }
xx.select{|e| e[1]['PublicIP'] =~ /\A10\.0\.\80\.\d+\z/ }
xx.select{|e| e[1]['PublicIP'] =~ /\A10\.0\.\80\.\d+\z/ }.sort{|a,b| a[1] <=> b[1] }
xx.select{|e| e[1]['PublicIP'] =~ /\A10\.0\.\80\.\d+\z/ }.sort{|a,b| a[1]['PublicIP'] <=> b[1]['PublicIP'] }
yy = xx.select{|e| e[1]['PublicIP'] =~ /\A10\.0\.\80\.\d+\z/ }.sort{|a,b| a[1]['PublicIP'] <=> b[1]['PublicIP'] }.last
yy.last.first.last
yy.last.first.last.size
require 'digest/sha1'
Digest::SHA1.hexdigest 'foo'
Digest::SHA1.hexdigest(Time.now.to_s)
Digest::SHA1.hexdigest(Time.now.to_s).size
Digest::SHA1.hexdigest(Time.now.to_s)[0..32]
Digest::SHA1.hexdigest(Time.now.to_s)[0..32].size
Digest::SHA1.hexdigest(Time.now.to_s)[0...32].size
Digest::SHA1.hexdigest(Time.now.to_s)[0...32].size.sub(/\A.{6}/, 'f00f00')
Digest::SHA1.hexdigest(Time.now.to_s)[0...32]\.sub(/\A.{6}/, 'f00f00')
Digest::SHA1.hexdigest(Time.now.to_s)[0...32].sub(/\A.{6}/, 'f00f00')
Digest::SHA1.hexdigest(Time.now.to_s)[0...32].sub(/\A.{6}/, 'f00f00').size
yy
yy.first
yy.first.split('/')
yy.first.split('/').join('/'_
yy.first.split('/').join('/')
yy.first.split('/').map.with_index{|e,i| e = 'foo' if i == 4 }.join('/')
yy.first.split('/').map.with_index{|e,i| i == 4 ? e = 'foo' : e }.join('/')
yy.first.split('/').map.with_index{|e,i| e = (i == 4 ? 'foo' : e) }.join('/')
yy.last
yy.last['ID]
yy.last['ID']
yy.last['PublicIP']
yy.last['PublicIP'].sub(/(\d+)\z/, $1.to_i + `)
yy.last['PublicIP'].sub(/(\d+)\z/, $1.to_i + 1)
yy.last['PublicIP'].sub(/(\d+)\z/, ($1.to_i + 1).to_s)
yy.last['PublicIP'].sub(/(\d+)\z/, $1.to_i + 1)
yy.last['PublicIP'].sub(/(\d+)\z/, ($1.to_i + 1).to_s)
yy.last['PublicIP'].sub(/(\d+)\z/, $1.to_i + 1)
yy.last['PublicIP'].sub(/(\d+)\z/, ($1.to_i + 1).to_s)
Pancakes.cluster.etcd.dios_add_machine
Pancakes.cluster.etcd.dios_add_worker_machine
prototype
c
Pancakes.cluster.etcd.dios_add_worker_machine
prototype
Pancakes.cluster.etcd.dios_add_worker_machine
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Pancakes.cluster.etcd.dios_add_worker_machine
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
Pancakes.cluster.etcd.dios_add_worker_machine
xx = Pancakes::Fleet::ClusterSupervisionGroup.start
xx = Pancakes::Fleet::ClusterSupervisionGroup.run!
xx.actors
Celluloid::Actor[:machines_watcher].state
Celluloid::Actor[:machines_watcher].async.watch
Pancakes.cluster.etcd.dios_add_worker_machine
xx = Pancakes::Fleet::MachinesWatcher.new
xx.watch
Pancakes.cluster.etcd.dios_add_worker_machine
Pancakes.cluster.etcd.all_machines
Pancakes.cluster.etcd.dios_rm_worker
all_machines
all_machines.select{|e| e.last['ID'].start_with?('f00f00;) }
all_machines.select{|e| e.last['ID'].start_with?('f00f00') }
Pancakes.cluster.etcd.dios_rm_worker
resp
Pancakes.cluster.etcd.dios_rm_worker
[{a: '1', b: '2'}, {a: '2', b: '3'}].map(&:first)
[{a: '1', b: '2'}, {a: '2', b: '3'}].map(&:a)
require 'hashie'
[{a: '1', b: '2'}, {a: '2', b: '3'}].map(&:first)
[{a: '1', b: '2'}, {a: '2', b: '3'}].map(&:Hashie::Msh)
[{a: '1', b: '2'}, {a: '2', b: '3'}].map(&:Hashie::Mash)
Integrity.start_service('logspout')
Integrity.start_service('logspout.service')
Integrity.start_service('logspout')
Pancakes::Fleet::Templating.metrics
Pancakes::Fleet::Templating.logspout
Integrity.start_service(:metrics)
Integrity.start_service(:logspout)
xx = Pancakes::Stack.create
xx = Pancakes::Stack.create(jarvis: 'L+master')
xx = Pancakes::Stack.create_with_git_strings
xx.commits
xx.bring_up
xx = Pancakes::Stack.create_with_git_strings(services: %w(jarvis-server mysql))
xx = Pancakes::Stack.create_with_git_strings(services: %w(jarvis-server jarvis-orchestra mysql))
xx.commits
xx.bring_up
xx = Pancakes::Stack.create_with_git_strings(services: %w(jarvis-server jarvis-orchestra mysql))
xx.bring_up
xx = Pancakes::Stack.create_with_git_strings(services: %w(jarvis-server jarvis-orchestra mysql))
Pancakes::Stack.dataset.destroy
Pancakes::Stack.al
Pancakes::Stack.all
xx = Pancakes::Stack.create_with_git_strings(services: %w(jarvis-server jarvis-orchestra mysql))
xx.bring_up
xx = Pancakes::Stack.all
xx = Pancakes::Stack.all.first
xx.state!
RestClient.get('http://10.0.80.29:32844/status')
RestClient.get('http://10.0.80.29:32844/')
RestClient.get('http://10.0.80.29:32844/status')
xx
xx.units
xx.units[10]
xx.units[10].codebase
xx.units[10].tcp_str
xx.units[10].service_addr
xx.units[9].service_addr
xx.units[9].codebase
xx.units[9].service_name
[{foo: 'bar'}, {}].reduce(&:merge)
[{foo: 'bar'}, {baz: 3}].reduce(&:merge)
[{foo: 'bar'}, {baz: 3}, nil].reduce(&:merge)
[{foo: 'bar'}, {baz: 3}, nil].compact.reduce(&:merge)
xx = Pancakes::Stack.last
xx.service_addrs
xx.service_addrs.map{|x| x.join('=')}
xx.service_addrs.map{|x| x.join('=')}.join(' ')
xx.units
xx.units[2].codebase
{foo: 'bar'}.merge(if true ; {baz: 'quxx'} ; end)
xx.all_unit_names
xx.all_unit_names.include?("banyan-server')
xx.all_unit_names.include?("banyan-server")
xx.unit_named("banyan-server')
xx.unit_named("banyan-server")
xx.unit_named("banyan-server").tcp_str
xx = Pancakes::Stack.last
xx.unit_named("banyan-serverx")
xx = Pancakes::Stack.last
xx.units
xx.units.last
xx.units.last.tcp_str
xx.units.first
xx.units.first.ports
xx = Pancakes::Stack.last
xx.state!
xx.bring_down
xx.bring_up
xx.bring_down
xx = Pancakes::Stack.last
xx.bring_down
xx.state!
xx.bring_up
nm
nm.refresh
xx.units
units
state!
units
Pancakes.docker.ip
Pancakes.docker.ip_address
Pancakes.docker.all_containers
Pancakes.docker.all_containers.map(&:port_mapping)
Pancakes.docker.get_container(nm.name)
Pancakes.docker.get_container(nm.name).port_mapping
xx.bring_down
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_down
xx.bring_up
hm
nm
nm.container
nm.container.port_mapping
units
units.last
units.last.port
units.last.ports
.first
units.last.primary_private_port
nm
nm.container
nm.container.port_mapping
nm.container.port_mapping[unit_named('postgres')]
unit_named('postgres')
nm.container.port_mapping[unit_named('postgres').primary_private_port]
nm.container.ip_address
nm.container.ip
nm.container.ipaddr
nm.machine_ip
nm.container.machine_ip
nm.container
pp nm.container
pp nm.container.info
pp nm.machine_ip
nm.machine_ip
netmaster_unit.container.port_mapping[unit_named('postgres').primary_private_port]
unit_named('banyan-server')
unit_named('banyan-server').service_def
xx = Pancakes::Stack.last
xx.jarvis_env
xx.bring_down
xx.bring_up
xx = Pancakes::Stack.last
xx.bring_down
xx.bring_up
xx = Pancakes::Stack.last
xx.refresh_commits
xx.bring_down
xx.bring_up
'H4sIAAiojVUAA+1UTY+bMBC9r7S/IJe5VGoPYCBNINz6QU9VtsqmaqVoFbFmAEvYRsZOm3/fCWxX0XalzYFIlTaAmMHzeH72PHmykUttBccU2K3NK9zIXKg79tGposEtz3mNhmW/kW8qtNv7YbgUDXZ3zKB1RnUpALyBtbZ5A326Qo5ihwWlP0s0BcCHHRoih9sWaRhgLSTCUXxIPjljUNnJJpBXk5GUvXB9bnROir63Q3xcBilV9gD4iqWFQfiIuq4Bgn7+4PH1T3oUPC/tn2eSHnAdBj02iQGO0mO6KJrHp9BFUbIYtQEXa/z31phFp1ljNksu1nhd1pgn0UnWiMOLNV6ZNZL5+5OssQiCERfq2s7mxnqodvTVCIUQpSCx64gxTYFrKXNVgNIWSk0c5517mkKhuZPU3NwKrbbONGdWka1WNyty4o+aCoCEdlaoCiqU4Ps+vM2MUTpNs+VNtly/G9n8Sw2d4zUcaqANFMIgt9rswQPmkwZm3P2ehf7CD1nPywY244V+TDdBRpTEai2ROapax57i/K7+a5FZCkPlvL0ZukFHEu0RJ0+Wrmn2T/m/CCW6mkA8p7NBV2CcAqFgGgf+fAodcq2Krv/rm2tbtEQrDqS/hK1JfoEwv/oD6na09boLAAA='
require 'base64'
xx = 'H4sIAAiojVUAA+1UTY+bMBC9r7S/IJe5VGoPYCBNINz6QU9VtsqmaqVoFbFmAEvYRsZOm3/fCWxX0XalzYFIlTaAmMHzeH72PHmykUttBccU2K3NK9zIXKg79tGposEtz3mNhmW/kW8qtNv7YbgUDXZ3zKB1RnUpALyBtbZ5A326Qo5ihwWlP0s0BcCHHRoih9sWaRhgLSTCUXxIPjljUNnJJpBXk5GUvXB9bnROir63Q3xcBilV9gD4iqWFQfiIuq4Bgn7+4PH1T3oUPC/tn2eSHnAdBj02iQGO0mO6KJrHp9BFUbIYtQEXa/z31phFp1ljNksu1nhd1pgn0UnWiMOLNV6ZNZL5+5OssQiCERfq2s7mxnqodvTVCIUQpSCx64gxTYFrKXNVgNIWSk0c5517mkKhuZPU3NwKrbbONGdWka1WNyty4o+aCoCEdlaoCiqU4Ps+vM2MUTpNs+VNtly/G9n8Sw2d4zUcaqANFMIgt9rswQPmkwZm3P2ehf7CD1nPywY244V+TDdBRpTEai2ROapax57i/K7+a5FZCkPlvL0ZukFHEu0RJ0+Wrmn2T/m/CCW6mkA8p7NBV2CcAqFgGgf+fAodcq2Krv/rm2tbtEQrDqS/hK1JfoEwv/oD6na09boLAAA='
Base64.decode64(xx)
puts Base64.decode64(xx)
File.open('/tmp/foo', 'w') {|f| f.puts Base64.decode64(xx) }
xx = Pancakes::Stack.last
xx = Pancakes::Stack.size
xx = Pancakes::Stack.count
xx = Pancakes::Stack.size
xx = Pancakes::Stack.last
xx.state!
xx.bring_up
xx.bring_down
xx = Pancakes::Stack.last
xx.bring_down
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+chore/97589424/banyan-pancakes', banyan: 'L+chore/97589424/banyan-pancakes')
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'L+chore/97589424/banyan-pancakes', banyan: 'L+chore/97589424/banyan-pancakes', services: Pancakes::Services.banyan_services)
xx = Pancakes::Stack.last
xx.bring_up
xx.bring_down
xx.refresh_commits
xx.refresh_commits!
xx.refresh_commits
xx.reload
xx.refresh_commits
xx.reload
xx.refresh_commits
xx.reload
xx.bring_up
fl
xx = Pancakes::Stack.last
xx.netmaster_unit
xx.netmaster_unit.container
xx.netmaster_unit.container.port_mapping
Pancakes::Services.services
Pancakes::Services.get_by_port(10010)
xx = Pancakes::Stack.last
xx.netmaster_unit.container.port_mapping
xx.netmaster_unit.container.port_mapping.map{|k,v| [k,v] }.to_h
xx.netmaster_unit.container.port_mapping.map{|k,v| [Pancakes::Services.get_by_port(k),v] }.to_h
xx.netmaster_unit.container.port_mapping.map{|k,v| [Pancakes::Services.get_by_port(k)[:name], v] }.to_h
xx.service_addrs
xx.netmaster_unit.container.port_mapping.map{|k,v| [Pancakes::Services.get_by_port(k)[:name], v] }.to_h
xx.netmaster_unit.container.port_mapping.map{|k,v| [Pancakes::Services.get_by_port(k)[:name], "#{xx.netmaster_unit.machine_ip}:#{v}"] }.to_h
xx.netmaster_unit.container.port_mapping.map{|k,v| [Pancakes::Services.get_by_port(k)[:name].split('-'), "#{xx.netmaster_unit.machine_ip}:#{v}"] }.to_h
xx.netmaster_unit.container.port_mapping.map{|k,v| [Pancakes::Services.get_by_port(k)[:name].split('-')[0], "#{xx.netmaster_unit.machine_ip}:#{v}"] }.to_h
xx = Pancakes::Stack.last
xx.service_addrs
xx.units.first.name
xx = Pancakes::Stack.last
xx.dbs
xx.dbs.delete_if{|x| x =~ /pg-/ }
xx.dbs.delete_if{|x| x =~ /\Apg-/ }
xx.dbs.delete_if{|x| x =~ /\Apg\-/ }
xx.dbs.delete_if{|x| x =~ /\Apg-/ }
xx = Pancakes::Stack.last
xx.reload
xx.refresh_commits
xx = Pancakes::Stack.create_with_git_strings(banyan: 'master', services: Pancakes::Services.banyan_services)
xx.bring_up
xx.restart_with_branch_heads
xx.bring_down
xx.bring_up
xx.state!
xx.bring_up
xx.bring_down
xx.reload
xx.bring_up
xx.non_netmaster_units
xx.units_dataset
xx.destroy
Pancakes::Stack.all
xx = Pancakes::Stack.create_with_git_strings(banyan: 'master', services: Pancakes::Services.banyan_services)
xx.commits
xx.units
xx.non_netmaster_units
xx.bring_up
xx.bring_down
xx.bring_up
2**0
2**1
2**2
2**3
2**4
2**5
5.times {|i| bb = 2 ** (i+1) ; puts bb ; sleep bb}
5.times {|i| bb = 2 ** (i+1) ; puts bb }
5.times {|i| bb = 2 ** (i+1) ; puts bb } ; nil
5.times {|i| bb = 2 ** i ; puts bb } ; nil
5.times {|i| bb = 2 ** i ; puts i  ; puts bb } ; nil
2**0
xx = Pancakes::Stack.last
xx.netmaster_unit
xx.netmaster_unit.container
xx.units
xx.units[9].container
Pancakes::Services.banyan_services
xx = Pancakes::Stack.create_with_git_strings(banyan: 'master', services: Pancakes::Services.banyan_services)
Pancakes::Stack.dataset.destroy
Pancakes::Services.banyan_services
xx = Pancakes::Stack.create_with_git_strings(banyan: 'chore/97589424/get-banyan-working', services: %w[mysql postgres banyan-server])
xx.bring_up
Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(banyan: 'chore/97589424/get-banyan-working', services: %w[mysql postgres banyan-server])
xx.bring_up
Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'chore/97589424/get-banyan-working')
xx.bring_up
xx.bring_down
xx.commits
xx.bring_down
xx.bring_up
Pancakes::Stack.dataset.destroy
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'chore/97589424/get-banyan-working'), banyan: 'chore/97589424/get-banyan-working', services: Pancakes::Services.banyan_services)
xx = Pancakes::Stack.create_with_git_strings(jarvis: 'chore/97589424/get-banyan-working', banyan: 'chore/97589424/get-banyan-working', services: Pancakes::Services.banyan_services)
xx.bring_up
xx.bring_down
xx.bring_up
xx.state!
xx = Pancakes::Stack.last
xx.bring_up
c
printf('%s', 'hi')
'-1'.to_i
Time.now.to_i.to_s
r = HTTParty.get('https://banyan.integration3.grandrounds.com')
require 'httparty'
r = HTTParty.get('https://banyan.integration3.grandrounds.com')
r.status
r.code
r = HTTParty.get('https://banyan.integration3.grandrounds.com', no_follow: true)
r.code
r = HTTParty.get('https://banyan.integration3.grandrounds.com', limit: 1)
r = HTTParty.get('https://banyan.integration3.grandrounds.com', limit: 0)
r = HTTParty.get('https://banyan.integration3.grandrounds.com', limit: 10)
r.code
r = HTTParty.get('https://banyan.integration3.grandrounds.com', limit: 2)
r.code
r = HTTParty.get('https://banyan.integration3.grandrounds.com', limit: 1)
r = HTTParty.get('https://banyan.integration3.grandrounds.com', limit: 0)
r
r.code
r = HTTParty.get('https://banyan.integration3.grandrounds.com/status')
RestClient.get()
xx = RestClient.get('10.0.80.31:32833/status')
xx = RestClient.get('10.0.80.31:32833/')
xx = RestClient.get('http://10.0.80.31:32833/')
xx.code
xx =  Net::HTTP.get(URI('http://10.0.80.31:32833/'))
xx.status
xx.vode
xx.code
xx =  Net::HTTP.get_response(URI('http://10.0.80.31:32833/'))
xx.status
xx.code
xx =  Net::HTTP.get_response(URI('http://10.0.80.31:32830/'))
xx.code
xx =  Net::HTTP.get_response(URI('http://10.0.80.31:32830/status'))
xx.code
xx =  Net::HTTP.get_response(URI('http://10.0.80.31:32831/status'))
xx.code
xx =  Net::HTTP.get_response(URI('http://10.0.80.31:32800/status'))
require 'hashie'
ff = Hashie::Mash.new
ff = Hashie::Mash.new(nil)
foo = Hashie::Mash.new(foo: 'bar')
require 'hashie'
foo = Hashie::Mash.new(foo: 'bar')
foo.is_a?(Hash)
bb = Integrity::Build.last
bb.commit
bb.reload
ENV
pp = Integrity::Project.last
pp = Integrity::Project.get(10)
pp.builds.count
pp.commits.count
DataMapper.auto_upgrade!
Integrity::Branch.all(name: 'master')
Integrity::Branch.all(name: 'master').sort{|a,b| b.project.pushes <=> a.project.pushes }
pp p= Integrity::Project.last
p
p.inc_pushes
p.save
Integrity::Branch.all(name: 'master').sort{|a,b| b.project.pushes <=> a.project.pushes }
pp p= Integrity::Project.first
p.inc_pushes
p.save
Integrity::Branch.all(name: 'master').sort{|a,b| b.project.pushes <=> a.project.pushes }
bb = Integrity::Project.last
bb.branches
bb.branches.first
bb.branches.first.most_recent_build
cc = bb.branches.first.most_recent_build
cc.successful = false
cc.save
x = [:a, :b, :c]
x = [:a, :b, :c][1..-1]
x = [:a, :b, :c][10..-1]
x = [][1..-1]
nil.join('')
nil.join(' ')
%i{foo}
3.even?
['']* 8
[:foo,:bar].take(1)
[:foo,:bar].drop(1)
[:foo,:bar].drop(2)
Time.now
Time.now - 100
Time.now - 60
7**3
125+124
Pancakes::Stack.all
Pancakes::Stack.all.last
Pancakes::Stack.all.last.sha
[].first
Pancakes.cluster.units
Pancakes.cluster.all_units
Integrity::FleetHelper.running_builds
Integrity::FleetHelper.running_build_count_by_machine
Integrity::FleetHelper.client
Integrity::FleetHelper.client.machines
Integrity::FleetHelper.client.get_machines
Integrity::FleetHelper.client.list_machines
Integrity::FleetHelper.client.list_machines["machines"]
Pancakes.cluster.worker_machine_ips
Pancakes.cluster.worker_machine_ips.map{|e| {e => 0}}
Pancakes.cluster.worker_machine_ips.map{|e| {e => 0}}.reduce(&:merge)
Integrity::FleetHelper.running_build_count_by_machine
Integrity.cluster.running_build_count_by_machine
Integrity.cluster.list_machines
Integrity.cluster.worker_machines
Integrity.cluster.worker_machines.first
ww = Pancakes::Fleet::Machine.new(Integrity.cluster.worker_machines.first)
ww.id
ww.ip
ww.metadata
ww = Pancakes::Fleet::Machine.new(Integrity.cluster.worker_machines.first)
ww.metadata
ww.metadata.role
ww = Pancakes::Fleet::Machine.new(Integrity.cluster.worker_machines.first)
ww.metadata.role
ww.metadata
ww = Pancakes::Fleet::Machine.new(Integrity.cluster.worker_machines.first)
ww.worker?
Integrity.cluster.list_machines
Integrity.cluster.list_machines.select(&:worker?)
Integrity.cluster.worker_machines
xx = Integrity.cluster.worker_machines.first
xx.inc_unit_count
xx = Integrity.cluster.worker_machines.first
xx.inc_unit_count
xx
xx = Integrity.cluster.running_build_count_by_machine
machines
xx = Integrity.cluster.unit_zen
xx = Integrity.cluster.running_builds
xx
xx = Integrity.cluster.running_builds
c
xx = Integrity.cluster.running_builds
Pancakes.cluster.group_by_machine(xx)
xx
xx.group_by {|_,v| v[:machine].ip }
xx.group_by {|_,v| v[:machine] }
yy = xx.group_by {|_,v| v[:machine] }
yy
yy.map{|k,v| [k.ip, v.size] }
yy.map{|k,v| [k.ip, v.size] }.to_h
yy.map{|k,v| [k.ip, v.size] }.each {|m,c| puts m, c }
yy.map{|k,v| [k.ip, v.size] }.each {|m,c| puts m, c } ; nil
yy
"10.0.80.31".match(/10\./)
"10.0.80.31".match(/10\.0\.\80\.(21|31)/)
"10.0.80.21".match(/10\.0\.\80\.(21|31)/)
"10.0.80.11".match(/10\.0\.\80\.(21|31)/)
"10.0.80.11".match(/10\.0\.80\.(21|31)/)
"10.0.80.21".match(/10\.0\.80\.(21|31)/)
"10.0.80.21".match(/10\.0\.80\.(?=21|31)/)
"10.0.80.21".match(/10\.0\.80\.(?!21|31)/)
"10.0.80.21".match(/10\.0\.80\.(?:21|31)/)
"10.0.80.21".match(/10\.0\.80\.(\d)/)
"10.0.80.21".match(/10\.0\.80\.(\d+)/)
"10.0.80.21".split('.').last
{}[:foo]
yy
yy.inspect
yy.to_h
ppyy.to_h
pp yy
ap yy
yy.ai index: false
yy.ai(index: false)
pp yy.ai(index: false)
puts yy.ai(index: false)
xx = Pancakes.cluster.running_builds_grouped_by_machine
xx
xx.to_a
xx.to_a.map{|m,b| [Pancakes::Fleet::Machine.new(m), b] }
xx = Pancakes.cluster.running_builds_grouped_by_machine
xx.firt
xx.first
xx.first.first
xx.first.first.ip
xx.first.first.max_builds
xx
xx.detect{ |machine,builds| builds.size < machine.max_builds }
xx.detect{ |machine,builds| builds.size < machine.max_builds-1 }
xx.detect{ |machine,builds| builds.size < machine.max_builds }
xx.detect{ |machine,builds| builds.size < machine.max_builds }.first
xx.detect{ |machine,builds| builds.size < machine.max_builds-1 }.first
xx.detect{ |machine,builds| builds.size < machine.max_builds-1 }
xx.select{ |machine,builds| builds.size < machine.max_builds-1 }
xx.to_a.select{ |machine,builds| builds.size < machine.max_builds-1 }
xx.to_a.select{ |machine,builds| builds.size < machine.max_builds }
Pancakes.cluster.next_available_machine_for_build
mach_and_builds = running_builds_grouped_by_machine .select{ |machine,builds| builds.size < machine.max_builds }
Pancakes.cluster.running_builds_grouped_by_machine .select{ |machine,builds| builds.size < machine.max_builds }
Pancakes.cluster.running_builds_grouped_by_machine.select{ |machine,builds| builds.size < machine.max_builds }
Pancakes.cluster.running_builds_grouped_by_machine
Pancakes.cluster.worker_machines
Pancakes.cluster.running_builds
Pancakes.cluster.running_builds_grouped_by_machine
Pancakes.cluster.worker_machines
Pancakes.cluster.worker_machines.map{|e| [e, []] }.to_h
Pancakes.cluster.running_builds_grouped_by_machine
mach_and_builds = running_builds_grouped_by_machine.select{ |machine,builds| builds.size < machine.max_builds }
mach_and_builds = Pancakes.cluster.running_builds_grouped_by_machine.select{ |machine,builds| builds.size < machine.max_builds }
mach_and_builds = Pancakes.cluster.running_builds_grouped_by_machine.select{ |machine,builds| builds.size < machine.max_builds }.sort{|(a_machine,a_builds),(b_machine,b_builds)| a_builds.size <=> b_builds.size }
mach_and_builds = Pancakes.cluster.running_builds_grouped_by_machine.select{ |machine,builds| builds.size < machine.max_builds }.sort{|(a_machine,a_builds),(b_machine,b_builds)| (a_machine.max_builds - a_builds.size) <=> (b_machine.max_builds - b_builds.size) }
mach_and_builds = Pancakes.cluster.running_builds_grouped_by_machine
mach_and_builds = Pancakes.cluster.running_builds_grouped_by_machine.select{ |machine,builds| builds.size < machine.max_builds }.sort{|(a_machine,a_builds),(b_machine,b_builds)| (a_machine.max_builds - a_builds.size) <=> (b_machine.max_builds - b_builds.size) }
mach_and_builds = Pancakes.cluster.running_builds_grouped_by_machine.select{ |machine,builds| builds.size < machine.max_builds }.sort{|(a_machine,a_builds),(b_machine,b_builds)| (b_machine.max_builds - b_builds.size) <=> (a_machine.max_builds - a_builds.size) }
mach_and_builds = Pancakes.cluster.running_builds_grouped_by_machine.select{ |machine,builds| builds.size < machine.max_builds }.sort{|(a_machine,a_builds),(b_machine,b_builds)| (a_machine.max_builds - a_builds.size) <=> (b_machine.max_builds - b_builds.size) }
Pancakes.cluster.next_available_machine_for_build
Pancakes.cluster.running_builds_grouped_by_machine
Pancakes.cluster.worker_machines_by_ip
Pancakes.cluster.running_builds_grouped_by_machine
unit
unit.machine_ip
machines
machines[unit.machine_ip]
machines[unit.machine_ip].add_unit(:foo)
c
unit
c
Pancakes.cluster.running_builds_grouped_by_machine
Pancakes.cluster.worker_machines_with_units
Pancakes.cluster.worker_machines_with_units.first
Pancakes.cluster.worker_machines_with_units.first.builds
Pancakes.cluster.worker_machines_with_units.first
Pancakes.cluster.worker_machines_with_units.first.builds
xx = Pancakes.cluster.worker_machines_with_units.first
xx.units
xx.builds
Pancakes.cluster.running_build_count_by_machine
Pancakes.cluster.running_build_count_by_machine.to_h
xx = Pancakes.cluster.worker_machines_with_units.first
Pancakes.cluster.next_available_machine_for_build
Pancakes.cluster.next_available_machine_for_build.capacity
Pancakes.cluster.next_available_machine_for_build
Pancakes.cluster.worker_machines_with_units
Pancakes.cluster.machines_with_build_capacity
Pancakes.cluster.select_machine_for_build
Pancakes.cluster.select_machine_for_build.capacity
Pancakes.cluster.select_machine_for_build
Integrity.sys_logger('delayed_job')
Delayed::Worker.logger
Delayed::Worker.default_queue_name
Rails.root
Delayed::Job.all
Delayed::Job.all.delete
Delayed::Job.delete
Delayed::Job.destroy
Delayed::Job.destroy!
Delayed::Job.destroy_all
Delayed::Job.all
Pancakes.cluster.select_machine_for_build
Pancakes.cluster.machines_with_build_capacity
Pancakes.cluster.worker_machines_with_build_capacity
xx = Pancakes.cluster.worker_machines_with_build_capacity
xx.first.capacity
xx = Pancakes.cluster.worker_machines_with_build_capacity
Pancakes.cluster.select_machine_for_build
Pancakes.cluster.select_machine_for_build_with_id("806f6acd34d14e29a44809dc81203927")
Pancakes.cluster.select_machine_for_build
xx = Pancakes.cluster.worker_machines_with_builds
xx = Pancakes.cluster.worker_machines_with_units
Pancakes.cluster.select_machine_for_build
xx = []
xx = [:a]
xx
xx[-1]= :b
xx
xx[-1,-1]= :a
xx[0,-1]= :a
xx[0,1]= :a
xx
xx[0,0]= :b
xx
Time.now
Time.now + 10
xx = Time.now
Time.now - xx
xx = Pancakes.cluster.worker_machines_with_units
xx
Pancakes.cluster.worker_machines_with_units.each {|m| puts machine } ; nil
Pancakes.cluster.worker_machines_with_units.each {|m| puts m } ; nil
pending = [1, 2]
pending = [:one, :two]
Pancakes.cluster.worker_machines_with_units.each {|m| puts m } ; nil
xx = Pancakes.cluster.worker_machines_with_units
xx
xx.first.running_builds
xx = [:foo, :bar]
yy = xx
yy << :quux
xx
xx = [:foo, :bar]
yy = xx
yy = Array(xx)
yy << :quux
xx
yy
xx = [:foo, :bar]
yy = xx.clone
yy << :quux
yy
xx
xx = Pancakes.cluster.worker_machines_with_units
xx.first.builds_with_intent
xx = Pancakes.cluster.worker_machines_with_units
xx.first.builds_with_intent
xx.first.capacity
cluster_add_intent(build_id: b.id, type: :build_start, machine: :foo)
Integrity::Builder.cluster_add_intent(build_id: 1, type: :build_start, machine: xx.first)
xx.first.capacity
Integrity::Builder.cluster_intents
Integrity::Builder.cluster_add_intent(build_id: 1, type: :build_start, machine: xx.first)
Integrity::Builder.cluster_intents
xx.first.capacity
Integrity::Builder.cluster_intents
Integrity::Builder.cluster_add_intent(build_id: 1, type: :build_start, machine: xx.first)
Integrity::Builder.cluster_intents
xx.first.capacity
xx = Pancakes.cluster.worker_machines_with_units
xx.first.capacity
Integrity::Builder.cluster_add_intent(build_id: 1, type: :build_start, machine: xx.first)
xx.first.capacity
Integrity::Builder.cluster_intents
xx = Pancakes.cluster.worker_machines_with_units
xx.first.capacity
Integrity::Builder.cluster_add_intent(build_id: 1, type: :build_start, machine: xx.first)
xx.first.capacity
start_intents = Integrity::Builder.cluster_query_intents(type: :build_start, machine_id: self.id)
start_intents = Integrity::Builder.cluster_query_intents(type: :build_start, machine_id: xx.first.id)
start_intents = Integrity::Builder.cluster_query_intents()
start_intents = Integrity::Builder.cluster_query_intents(type: :build_start)
Integrity::Builder.cluster_add_intent(build_id: 1, type: :build_start, machine: xx.first)
xx = Pancakes.cluster.worker_machines_with_units
Integrity::Builder.cluster_add_intent(build_id: 1, type: :build_start, machine: xx.first)
start_intents = Integrity::Builder.cluster_query_intents(type: :build_start)
intent
type
(build_id.nil?   || intent.build_id   == build_id)
(type.nil?       || intent.type       == type)
(machine_id.nil? || intent.machine.id == machine_id)
(not_machine_id.nil? || intent.machine.id == not_machine_id)
(not_machine_id && intent.machine.id == not_machine_id)
xx = Pancakes.cluster.worker_machines_with_units
Integrity::Builder.cluster_add_intent(build_id: 1, type: :build_start, machine: xx.first)
start_intents = Integrity::Builder.cluster_query_intents(type: :build_start)
c
xx = Pancakes.cluster.worker_machines_with_units
start_intents = Integrity::Builder.cluster_query_intents(type: :build_start)
Integrity::Builder.cluster_add_intent(build_id: 1, type: :build_start, machine: xx.first)
start_intents = Integrity::Builder.cluster_query_intents(type: :build_start, machine_id: xx.first.id)
xx.first.capacity
xx = Pancakes.cluster.worker_machines_with_units
Integrity::Builder.cluster_add_intent(build_id: 1, type: :build_start, machine: xx.first)
xx.first.capacity
xx = Pancakes.cluster.worker_machines_with_units
xx.first.capacity
xx = Pancakes.cluster.worker_machines_with_units
Integrity::Builder.cluster_add_intent(build_id: 11364, type: :build_destroy)
xx.first.capacity
xx = Pancakes.cluster.worker_machines_with_units
xx.first.capacity
xx = Pancakes.cluster.worker_machines_with_units
xx.first.capacity
Integrity::Builder.cluster_add_intent(build_id: 11364, type: :build_destroy)
xx.first.capacity
Integrity::Builder.cluster_add_intent(build_id: 11365, type: :build_destroy)
xx.first.capacity
xx.first.builds
xx.first.builds.size
xx = Pancakes.cluster.worker_machines_with_units
Integrity::Builder.cluster_add_intent(build_id: 11367, type: :build_destroy)
xx.first.builds.size
xx.first.builds
xx = Pancakes.cluster.worker_machines_with_units.last
xx.builds
xx.builds.size
xx.capacity
build_info.build_id,
build_info.build_id
self.id
Integrity::Builder.cluster_query_intents(build_id: build_info.build_id, type: :build_destroy, machine_id: self.id)
Integrity::Builder.cluster_intents
xx = Pancakes.cluster.worker_machines_with_units.last
xx.capacity
Integrity::Builder.cluster_add_intent(build_id: 11367, type: :build_destroy)
xx.capacity
Pancakes::Stack.all
Pancakes::Stack.all.units
Pancakes::Stack.first.units.first
Pancakes::Stack.first.units.first.dumb_sibling_name
Pancakes.cluster.start_template(unit.dumb_sibling_service_file_name, Pancakes::Fleet::Templating.dumb_sibling_template(unit))
Pancakes::Fleet::Templating.dumb_sibling_template(Pancakes::Stack.first.units.first)
Pancakes::Stack.first.units.first.name
Pancakes::Stack.first.units.first.stack.name
Pancakes::Stack.first.units.first.name
Pancakes::Stack.first.units.first.service_name
xx = Pancakes::Stack.create_with_git_strings(services: Pancakes::Services.full_services)
xx.commit
xx.commits
xx.units.last
xx.units
xx.units[3]
yy = xx.units[3]
yy
env
ENV
yy.env
yy.stack
yy.stack.jarvis_env
Pancakes::Fleet::Templating::Helpers.env_str(yy.env)
yy = xx.units[6]
Pancakes::Fleet::Templating::Helpers.env_str(yy.env)
Integrity.start_service(:metrics, 'dev')
Integrity.cluster.        list_machines.select(&:etcd?)
Integrity.cluster.list_machines.select{|e| e.metadata.role == 'etcd' }
Integrity.cluster.list_machines.select{|e| e.metadata.role == 'etcd' }.map(&:ip)
Integrity.cluster.etcdctl_peers
Integrity.start_metrics
Integrity.start_metrics(image_tag: 'dev')
Integrity.destroy_metrics_units
Integrity.start_metrics(image_tag: 'dev')
cl = Pancakes.cluster
ed = cl.etcd
ed.get_all_metrics
@client.get(METRICS_KEY, recursive: true)
rr = @client.get(METRICS_KEY, recursive: true)
pp rr
rr
rr.node.children
cl = Pancakes.cluster
cl = Pancakes.cluster ; ed = cl.etcd
ed.get_all_metrics
metrics
metrics.value
JSON.parse(metrics.value)
Integrity.start_service(:metrics, 'dev')
Integrity.start_metrics(image_tag: 'dev')
cl = Pancakes.cluster ; ed = cl.etcd
ed.get_all_metrics
n
c
cl = Pancakes.cluster ; ed = cl.etcd
ed.get_all_metrics
Metrics.new(JSON.parse(metrics_node.value))
Metrics.new(JSON.parse(metrics_node.value)).memfree
cl = Pancakes.cluster ; ed = cl.etcd
ed.get_all_metrics
Metrics.new(JSON.parse(metrics_node.value)).memfree
Metrics.new(JSON.parse(metrics_node.value)).load
Metrics.new(JSON.parse(metrics_node.value)).load1
Metrics.new(JSON.parse(metrics_node.value)).load5
Metrics.new(JSON.parse(metrics_node.value)).load15
cl = Pancakes.cluster ; ed = cl.etcd
ed.get_all_metrics
cl = Pancakes.cluster ; ed = cl.etcd
ed.get_all_metrics
metrics_node
metrics_node.key
metrics_node.key.split('/').last
"asdf.asdf"['.']
!! "asdf.asdf"['.']
!! "asdfxasdf"['.']
mm = Pancakes.cluster.worker_machines
mm = mm.first
mm
mm.clear_cache_for_codebase("foo")
%Q[ssh core@#{ip} "sudo find /var/lib/docker/tmp/bundler -maxdepth 1 -name #{codebase}-* -exec rm -rfv {} \;"]
c
%Q[ssh core@#{ip} "sudo find /var/lib/docker/tmp/bundler -maxdepth 1 -name #{codebase}-* -exec rm -rfv {} \;"]
mm.clear_cache_for_codebase("foo")
%Q[ssh core@#{ip} "sudo find /var/lib/docker/tmp/bundler -maxdepth 1 -name #{codebase}-* -exec rm -rfv {} \;"]
puts %Q[ssh core@#{ip} "sudo find /var/lib/docker/tmp/bundler -maxdepth 1 -name #{codebase}-* -exec rm -rfv {} \;"]
system %Q[ssh core@#{ip} "sudo find /var/lib/docker/tmp/bundler -maxdepth 1 -name #{codebase}-* -exec rm -rfv {} \;"]
system %Q[ssh core@#{ip} 'sudo find /var/lib/docker/tmp/bundler -maxdepth 1 -name #{codebase}-* -exec rm -rfv {} \;']
system %Q[ssh core@#{ip} "sudo find /var/lib/docker/tmp/bundler -maxdepth 1 -name foo-*"]
system %Q[ssh core@#{ip} "sudo find /var/lib/docker/tmp/bundler -maxdepth 1 -name foo-* -exec rm -rfv {} \;"]
system %Q[ssh core@#{ip} "sudo find /var/lib/docker/tmp/bundler -maxdepth 1 -name foo-* -exec rm -rfv {} \\;"]
mm = Pancakes.cluster.worker_machines
mm = mm.first
mm.clear_cache_for_codebase("foo")
mm = Pancakes.cluster.worker_machines
mm = Pancakes.cluster.worker_machines.first
mm.clear_machine_cache("foo")
mm = Pancakes.cluster.worker_machines.first
mm.clear_machine_cache("foo")
mm.clear_cache("foo")
mm = Pancakes.cluster.worker_machines.first
mm.clear_cache("foo")
mm = Pancakes.cluster.worker_machines.first
mm.clear_cache("foo")
mm = Pancakes.cluster.worker_machines.first
mm.clear_cache("foo")
mm = Pancakes.cluster.worker_machines.first
mm.clear_cache("foo")
mm = Pancakes.cluster.worker_machines.first
mm.clear_cache("foo")
HonestyCluster::BundleCache.clear_aws_cache(codebase: 'foo')
s3_development
HonestyCluster::BundleCache.clear_aws_cache(env: :development, codebase: 'foo')
n
s3_client.head_object(bucket: BUCKETS[env], key: cache_key(codebase))
xx = s3_client.head_object(bucket: BUCKETS[env], key: cache_key(codebase))
xx
xx.class
xx.name
xx.pager
w
c
HonestyCluster::BundleCache.clear_aws_cache(env: :development, codebase: 'foo')
c
Pancakes.cluster.worker_machines
Pancakes.cluster.worker_machine_ips
HonestyCluster::BundleCache.clear_cache(codebase: 'foo')
Integrity.cluster.running_build_count_by_machine.each do |machine_ip,count|
Integrity.cluster.running_build_count_by_machine
Pancakes.cluster.client.list_machines
Pancakes.cluster.client.list_machines.first
Pancakes.cluster.client.list_machines.first.last
Pancakes.cluster.client.list_machines.first.last.to_s
puts "Pancakes.cluster.client.list_machines.first.last.to_s
puts "#{Pancakes.cluster.client.list_machines.first.last.to_s}"
puts "#{Pancakes.cluster.client.list_machines.first.last}"
Pancakes.cluster.client.list_units
create_droplet_with_dns('foo')
dd = last_droplet
write_inventory_file(dd)
"xxxxxxxxat".include?("at")
[].first
domain_records_matching('foo')
domain_records_matching('foo-')
xx = domain_records_matching('foo-')
a_record(xx.first)
a_record(last_droplet)
write_inventory_file
dd = last_droplet
delete_droplet_and_dns(dd)
write_inventory_file
HonestyCluster::Turds.deturd_all
cluster.workers
cluster.worker_machine_ips
self.cluster.worker_machine_ips
HonestyCluster::Turds.cluster.worker_machine_ips
HonestyCluster.cluster.worker_machine_ips
HonestyCluster::Turds.deturd_reset_failed
HonestyCluster.cluster.dry_run = false
HonestyCluster::Turds.deturd_reset_failed
HonestyCluster.cluster.dry_run?
HonestyCluster::Turds.deturd_orphaned_mysqls
HonestyCluster.cluster.worker_machine_ips
HonestyCluster::Turds.deturd_orphaned_mysqls
`ls`
`ls | wc -l`
HonestyCluster::Turds.deturd_orphaned_mysqls
HonestyCluster::Turds.deturd_fleet_units_for_repos_which_cant_build
cluster.units
HonestyCluster.cluster.units
HonestyCluster.cluster.build_units
HonestyCluster::Turds.deturd_fleet_units_for_repos_which_cant_build
unit.name
REPOS_WHICH_CANT_BUILD
exit
q!
HonestyCluster::Turds.deturd_fleet_units_for_repos_which_cant_build
REPOS_WHICH_CANT_BUILD
unit
REPOS_WHICH_CANT_BUILD.map{|repo| unit.name.start_with?("#{repo}_") }
REPOS_WHICH_CANT_BUILD.map{|repo| unit.name.start_with?("#{repo}_") }.reduce(false){|memo,val| memo || val }
REPOS_WHICH_CANT_BUILD.map{|repo| unit.name.start_with?("#{repo}_x") }.reduce(false){|memo,val| memo || val }
REPOS_WHICH_CANT_BUILD.map{|repo| unit.name.start_with?("#{repo}_") }.reduce(false){|memo,val| memo || val }
HonestyCluster::Turds.deturd_fleet_units_for_repos_which_cant_build
fl
%i[x x]
puts "hi"
puts
send("puts")
send("puts", "hi")
HonestyCluster::Turds.deturd_fleet_failed_units
unit
q!
quit-all
h
help
!!@
HonestyCluster::Turds.deturd_fleet_failed_units
HonestyCluster::Turds.deturd_all
Pancakes.cluster.build_units
xx = Pancakes.cluster.build_units
Pancakes.cluster.client.list.size
Pancakes.cluster.client
Pancakes.cluster.client.list.size
bb = Integrity::Build.last
bb
bb.branch
Integrity.cluster.start_build(bb)
bb = Integrity::Build.last
Integrity.cluster.start_build(bb)
Integrity.cluster.destroy_build(bb)
Integrity::Project.last
"foob.ar".end_with?("ar")
[[:a, :b], :c].flatten
[[{a:1, b:2}], :c].flatten
[[{a:1, b:2}], {c:3}].flatten
[[{a:1}, {b:2}], {c:3}].flatten
[[{a:1}, {b:2},{d:[4,5]}], {c:3}].flatten
bb = Integrity::Build.last
Integrity.cluster.start_build(bb)
Integrity.cluster.destroy_build(bb)
[].first
bb = Integrity::Build.last
Integrity.cluster.start_build(bb)
$!
bb = Integrity::Build.last
Integrity.cluster.start_build(bb)
bb = Integrity::Build.last
Integrity.cluster.start_build(bb)
service_def[:service_name]
service_def
bb = Integrity::Build.last
Integrity.cluster.start_build(bb)
service_def
bb = Integrity::Build.last
Integrity.cluster.start_build(bb)
Integrity.cluster.destroy_build(bb)
pp = Integrity::Project.get(11)
pp.last_build
bb = pp.last_build
Integrity.cluster.start_build(bb)
Integrity.cluster.destroy_build(bb)
Integrity.cluster.start_build(bb)
Integrity.cluster.destroy_build(bb)
pp
pp = Integrity::Project.get(11)
bb = pp.last_build
bb.commit
b2 = pp.build(bb.commit, true)
Integrity.cluster.start_build(b2)
pp = Integrity::Project.get(11)
b2 = pp.build(bb.commit, true)
bb = pp.last_build
b2 = pp.build(bb.commit, true)
Integrity.cluster.start_build(b2)
unit_fname
unit_hash
pp = Integrity::Project.get(11)
bb = pp.last_build
b2 = pp.build(bb.commit, true)
Integrity.cluster.start_build(b2)
c
pp
pp.datastore = 'postgres'
pp.save!
Integrity.cluster.destroy_build(b2)
pp.datastore = nil
pp.save
Integrity.cluster.destroy_build(b2)
pp.datastore = 'postgres'
pp.save
Integrity.cluster.start_build(b2)
c
pp = Integrity::Project.get(11)
bb = pp.last_build
b2 = pp.build(bb.commit, true)
Integrity.cluster.start_build(b2)
HonestyCluster::SwarmBackend.docker_stuff
Docker::Container.all({}, conn)
xx = Pancakes::Stack.create_with_git_strings(services: Pancakes::Services.full_services)
xx.units
xx = Pancakes::Stack.last
xx.swarm_bring_up
SwarmBackend.conn
Docker::Container.all({}, SwarmBackend.conn)
xx = Pancakes::Stack.last
xx.swarm_bring_up
xx = Pancakes::Stack.last
xx.swarm_bring_up
xx = Pancakes::Stack.last
xx.swarm_bring_up
s
f
c
xx = Pancakes::Stack.last
xx.swarm_bring_up
xx = Pancakes::Stack.last
xx.swarm_bring_up
container
container.info
container.start
xx = Pancakes::Stack.last
xx.swarm_bring_up
xx = Pancakes::Stack.last
xx.swarm_bring_up
xx = Pancakes::Stack.last
xx.swarm_bring_up
c
xx = Pancakes::Stack.last
xx.swarm_bring_up
c
xx = Pancakes::Stack.last
xx.swarm_bring_up
c
cc
c
xx = Pancakes::Stack.last
xx.swarm_bring_up
xx = Pancakes::Stack.last
xx.swarm_bring_up
c
xx = Pancakes::Stack.last
xx.swarm_bring_up
c
xx = Pancakes::Stack.last
xx.swarm_bring_up
xx = Pancakes::Stack.last
xx.swarm_bring_up
xx = Pancakes::Stack.last
xx.swarm_destroy
Docker::Container.all({}, SwarmBackend.conn)
cc = Docker::Container.all({}, SwarmBackend.conn)
cc.first
cc.first.info
cc.first.info.names
Hashie::Mash.new cc.first.info
Hashie::Mash.new(cc.first.info).Names
Hashie::Mash.new(cc.first.info).Names.first
Hashie::Mash.new(cc.first.info).Ports.first
Hashie::Mash.new(cc.first.info).Ports.IP
Hashie::Mash.new(cc.first.info).Ports
Hashie::Mash.new(cc.first.info).Ports.first.IP
Hashie::Mash.new(cc.first.info).Ports
xx = Pancakes::Stack.last
xx.swarm_destroy
cc = Docker::Container.all({}, SwarmBackend.conn)
cc.first.name
cc.first.info.names.first
cc.first.info.Names.first
xx = Pancakes::Stack.last
xx.swarm_destroy
cc = Docker::Container.all({}, SwarmBackend.conn)
cc.first.name
cc.first.name.split('/')
cc.first.name.sub(/^\//, '').split('/')
xx = Pancakes::Stack.last
xx.swarm_destroy
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx.swarm_destroy
SwarmBackend.all_containers(filters'{"foo":"label=\"pancakes-stack=pancakes_de95317_322\""}')
xx = Pancakes::Stack.last
xx.swarm_destroy
SwarmBackend.all_containers(filters'{"foo":"label=\"pancakes-stack=pancakes_de95317_322\""}')
SwarmBackend.all_containers(filters: '{"foo":"label=\"pancakes-stack=pancakes_de95317_322\""}')
SwarmBackend.all_containers(filters: '{"label":"pancakes-stack=pancakes_de95317_322"}')
SwarmBackend.all_containers(filters: {"label" => "pancakes-stack=pancakes_de95317_322" })
SwarmBackend.all_containers(filters: {"label" => "pancakes-stack" })
SwarmBackend.all_containers(filters: {"label=pancakes-stack" => '' })
SwarmBackend.all_containers(filters: {'filter' => "label=pancakes-stack"  })
SwarmBackend.all_containers(filters: ["label=pancakes-stack"] })
SwarmBackend.all_containers(filters: ["label=pancakes-stack"])
SwarmBackend.all_containers(filters: '["label=pancakes-stack"]')
SwarmBackend.all_containers(filters: '{"f": "label=pancakes-stack"}')
SwarmBackend.all_containers(filters: '{"label": "pancakes-stack"}')
SwarmBackend.all_containers(filters: {"label" => ["pancakes-stack"]})
SwarmBackend.all_containers(filters: {"label" => ["pancakes-stack"] })
SwarmBackend.all_containers(filters: {"label" => ["pancakes-stack=foo"] })
SwarmBackend.all_containers(filters: %Q[ {"label": ["pancakes-stack=foo"] }])
SwarmBackend.all_containers(filters: %Q[ {"label": ["pancakes-stack=pancakes_de95317_322"] }])
SwarmBackend.all_containers()
SwarmBackend.all_containers(filters: JSON.dump({"label" => ["pancakes-stack=pancakes_de95317_322"] }))
xx = Pancakes::Stack.last
xx.swarm_destroy
xx = Pancakes::Stack.last
xx.swarm_destroy
cc
cc.first.stop
cc.remove
cc.first.remove
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx.swarm_destroy
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx.swarm_destroy
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx = Pancakes::Stack.last
xx.swarm_destroy
xx.swarm_spin_up
xx.swarm_destroy
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx.swarm_destroy
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx.swarm_destroy
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx.swarm_destroy
xx.swarm_spin_up
xx.swarm_destroy
xx.swarm_spin_up
xx = Pancakes::Stack.last
xx.swarm_destroy
xx.swarm_spin_up
xx.units.last
xx.units[5]
xx.units[4]
xx.units[4].ports
xx.units[3].ports
xx.units[2].ports
xx.units[`].ports
xx.units[1].ports
xx.swarm_destroy
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx.swarm_destroy
xx.containers
xx = Pancakes::Stack.last
xx.swarm_destroy
xx.swarm_spin_up
xx = Pancakes::Stack.last
xx.swarm_destroy
xx.swarm_spin_up
xx = Pancakes::Stack.last
xx.swarm_destroy
xx.swarm_spin_up
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx.swarm_destroy
xx.swarm_spin_up
xx = Pancakes::Stack.last
Diplomat::Service.get_all
Diplomat::Services.get_all
Diplomat::Service.get_all
Diplomat::Service
Diplomat::Services
Diplomat::Service.get_all
Diplomat::Service.new.get_all
Diplomat::Service.methods
Diplomat::Service.get
Diplomat::Service.get('mysql')
Diplomat::Service.get_all
Diplomat::Service.get_all.mysql
Diplomat::Service.get('mailcatcher')
xx = Pancakes::Stack.last
xx.units
xx.units.map{|u| u.image }
xx.units[3]
xx.units[4]
xx.units[2]
xx.units[1]
xx.units[1].ports
xx.units[2].ports
xx.units[3].ports
"grnds/foo:latest".split(/[\/:]/)
"grnds/foo".split(/[\/:]/)
"foo:latest".split(/[\/:]/)
"grnds/foo:latest".split(/[\/:]/)
"grnds/foo:latest".split(/[\/:]/)[1]
xx.units[3].image
[23, 3].join
xx = Pancakes::Stack.last
xx.units[1].consul_service_names
xx = Pancakes::Stack.last
xx.consul_services
xx
xx.consul_services
xx = Pancakes::Stack.last
xx.consul_services
xx = Pancakes::Stack.last
xx.consul_services
service_instances
service_instances.class
xx = Pancakes::Stack.last
xx.consul_services
xx = Pancakes::Stack.last
xx.consul_services
xx = Pancakes::Stack.last
xx.consul_services
xx.units[2].service_type
yy = xx.units[2]
yy.service_name
xx = Pancakes::Stack.last
xx.service_addrs
xx = Pancakes::Stack.last
xx.service_addrs
xx.swarm_service_addrs
xx = Pancakes::Stack.last
xx.swarm_service_addrs
xx = Pancakes::Stack.last
xx.swarm_service_addrs
xx = Pancakes::Stack.last
xx.swarm_service_addrs
xx = Pancakes::Stack.last
xx.swarm_service_addrs
xx.service_addrs
xx = Pancakes::Stack.last
xx.service_addrs
xx = Pancakes::Stack.last
xx.service_addrs
xx = Pancakes::Stack.last
xx.service_addrs
xx = Pancakes::Stack.last
xx.service_addrs
xx = Pancakes::Stack.last
xx.service_addrs
xx.units.first.active?
xx = Pancakes::Stack.last
xx.service_addrs
xx.units.first.active?
xx.unit_with_service_name(:mysql)
uu = xx.unit_with_service_name(:mysql)
uu
uu && uu.active?
xx = Pancakes::Stack.last
uu = xx.unit_with_service_name(:mysql)
uu.name
uu.service_name
uu.service_type
uu = xx.unit_with_service_name(:javris)
uu = xx.unit_with_service_name(:jarvis)
uu = xx.unit_with_service_name('jarvis-server')
uu.service_type
uu.service_name
xx.units
xx = Pancakes::Stack.last
xx.swarm_destroy
xx.swarm_spin_up
unit.service_name
bash_minus_c
Templating.bash_minus_c
Pancakes::Templating.bash_minus_c
Pancakes::Fleet::Templating.bash_minus_c
xx.units.map(&:service_def)
q!
xx.units.map(&:service_def)
xx = Pancakes::Stack.last
xx.swarm_destroy
xx.swarm_spin_up
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx.swarm_destroy
xx.swarm_spin_up
c
xx = Pancakes::Stack.last
xx.swarm_destroy
xx.swarm_spin_up
cnd
cmd
c
cmd
c
cmd
c
foo = "/bin/bash -c 'git clone --depth 1 --branch master https://112999f62d2234aabcabef728f5726ba28c2155a@github.com/consultingmd/timberland timberland && cd timberland && ./timberland --boot-type pancakes -s f59ef1b4f84ae5aa4ca6b45ea21adfba6e43f4af -r consultingmd/consultingmd --service-type server -b master |& sed -u -e\"s/^/[pancakes_de95317_322_jarvis-server] /\"'"
foo
puts foo
foo = "/bin/bash -c 'git clone --depth 1 --branch master https://112999f62d2234aabcabef728f5726ba28c2155a@github.com/consultingmd/timberland timberland && cd timberland && ./timberland --boot-type pancakes -s f59ef1b4f84ae5aa4ca6b45ea21adfba6e43f4af -r consultingmd/consultingmd --service-type server -b master |& sed -u -e\"s/^/[pancakes_de95317_322_jarvis-server] /\"'".sub('\', '\\')
foo = "/bin/bash -c 'git clone --depth 1 --branch master https://112999f62d2234aabcabef728f5726ba28c2155a@github.com/consultingmd/timberland timberland && cd timberland && ./timberland --boot-type pancakes -s f59ef1b4f84ae5aa4ca6b45ea21adfba6e43f4af -r consultingmd/consultingmd --service-type server -b master |& sed -u -e\"s/^/[pancakes_de95317_322_jarvis-server] /\"'".sub('\', '\\\\')
foo = "/bin/bash -c 'git clone --depth 1 --branch master https://112999f62d2234aabcabef728f5726ba28c2155a@github.com/consultingmd/timberland timberland && cd timberland && ./timberland --boot-type pancakes -s f59ef1b4f84ae5aa4ca6b45ea21adfba6e43f4af -r consultingmd/consultingmd --service-type server -b master |& sed -u -e\"s/^/[pancakes_de95317_322_jarvis-server] /\"'".sub('\\', '\\\\')
foo = "/bin/bash -c 'git clone --depth 1 --branch master https://112999f62d2234aabcabef728f5726ba28c2155a@github.com/consultingmd/timberland timberland && cd timberland && ./timberland --boot-type pancakes -s f59ef1b4f84ae5aa4ca6b45ea21adfba6e43f4af -r consultingmd/consultingmd --service-type server -b master |& sed -u -e\"s/^/[pancakes_de95317_322_jarvis-server] /\"'".sub('\', '\\')
foo
foo = "/bin/bash -c 'git clone --depth 1 --branch master https://112999f62d2234aabcabef728f5726ba28c2155a@github.com/consultingmd/timberland timberland && cd timberland && ./timberland --boot-type pancakes -s f59ef1b4f84ae5aa4ca6b45ea21adfba6e43f4af -r consultingmd/consultingmd --service-type server -b master |& sed -u -e\"s/^/[pancakes_de95317_322_jarvis-server] /\"'"
puts foo
puts foo.sub('\', '\\')
puts foo.sub('\', '\\\\')
puts foo.sub('\', '\\\')
puts foo.sub('\\', '\\\')
puts foo.sub("\\", "\\\\)
puts foo.sub("\\", "\\\\")
xx = Pancakes::Stack.last
xx.swarm_destroy
xx.swarm_spin_up
cmd
c
cmd
c
cmd
c
xx = Pancakes::Stack.last
xx.swarm_destroy
xx.swarm_spin_up
cmd
c
cmd
c
cmd
cmd = "which ls"
c
xx = Pancakes::Stack.last
xx.swarm_destroy
xx.swarm_spin_up
c
cmd
cmd = [cmd]
c
xx = Pancakes::Stack.last
xx.swarm_destroy
xx = Pancakes::Stack.last
xx.swarm_spin_up
xx = Pancakes::Stack.last
xx.swarm_destroy
xx.swarm_destroy ; xx.swarm_spin_up
xx = Pancakes::Stack.last
xx.swarm_destroy ; xx.swarm_spin_up
cmd
c
cmd
cm
c
cmd
cmd.split("'")
cmd.split("'")[1]
foo = cmd.split("'")[1]
cmd = ['/bin/bash', '-c', foo]
c
cmd
foo = cmd.split(" ")
cmd = cmd.split(" ")
c
cmd = cmd.split(" ")
cmd
cmd[2] = 'git'
cmd
cmd[24] = 'master'
cmd
c
cmd
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
cmd
c
cmd
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
c
cm
cmd
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
cmd
c
cmd
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
c
cmd
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
cmd
c
cmd
Docker::Container.create(container_json(unit, ["/bin/bash", '-c' 'ls']), conn)
foo = Docker::Container.create(container_json(unit, ["/bin/bash", '-c' 'ls']), conn)
foo.start
Docker::Container.create(container_json(unit, ["/bin/bash", '-c', 'ls']), conn).start
Docker::Container.create(container_json(unit, ["/bin/bash", '-c', 'git', 'status']), conn).start
cmd
cmd.sub("'", '')
cmd.sub("'", '').split(/\s+/)
cmd.gsub("'", '').split(/\s+/)
Docker::Container.create(container_json(unit, cmd.gsub("'", '').split(/\s+/)), conn).start
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
cmd
c
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
cmd2
cmd2[0..9]
list=
list
l
whereami
Docker::Container.create(container_json(unit, cmd2[0..9]), conn).start
Docker::Container.create(container_json(unit, ['/bin/bash', '-c', 'git status']), conn).start
cmd
cmd.split("'")
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
cmd
Docker::Container.create(container_json(unit, cmd), conn).start
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
Docker::Container.create(container_json(unit, cmd), conn).start
ENV
Docker::Container.create(container_json(unit, "env"), conn).start
ENV['GITHUB_READ_TOKEN']
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
Docker::Container.create(container_json(unit, cmd), conn).start
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
Docker::Container.create(container_json(unit, cmd), conn).start
ENV
ENV['SWARM_MASTER']
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
cmd
c
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
[].reduce(&:merge)
{}.reduce(&:merge)
{}.reduce(&:merge).to_h
nil.to_h
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
docker_api_exposed_ports(unit)
docker_api_port_bindings(unit)
docker_api_port_bindings(unit).to_h
docker_api_port_bindings(unit).reduce(&:merge)
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
docker_api_port_bindings(unit)
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
docker_api_port_bindings(unit)
c
{a: 3}.merge(nil)
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
unit.container
unit
unit.name
unit.service_name
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx
xx.units
xx = Pancakes::Stack.last 
xx.units
yy = xx.units.last
yy
cc = xx.containers
Pancakes::Unit.get_unit_from_container(cc[10].name)
xx = Pancakes::Stack.last 
cc = xx.containers
HonestyCluster::SwarmBackend.get_unit_from_container(cc[10].name)
container
Pancakes::Unit.parse_unit_name(container)
xx = Pancakes::Stack.last 
cc = xx.containers
HonestyCluster::SwarmBackend.get_unit_from_container(cc[10].name)
cc = xx.containers
Pancakes::Unit.parse_unit_name(container)
xx = Pancakes::Stack.last 
cc = xx.containers
HonestyCluster::SwarmBackend.get_unit_from_container(cc[10].name)
xx = Pancakes::Stack.last 
cc = xx.containers
HonestyCluster::SwarmBackend.get_unit_from_container(cc[10].name)
xx = Pancakes::Stack.last 
cc = xx.containers
HonestyCluster::SwarmBackend.get_unit_from_container(cc[10].name)
HonestyCluster::SwarmBackend.get_unit_from_container(cc[10].name).first
xx
xx.units
xx.units_dataset
xx.units_dataset[3506]
xx = Pancakes::Stack.last 
xx.units_dataset[3506]
xx = Pancakes::Stack.last 
xx.units_dataset
xx = Pancakes::Stack.last 
xx.units_dataset
xx.units_dataset.update(machine_ip: nil, port: nil)
xx.units
xx.refresh_unit_addrs
c
xx.refresh_unit_addrs
_containers
_containers.map{|container| { Pancakes::Unit.parse_unit_name(container.name).service_name => container} }
foo = _containers.map{|container| { Pancakes::Unit.parse_unit_name(container.name).service_name => container} }
foo.first
foo.first.first
foo.first.last.port
foo.first.last.port_mapping
foo.first
foo.first.last
foo.first.to_a.last
foo.first.to_a.last.last
foo.first.to_a.last.last.port_mapping
foo.first.to_a.last.last.port_mapping.first
foo.first.to_a.last.last.port_mapping.first.last
foo.first.to_a.last.last.port_mapping.first[1000]
foo.first.to_a.last
foo.first.to_a.last.last
foo.first.to_a.last.last.Ports
foo.first.to_a.last.last.info.Ports
foo.first.to_a.last.last.info.Ports.first
xx = Pancakes::Stack.last 
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
mapped_containers
c
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.refresh_unit_addrs
xx = Pancakes::Stack.last 
xx.refresh_unit_addrs
Integrity.base_url
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.refresh_unit_addrs
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
ENV
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.swarm_destroy
xx
xx.service_addrs
!nil
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.service_addrs
"https://foo.bar".match(/^http/)
"http://foo.bar".match(/^http/)
!!"http://foo.bar".match(/^http/)
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.service_addrs
xx.refresh_unit_addrs
xx.service_addrs
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
"https:foo.bash".match(/^http/) ? 'true' : 'fuck'
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.service_addrs
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx = Pancakes::Stack.create_with_git_strings(services: Pancakes::Services.full_services)
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.service_addrs
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.service_addrs
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.units
cc = xx.containers
cc.each {|c| puts c.Status }
cc.each {|c| puts c.info.Status }
cc.each {|c| puts c.exited? }
xx = Pancakes::Stack.last 
cc = xx.containers
cc.each {|c| puts c.exited? }
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx = Pancakes::Stack.last ; xx.swarm_destroy 
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.service_addrs
xx = Pancakes::Stack.last ; xx.swarm_destroy 
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.service_addrs
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.id
xx.containers.map(&:status)
xx.containers.map(&:nane)
xx.containers.map(&:name)
xx.containers.map{|c|[c.name,c.status]}
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.units
xx = Pancakes::Stack.last ; xx.swarm_destroy 
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.containers.map{|c|[c.name,c.status]}
xx.units
xx.units[6]
xx.units[6].service_subtype
xx.units[6].service_subnanme
xx.units[6].service_subname
xx.units[6].subtype
xx = Pancakes::Stack.last 
xx.containers
xx = Pancakes::Stack.last 
xx.containers.map(&:exit_code)
0 == false
xx = Pancakes::Stack.last 
xx.containers.map(&:exit_code)
xx = Pancakes::Stack.last 
xx.containers.map(&:container_nominal?)
xx.containers.map(&:'container_nominal?')
xx.containers.map(&:"container_nominal?")
xx.containers.map{|x| x.container_nominal? }
xx.units.map{|x| x.container_nominal? }
xx.units.map{|x| puts x }
xx.units.map{|x| puts x.name }
xx.units.map{|x| [x.name, x.container] }
xx = Pancakes::Stack.last 
xx.units.map{|x| [x.name, x.container] }
xx = Pancakes::Stack.last 
xx.units.map{|x| [x.name, x.container_nominal?] }
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.units.map{|x| [x.name, x.container_nominal?] }
xx.units[3]
xx.units[3].container
xx.units[2].container
xx.units[2]
xx.units[2].ports
xx.units[3].ports
xx.units[2].ports
xx.units[1].ports
xx.units[1].ping_url
xx.units[4].ping_url
xx.units[3].ping_url
xx.units[2].ping_url
xx.units.map{|x| [x.name, x.container_nominal?] }
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
yy = xx.units.map(&:"container_nominal?")
xx.units.map{|x| [x.name, x.container_nominal?] }
yy = xx.units.map{|x| [x.name, x.container_nominal?] }
xx = Pancakes::Stack.last ; 
yy = xx.units.map{|x| [x.name, x.container_nominal?] }
yy
yy.reduce(true){|memo,val| memo && val}
yy = xx.units.map{|x| x.container_nominal? }
yy.reduce(true){|memo,val| memo && val}
xx = Pancakes::Stack.last ; 
xx.service_containers_nominal?
xx = Pancakes::Stack.last ; 
xx.healthy?
self.units.select(:"pingable?")
self.units.select(&:"pingable?")
xx = Pancakes::Stack.last ; 
xx.healthy?
unit
unit.ping
q!
xx = Pancakes::Stack.last ; 
xx.healthy?
unit.ping
q!
quit-all
exit-program
xx = Pancakes::Stack.last ; \
xx = Pancakes::Stack.last \
xx = Pancakes::Stack.last 
xx.healthy?
unit.ping
xx = Pancakes::Stack.last 
xx.healthy?
xx = Pancakes::Stack.last 
xx.healthy?
xx = Pancakes::Stack.last 
xx.healthy?
xx = Pancakes::Stack.last 
xx.healthy?
xx = Pancakes::Stack.last 
xx.healthy?
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.healthy?
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.healthy?
xx = Pancakes::Stack.last 
xx.healthy?
type
xx.units
xx = Pancakes::Stack.last 
xx.events
xx = Pancakes::Stack.last 
xx.events
xx = Pancakes::Stack.last 
xx.events
xx = Pancakes::Stack.last 
xx.healthy?
self.events
self.add_event()
self.add_event(foo: 'bar')
self.add_event(source: 'honesty', type: 'Pancakes::Service::HealthCheck', status: 'HEALTH_CHECK_COMPLETE')
exit-program
xx = Pancakes::Stack.last 
xx.healthy?
self.add_event(source: 'honesty', type: 'Pancakes::Service::HealthCheck', status: 'HEALTH_CHECK_COMPLETE')
self.events
xx = Pancakes::Stack.last 
xx.units.first.events
xx.units
xx.units[2]
xx.units[2].events
xx.units[2].events_dataset.destroy
xx.units[2].events
xx.units[2].events_dataset.destroy_all
xx.units[2].events
xx.units[2].events.reload
xx.units[2].reload
xx.units[2].events
xx = Pancakes::Stack.last 
xx.service_events
units.map(&:events)
xx = Pancakes::Stack.last 
xx.check_health
xx = Pancakes::Stack.last 
xx.check_health
xx.service_events
units.map(&:events)
units.map(&:events).reject(&:"empty?")
units.map(&:events).flatten
units.map(&:events).flatten.sort{|a,b| a.created_at <=> b.created_at }
events
xx = Pancakes::Stack.last 
xx.all_events
xx = Pancakes::Stack.last 
xx.all_events
xx.clear_events
xx.all_events
xx.reload
xx.all_events
xx = Pancakes::Stack.last 
xx.check_health
xx.all_events
xx.events_dataset.where(type: 'Pancakes::Stack::HealthCheck').limit(1)
xx.events_dataset.where(type: 'Pancakes::Stack::HealthCheck').limit(10)
xx.check_health
xx.events_dataset.where(type: 'Pancakes::Stack::HealthCheck').limit(10)
xx.events_dataset.where(type: 'Pancakes::Stack::HealthCheck').limit(1)
xx.events_dataset.where(type: 'Pancakes::Stack::HealthCheck').order(:created_at.desc).limit(1)
xx.events_dataset.where(type: 'Pancakes::Stack::HealthCheck').reverse_order(:created_at).limit(1)
xx = Pancakes::Stack.last 
xx.last_health_check
xx = Pancakes::Stack.last 
xx.last_health_check
xx.last_health_check.complete?
xx.last_health_check.first.complete?
xx = Pancakes::Stack.last 
xx.last_health_check.complete?
xx = Pancakes::Stack.last 
xx.last_health_check.complete?
xx.last_health_check
xx.last_health_check.complete?
xx = Pancakes::Stack.last 
xx.last_health_check.complete?
xx = Pancakes::Stack.last 
xx.units.first.name
xx.units.first.codebase
xx.units[2].codebase
xx.commits
xx.commits.first.codebase
xx.units
xx.units[1]
xx.units[1].codebase
xx.units[1].name
xx.units[1].service_name
xx.units[1].ports
xx.units[1].port
xx = Pancakes::Stack.last 
uu = xx.units[1]
uu.lookup_public_port(port)
xx = Pancakes::Stack.last 
uu = xx.units[1]
uu.lookup_public_port(1080)
container
container.port_mapping
container.port_mapping[port]
container.port_mapping[private_port]
xx.units
xx.units[7].container.port_mapping
xx.units[7].state_updated_at
xx.units[7].state_updated_at.strftime("%c")
xx.units[7].state_updated_at.zone
xx.services_healthy?
xx = Pancakes::Stack.last ; xx.swarm_destroy ; xx.swarm_spin_up
xx.check_health
bb = Integrity::Build.last
bb.project
bb = Integrity::Build.get(11379)
bb = Integrity::Build.get(11378)
bb = Integrity::Build.get(11377)
bb = Integrity::Project.get(10)
bb = Integrity::Project.get(10).last_build
bb
bb.swarm_start
self.name
self.service_name
bb = Integrity::Project.get(10).last_build
bb.swarm_start
build_container_name
datastore_container_name
self.sha
self.repo
self.repo.name
self.repo
self.repo.to_s
bb = Integrity::Project.get(10).last_build
bb = Integrity::Project.get(10).last_build ; bb.swarm_start
self.repo.full_name
self.branch
self.branch.name
self.branch_name
bb = Integrity::Project.get(10).last_build ; bb.swarm_start
cmd
cmd.to_s
cmd.to_docker_api_cmd
cmd
bb = Integrity::Project.get(10).last_build ; bb.swarm_start
cmd
cmd.to_s
cmd.to_docker_api_cmd
bb = Integrity::Project.get(10).last_build ; bb.swarm_start
cmd.to_docker_api_cmd
{a: 3}.merge(nil)
bb = Integrity::Project.get(10).last_build ; bb.swarm_start
json
bb = Integrity::Project.get(10).last_build ; bb.swarm_start
json
bb = Integrity::Project.get(10).last_build ; bb.swarm_start
json
bb = Integrity::Project.get(10).last_build ; bb.swarm_start
json
bb = Integrity::Project.get(10).last_build ; bb.swarm_start
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy ; bb.swarm_start
json
c
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy ; bb.swarm_start
c
bb.
bb.swarm_destroy
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy ; bb.swarm_start
c
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy ; bb.swarm_start
c
bb.datastore
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy ; bb.swarm_start
c
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy ; bb.swarm_start
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy 
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy ; bb.swarm_start
bb.containers
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy ; bb.swarm_start
json
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy ; bb.swarm_start
bb.codebase
bb.repo
bb = Integrity::Project.get(10).last_build 
bb.repo
bb.repo.name
bb = Integrity::Project.get(11).last_build 
bb.repo.name
File.readlines('/tmp/foobar')
File.readlines('/tmp/foobar').map(&:chomp)
File.readlines('/tmp/foobarx').map(&:chomp)
$!
File.readlines('/tmp/foobarx').map(&:chomp) rescue Errno::ENOENT
lines = File.readlines('/tmp/foobar').map(&:chomp) rescue Errno::ENOENT
lines
lines.include?('one')
bb = Integrity::Project.get(11).last_build 
bb.containers
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy ; bb.swarm_start
xx = Pancakes::Stack.last ; 
xx.units.first.name
xx.units.first.stack.name
bb.service_
bb.service_name
xx.units.first.service_name
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy ; bb.swarm_start
bb = Integrity::Project.get(10).last_build ; bb.swarm_destroy ; bb.swarm_run
bb = Integrity::Project.get(10).last_build ; bb.swarm_run
ENV
foo = [1, 2]
foo
foo.unshift(3)
foo
bb = Integrity::Project.get(10).last_build
bb.run_containers
bb = Integrity::Project.get(10).last_build
bb.run_containers
self.cargo_type
cargo_type
self.class
bb = Integrity::Project.get(10).last_build
bb.run_containers
self
self.class
self.cargo_type
bb = Integrity::Project.get(10).last_build
bb.run_containers
bb
bb.datastore
bb = Integrity::Project.get(10).last_build
bb.run_containers
c
bb.run_containers
bb = Integrity::Project.get(10).last_build ; bb.run_containers
c
bb = Integrity::Project.get(10).last_build ; bb.run_containers
c
bb.containers
bb = Integrity::Project.get(10).last_build
bb.containers
bb = Integrity::Project.get(10).last_build
bb.containers
filters
n
filters
bb.run_containers
c
bb = Integrity::Project.get(10).last_build
bb.run_containers
bb = Integrity::Project.get(10).last_build
bb.run_containers
bb = Integrity::Project.get(10).last_build
bb.run_containers
self.class
start
bb = Integrity::Project.get(10).last_build
bb.run_containers
c
bb.run_containers
super
bb = Integrity::Project.get(10).last_build
bb.run_containers
bb.datastore_cargo
bb.datastore_cargo.container_name
bb = Integrity::Project.get(10).last_build
bb.run_containers
bb = Integrity::Project.get(10).last_build
bb.run_containers
bb.containers
bb.datastore_cargo.container
bb = Integrity::Project.get(10).last_build ; bb.destroy_containers ; bb.run_containers
bb.containers
bb.containers.map(&:name)
bb.destroy_containers
bb.containers.map(&:name)
bb = Integrity::Project.get(10).last_build ; bb.destroy_containers ; bb.run_containers
create_container(container_def)
container_def
c
bb = Integrity::Project.get(10).last_build ; bb.destroy_containers ; bb.run_containers
c
container_def
container_def[:Env]
container_def[:Env][12]
container_def[:Env][12] = "constraint:grnds.node_type==builds"
c
bb = Integrity::Project.get(10).last_build ; bb.destroy_containers ; bb.run_containers
c
bb = Integrity::Project.get(10).last_build ; bb.destroy_containers ; bb.run_containers
c
bb = Integrity::Project.get(10).last_build ; bb.destroy_containers ; bb.run_containers
c
env_var
k
c
exit-program
bb = Integrity::Project.get(10).last_build ; bb.destroy_containers ; bb.run_containers
env_hash
bb = Integrity::Project.get(10).last_build ; bb.destroy_containers ; bb.run_containers
jj
c
jj
bb = Integrity::Project.get(10).last_build ; bb.destroy_containers ; bb.run_containers
c
bb = Integrity::Project.get(10).last_build ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds
bb = Integrity::Project.get(10).builds[-1]
bb = Integrity::Project.get(10).builds[-2]
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; bb.run_containers
bb.repo_full_name
bb.destroy_containers
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-1] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-1] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-1] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-1] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; bb.run_containers
e
e.message
c
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; bb.run_containers
c
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-1] ; 
bb.containers
bb.containers.each {|c| puts c.info.Names }
bb.containers.each {|c| puts c.info.Names.inspect }
"/ip-10-161-122-252.ec2.internal/jarvis_62ec9f8_15774_app/mysql".split('/')
"/ip-10-161-122-252.ec2.internal/jarvis_62ec9f8_15774_app".split('/')
bb = Integrity::Project.get(10).builds[-1] ; 
bb.containers.each {|c| puts c.info.Names.inspect }
bb.containers.each {|c| puts c.name }
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-1] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; bb.run_containers
"/ip-10-161-122-252.ec2.internal/jarvis_62ec9f8_15774_app".split('/').last
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-1] ; bb.destroy_containers ; 
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; 
bb = Integrity::Builds.get(15815)
bb = Integrity::Build.get(15815)
bb = Integrity::Build.get(15815) ; bb.destroy_containers ; bb.run_containers
[A
bb = Integrity::Build.get(15815) ; bb.destroy_containers ; bb.run_containers
bb.container
bb.container.tcp_str
bb = Integrity::Build.get(15815) ; 
bb.container.tcp_str
bb.container
bb.container.info
bb.container.ip
bb.container.public_port
bb.datastore_cargo.tcp_str
bb.datastore_cargo.container.tcp_str
bb = Integrity::Build.get(15815) ; 
bb.datastore_cargo.container.tcp_str
bb = Integrity::Build.get(15815) ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Build.get(15815) ; bb.destroy_containers ; 
bb = Integrity::Build.get(15815) ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-1] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-2] ; bb.containers
bb = Integrity::Project.get(10).builds[-1] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Build.get(15815) ; bb.destroy_containers ; bb.run_containers
HonestyCluster::SwarmBackend.node_count
Docker::Info
Docker.info
Docker.info(SwarmBackend.connection)
Docker.info(SwarmBackend.connection).detect{|e| e[0]['Nodes']}
Docker.info(SwarmBackend.connection).detect{|e| e[0].include?('Nodes') }
Docker.info(SwarmBackend.connection).detect{|e| e[0].match(/Nodes/) }
Docker.info(SwarmBackend.connection)['DriverInfo'].detect{|e| e[0].match(/Nodes/) }
Docker.info(SwarmBackend.connection)['DriverStatus'].detect{|e| e[0].match(/Nodes/) }
Docker.info(SwarmBackend.connection)['DriverStatus'].detect{|e| e[0].match(/Nodes/) }.last
Docker.info(SwarmBackend.connection)['DriverStatus'].detect{|e| e[0].match(/Nodes/) }.last.to_i
HonestyCluster::SwarmBackend.node_count
HonestyCluster::Waitress.new('foo') { puts 'bar' }
HonestyCluster::Waitress.new('foo').run_async { puts 'bar' }
1/0
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all
HonestyCluster::Waitress.all.first
HonestyCluster::Waitress.all.first.run { 1 / 0 }
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all.first.run { 1 / 0 }
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all.first.run { 1 / 0 }
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all.first.run { 1 / 0 }
HonestyCluster::Waitress.all.first.run_async { 1 / 0 }
HonestyCluster::Waitress.new('foo').run_async { 1 / 0 }
HonestyCluster::Waitress.all.first.run { 1 / 0 }
HonestyCluster::Waitress.new('foo').run_async { 1 / 0 }
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all.first.run { 1 / 0 }
HonestyCluster::Waitress.all.first.run_async { 1 / 0 }
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all.first.run_async { 1 / 0 }
HonestyCluster::Waitress.all.first.thread
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all.first.run_async { 1 / 0 }
HonestyCluster::Waitress.all.first.thread
HonestyCluster::Waitress.all.first.run { 1 / 0 }
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all.last.run { 1 / 0 }
Thread.main
Thread.main.raise("foo")
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all.last.run { 1 / 0 }
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all.last.run_async { 1 / 0 }
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all.last.run_async { 1 / 0 }
HonestyCluster::Waitress.new('foo')
HonestyCluster::Waitress.all.last.run_async { 1 / 0 }
HonestyCluster.run_waitresses
Delayed::Job.all
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-1] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-5] ; bb.destroy_containers ; bb.run_containers
bb = Integrity::Project.get(10).builds[-2] ; bb.destroy_containers ; bb.run_containers
HonestyCluster.run_waitresses
bb = Integrity::Project.get(10).builds[-5] 
bb.repo.name
Integrity.logger.info "hi"
Integrity.logger
Integrity.logger = Logging.logger['dj']
Integrity.config.logger = Logging.logger['dj']
Integrity.logger.info "hi"
bb = Integrity::Project.get(10).builds[-5] 
bb.codebase
bb.repo
bb.repo.name
bb.repo_name
bb = Integrity::Project.get(10).builds[-3] ; bb.destroy_containers ; bb.run_containers
xx = Pancakes::Stack.last ; 
xx = Pancakes::Stack.create_with_git_strings(services: Pancakes::Services.full_services)
Pancakes::Stack.all
xx = Pancakes::Stack.last
xx.bring_down
xx.units.first
xx.units.first.run_container
xx.units.first.name
xx.units.first.service_type
xx.units.each{|e| puts e.service_type }
xx.units.each{|e| puts e.service_type } ; nli
xx.units.each{|e| puts e.service_type } ; nil
xx.units.each{|e| puts e.service_name } ; nil
xx.units.each{|e| puts e.subtype } ; nil
xx = Pancakes::Stack.last
xx.units.first.run_container
$!
help
wtf?
xx = Pancakes::Stack.last
xx.units.first.run_container
xx = Pancakes::Stack.last
xx.units.first.run_container
[true, false, false].reduce(false){|m,v| m || v }
xx = Pancakes::Stack.last
xx.units.first.run_container
wtf?
xx = Pancakes::Stack.last
xx.units.first.run_container
xx = Pancakes::Stack.last
xx.units.first.run_container
xx.units[1].run_container
xx.units[2].run_container
xx = Pancakes::Stack.last
xx.units[2].run_container
xx.units[3].run_container
xx.units[4].run_container
xx.units[5].run_container
xx.units[6].run_container
xx.units[7].run_container
xx.units[8].run_container
xx.units[9].run_container
xx.units[10].run_container
xx.units[11].run_container
xx.units[12].run_container
xx = Pancakes::Stack.last
xx.run_containers
Pancakes::Stack.dataset.destroy
Pancakes::Stack.last.errors
Pancakes::Stack.last.errors.to_s
Pancakes::Stack.last.error_messages
xx = Pancakes::Stack.create_with_git_strings(services: Pancakes::Services.full_services)
Pancakes::Stack.dataset.destroy
ActiveSupport::VERSION
puts ActiveSupport::VERSION
ActiveSupport::VERSION
xx = Pancakes::Stack.create_with_git_strings(services: Pancakes::Services.full_services, pocs: ['foo'])
xx = Pancakes::Stack.create_with_git_strings(services: Pancakes::Services.full_services, pocs: 'foo')
xx.units
%i[a]
:asdf.upcase
"asdf#{:asdf.upcase}"
xx = Pancakes::Stack.last
xx.units
xx.units[3]
xx.units[2]
xx.units[2].create_initial_health_check_event
xx = Pancakes::Stack.last
xx.units[2].initial_health_check
xx = Pancakes::Stack.last
xx.units[2].initial_health_check
health_check_type
self.class
self.class.health_check_type
self.class.health_check_type 
self.class.health_check_type "foo"
xx = Pancakes::Stack.last
xx.units[2].initial_health_check
wtf?
xx = Pancakes::Stack.last
xx.units[2].initial_health_check
wtf?
xx = Pancakes::Stack.last
xx.units[2].initial_health_check
wtf?
xx = Pancakes::Stack.last
xx.units[2].initial_health_check
xx.units[2].last_health_check
xx = Pancakes::Stack.last
xx.name
xx.units[2].name
xx = Pancakes::Stack.last
xx.units[2].service_name
HonestyCluster::SwarmBackend::HealthCheck.new(*[:foo])
ss = HonestyCluster::SwarmBackend::HealthCheck.new([HonestyCluster::SwarmBackend::StatusCheck.new(:complete, 'created'), HonestyCluster::SwarmBackend::StatusCheck.new(:in_progress, 'created')])
ss.status
ss.message
ss = HonestyCluster::SwarmBackend::HealthCheck.new([HonestyCluster::SwarmBackend::StatusCheck.new(:complete, 'created'), HonestyCluster::SwarmBackend::StatusCheck.new(:in_progress, 'createdasfasf')])
ss.message
ss = HonestyCluster::SwarmBackend::HealthCheck.new([HonestyCluster::SwarmBackend::StatusCheck.new(:complete, 'created'), HonestyCluster::SwarmBackend::StatusCheck.new(:in_progress, 'createdasfasf'), HonestyCluster::SwarmBackend::StatusCheck.new(:failed, 'fuck')])
ss.message
ss.status
nil.to_sym
xx = Pancakes::Stack.last
xx.units[2]
xx.units[2].check_health
xx.units[2].check_health!
xx.units[2].current_health
xx = Pancakes::Stack.last
xx.units[2].check_health!
xx = Pancakes::Stack.last
xx.units[2].check_health!
xx = Pancakes::Stack.last
xx.units[2].check_health!
xx = Pancakes::Stack.last
xx.units[2].check_health!
xx.units[2].last_health_check
xx.units[2].check_health!
xx = Pancakes::Stack.last
xx.units[2].check_health!
prioritized_status_checks
xx = Pancakes::Stack.last
xx.units[2].check_health!
status_checks
xx = Pancakes::Stack.last
xx.units[2].check_health!
xx.units[2].last_health_check
xx = Pancakes::Stack.last
xx.units[2].check_health!
xx.units[2].last_health_check
xx = Pancakes::Stack'0
xx = Pancakes::Stack[0]
xx = Pancakes::Stack.all[0]
xx = Pancakes::Stack.all[0].destroy
Pancakes::Stack.size
Pancakes::Stack.count
Pancakes::Stack.last
Pancakes::Stack.last.units
Pancakes::Stack.dataset
Pancakes::Stack.dataset.delete
Pancakes::Stack.dataset
Pancakes::Commit.dataset.delete
Pancakes::Unit.dataset.delete
Pancakes::Unit.dataset
Integrity::Build.where(sha: 'c4ce3f9')
Integrity::Build.get(sha: 'c4ce3f9')
Integrity::Build.find(sha: 'c4ce3f9')
Integrity::Build.get(sha: 'c4ce3f9')
Integrity::Build.last
Integrity::Commit.last
xx = Pancakes::Stack.last
xx.units[2].commit.build
icommit
Integrity::Commit.last
Integrity::Commit.last.build
Integrity::Commit.last.build.successful
Integrity::Commit.last.build.successful?
xx = Pancakes::Stack.last
xx.health?
xx.healthy?
xx.last_health_check_event
512 *  2**20
Integrity::Branch.last.name
Integrity::Project.last.name
Integrity::Project.last.full_name
Integrity::Project.last.repo.name
Integrity::Project.last.repo.full_name
Integrity::Project.last.repo_full_name
xx = Pancakes::Stack.create_with_git_strings(pocs: 'foo')
ref = Octokit.ref(repo, Pancakes.heads_str(branch))
c
ref = Octokit.ref(repo, Pancakes.heads_str(branch))
c
ref = Octokit.ref(repo, Pancakes.heads_str(branch))
c
ref = Octokit.ref(repo, Pancakes.heads_str(branch))
c
ref = Octokit.ref(repo, Pancakes.heads_str(branch))
c
ref = Octokit.ref(repo, Pancakes.heads_str(branch))
c
ref = Octokit.ref(repo, Pancakes.heads_str(branch))
c
ref = Octokit.ref(repo, Pancakes.heads_str(branch))
c
ref = Octokit.ref(repo, Pancakes.heads_str(branch))
c
ref = Octokit.ref(repo, Pancakes.heads_str(branch))
c
ref = Octokit.ref(repo, Pancakes.heads_str(branch))
Pancakes.heads_str(branch)
branch
repo
repo = "ConsultingMD/wp"
ref = Octokit.ref(repo, Pancakes.heads_str(branch))
exit!!!
exit-program
Pancakes::Stack.to_s
raise RuntimeError, "hi"
puts "#{nil.inspect}"
[nil, nil].compact.join(', ')
Pancakes::Stack.all
Pancakes::Stack.all.each {|s| s.destroy }
Time.now
HonestyCluster::Event.last
HonestyCluster::Event.last.created_at
HonestyCluster::Event.last.created_at.class
Time.utc_time
Time.utc.now
Time.utc
Time.now.utc
Pancakes::Stack.last
Pancakes::Stack.last.destroy
["foo", nil].join(' ')
Integrity::Project.last.repo
Integrity::Project.last.repo.name
Integrity::Project.last.repo.full_name
nil.map
Pancakes::Stack.last
xx = Pancakes::Stack.last
xx.async.restart_containers
xx.async
xx.await
xx.async
xx.first
xx
xx.all
xx.class
xx
xx.async
"".async
xx = Pancakes::Stack.last
xx.units.first.stack
mc = xx.unit_named('mailcatcher')
mc.stack
xx.    full_name.split('/')[1]
xx.units_dataset.first(service_name: service_name.to_s)
xx.units_dataset.first(service_name: 'mailcatcher')
xx.units_dataset.first(service_name: 'mailcatcher').stack
xx.units_dataset.first(service_name: 'mailcatcher').class
xx.units_dataset.where(service_name: 'mailcatcher')
xx.units_dataset.where(service_name: 'mailcatcher').first
xx.units_dataset.where(service_name: 'mailcatcher').first.stack
ss = Pancakes::Stack.last
bb = ss.unit_named("banyan-server")
bb.restart_container
kontainer
c
ss = Pancakes::Stack.last
bb = ss.unit_named("banyan-server")
bb.restart_container
c
ENV
bb.restart_container
c
ss = Pancakes::Stack.last
bb = ss.unit_named("banyan-server")
bb.restart_container
cargo_json
cargo_json.env
cargo_json[:env]
cargo_json[:env]['HONESTY_BASE_URL']
cargo_json[:env]['HONESTY_BASE_URL'].to_s
=list
list=
l
=l
l
list
whereami
w
n
cargo_json_overrides
n
cargo_json_overrides
cargo_json.deep_merge(cargo_json_overrides)
docker_api_container_json(cargo_json.deep_merge(cargo_json_overrides))
c
ss = Pancakes::Stack.last
bb = ss.unit_named("banyan-server")
bb.restart_container
c
bb.restart_container
n
cargo_json_overrides
cargo_json.deep_merge(cargo_json_overrides)
docker_api_container_json(cargo_json.deep_merge(cargo_json_overrides))
c
ls
bb.destroy_container
yy = File.read('/tmp/foo')
YAML
require 'yaml' ;YAML
YAML.load(yy)
Pancakes::Stack.last.units.map(&:resource_name)
Pancakes::Stack.last.units.map(&:image_name)
img_s = Pancakes::Stack.last.units[12]
xx = Docker::Image.create('fromImage' => img_s)
xx.pull
xx = Docker::Image.create('fromImage' => img_s, HonestyCluster::SwarmBackend.connection)
xx = Docker::Image.create({'fromImage' => img_s}, HonestyCluster::SwarmBackend.connection)
img_2
img_s
img_s.image_name
img_s = img_s.image_name
xx = Docker::Image.create({'fromImage' => img_s}, HonestyCluster::SwarmBackend.connection)
xx = Docker::Image.create({'fromImage' => "grnds/tim_ci"}, HonestyCluster::SwarmBackend.connection)
Docker.options[:read_timeout]
Docker.options[:read_timeout] = 2
xx = Docker::Image.create({'fromImage' => "grnds/banyan_ci"}, HonestyCluster::SwarmBackend.connection)
include HonestyCluster::SwarmBackend::DockerUtil
docker_api_with_timeout(2) {puts "foo" }
Docker.options[:read_timeout] 
Docker::Image.get('grnds/mysql', HonestyCluster::SwarmBackend.connection)
Docker::Image.search('term' => 'grnds/tp_ci')
Docker::Image.search('term' => 'grnds/tp_ci:latest')
Docker::Image.search('term' => 'grnds/tp_ci')
Docker::Image.search('term' => 'grnds/tp_ci latest')
Docker::Image.search('term' => 'grnds/tp_ci-latest')
Pancakes::Stack.last.unit_named('tp-server')
Pancakes::Stack.last.unit_named('tp-server').pull_image
Pancakes::Stack.last.unit_named('wp-server')
Pancakes::Stack.last.unit_named('wp-server').image_name
Pancakes::Stack.last.unit_named('tp-server').pull_image
Pancakes::Stack.last.unit_named('wp-server').pull_image
xx = Pancakes::Stack.last.unit_named('jarvis-server').pull_image
xx = Pancakes::Stack.last.units.map(&:pull_image)
xx = Pancakes::Stack.last
xx.units
xx.units[1].delete_container
xx.units[1].destroy_container
xx.units[1].run_container
ENV
ENV['DOCKER_HOST']
xx.units[1].run_container
xx.units[1].pull_image
HonestyCluster::SwarmBackend.docker_info
HonestyCluster::SwarmBackend.docker_info['DriverStatus']
HonestyCluster::SwarmBackend.docker_info.DriverStatus
HonestyCluster::SwarmBackend.node_info
docker_info
docker_info.drop(4)
docker_info.DriverStatus.drop(4)
docker_info.DriverStatus.drop(3)
docker_info.DriverStatus.drop(4)
HonestyCluster::SwarmBackend.node_info
docker_info.DriverStatus.drop(4)
nodes = docker_info.DriverStatus.drop(4)
nodes
nodes.map.with_index{|e,i| [nodes.drop(i*5).take(5)] }
(nodes.size/5).each_index{|i| [nodes.drop(i*5).take(5)] }
(nodes.size/5).times{|i| [nodes.drop(i*5).take(5)] }
(nodes.size/5).times.map.with_index{|_,i| [nodes.drop(i*5).take(5)] }
(nodes.size/5).times.map.with_index{|_,i| [nodes.drop(i*5).take(5)] }.to_h
(nodes.size/5).times.map.with_index{|_,i| [nodes.drop(i*5).take(5)] }.map(&:to_h)
(nodes.size/5).times.map.with_index{|_,i| [nodes.drop(i*5).take(5)] }.map(&:to_h).map(&:to_h)
nodes = (nodes.size/5).times.map.with_index{|_,i| [nodes.drop(i*5).take(5)] }
nodes
nodes.inspect
nodes[0]
nodes[0][0]
nodes[0][0].to_h
nodes
nodes.flatten
nodes.size
nodes.map(&:first)
nodes.map{|e| e.first.to_h }
nodes.map{|e| e.first.map{|k,v| [k.gsub("└", ''), v] } }
nodes.map{|e| e.first.map{|k,v| [k.gsub("└", '').strip, v] } }
HonestyCluster::SwarmBackend.node_info
nodes
nodes.map(&:to_h)
HonestyCluster::SwarmBackend.node_ips
HonestyCluster::SwarmBackend.node_info
nodes
w
nodes.map{|e| e.first.map{|k,v| [rm_non_ascii(k).strip, v] } }.to_h
nodes.map{|e| e.first.map{|k,v| [rm_non_ascii(k).strip, v] } }
nodes.map{|e| e.first.map{|k,v| [rm_non_ascii(k).strip, v] } }.map{|e| e.first.to_h }
nodes.map{|e| e.first.map{|k,v| [rm_non_ascii(k).strip, v] } }.map{|e| e.first}
nodes.map{|e| e.first.map{|k,v| [rm_non_ascii(k).strip, v] } }.map{|e| e.to_h }
nodes.map{|e| e.first.map{|k,v| [rm_non_ascii(k).strip, v] } }.map(&:to_h)
HonestyCluster::SwarmBackend.node_info
HonestyCluster::SwarmBackend.node_ips
node_info
nn = HonestyCluster::SwarmBackend.node_info
nn.map {|n| n.values  }
HonestyCluster::SwarmBackend.node_ips
HonestyCluster::SwarmBackend.node_ips.each {|n| system("ssh core@#{n} sudo systemctl restart swarm-minion") }
HonestyCluster::SwarmBackend.node_ips
HonestyCluster::SwarmBackend.master_ip
HonestyCluster::SwarmBackend.restart_minions
HonestyCluster::SwarmBackend.restart_master
system "ssh core@10.161.123.146"
HonestyCluster::SwarmBackend.master
HonestyCluster::SwarmBackend.master.ssh
HonestyCluster::SwarmBackend.minions.first.ssh
HonestyCluster::SwarmBackend.minions
HonestyCluster::SwarmBackend.minions.first.ssh
w
ENV['SWARM_ENV']
ENV[/SWARM/]
HonestyCluster::SwarmBackend.master.ssh
Pancakes::Stack.last
Pancakes::Stack.last.pull_images
[:a, :a, :b, :c].uniq
['a', :a, :b, :c].uniq
['a', 'a', :b, :c].uniq
HonestyCluster::Event.new
Time.now.utc
st = Time.now.utc
too = Time.now.utc - st
format('%.2f', too)
format('%.3f', too)
format('%.1f', too)
xx = RuntimeError.new("foo")
xx.to_s
xx.inspect
HonestyCluster::Event.new_benchmarked_event(type: 'Foo', status: 'RAN', msg: 'hey dude') { puts :bar }
ee = HonestyCluster::Event.new_benchmarked_event(type: 'Foo', status: 'RAN', msg: 'hey dude') { puts :bar }
ee
puts ee.inspect
ee.inspect
ee.class
ee = HonestyCluster::Event.new_benchmarked_event(type: 'Foo', status: 'RAN', msg: 'hey dude') { puts :bar }
ee = HonestyCluster::Event.new_benchmarked_event(type: 'Foo', status: 'RAN', msg: 'hey dude') { puts :bar ; sleep 1.1 }
ee.class
ee = HonestyCluster::Event.new_benchmarked_event(type: 'Foo', status: 'RAN', msg: 'hey dude') { puts :bar ; sleep 1.1 }
ee
ee = HonestyCluster::Event.new_benchmarked_event(type: 'Foo', status: 'RAN', msg: 'hey dude') { puts :bar ; sleep 1.1; raise "hi" }
xx = Pancakes::Stack.last
xx[:1]
xx[:d]
xx.unit_named('banyan-server')
xx = Pancakes::Stack.last
cc = xx.unit_named('banyan-server')
cc.container
cc.container.id
cc.container.id[0..12]
cc.container.id[0...12]
xx = Pancakes::Stack.last
xx.containers
xx.containers.map(&:short_id)
xx.containers.map(&:short_id).join('+OR+')
HonestyCluster::SwarmBackend.minions.first.ssh
Docker.logger.info "hi"
xx
xx = Pancakes::Stack.last
xx.wp_database_version = Pancakes::Stack::DEFAULT_WP_DB
xx.save
xx = Pancakes::Stack.last
xx.pull_images
xx = Pancakes::Stack.last
xx.pull_images
xx = Pancakes::Stack.last
xx.units.first.container
filters
filters.keys.sort
filters.keys.sort.map{|key| [key, filters[key]].join('=') }.join(',')
filters.keys.sort.map{|key| [key, filters[key]].join('=') }.join(',').sha
filters.keys.sort.map{|key| [key, filters[key]].join('=') }.join(',')
filterrs
filters
xx = Pancakes::Stack.last
Docker.logger.level = :debug
xx.units.first.container
Docker.logger.level = :debug
xx = Pancakes::Stack.last
xx.units.first.container
xx = Pancakes::Stack.last
xx.units.first.container
xx = Pancakes::Stack.last
HonestyCluster::SwarmBackend::DockerUtil.with_cached_containers { xx.containers }
xx = Pancakes::Stack.last
HonestyCluster::SwarmBackend::DockerUtil.with_cached_containers { xx.containers }
xx = Pancakes::Stack.last
HonestyCluster::SwarmBackend::DockerUtil.with_cached_containers { xx.containers }
xx = Pancakes::Stack.last
HonestyCluster::SwarmBackend::DockerUtil.with_cached_containers { xx.containers ; nil }
HonestyCluster::SwarmBackend::DockerUtil.with_cached_containers { xx.containers ; xx.containers ; nil }
HonestyCluster::SwarmBackend::DockerUtil.with_cached_containers { xx.containers ; xx.containers  }
"x=y=z".split('=', 2)
require 'hashie'
require 'hashie/mash'
require 'hashie'
require 'rubygems'
require 'hashie'
xx = Hashie::Mash.new
xx.foo = 32
xx
xx.foob
xx = Pancakes::Stack.last
xx.pull_images
bb = Integrity::Build.last
bb.datastore_cargo.exposed_ports
bb.datastore_cargo
bb = Integrity::Build.get(16160)
bb = Integrity::Build.get(16159)
bb = Integrity::Build.get(16149)
bb.datastore_cargo.exposed_ports
bb.datastore_cargo.cargo_json
bb.datastore_cargo.docker_json
class Foo ; def bar ; "bar" ; end ;end
Foo.new.bar
xx = Foo.new
xx.try(:bar)
require 'active_support'
xx.try(:bar)
require 'active_support/core_ext/object/try'
xx.try(:bar)
xx.try(:barx)
ActiveSupport::VERSION
puts ActiveSupport::VERSION
xx
xx.bar
xx.barx
xx.try :barx
xx.try(:barx)
xx = Pancakes::Stack.last
xx = Pancakes::Stack[2]
xx = Pancakes::Stack.last
xx.units
xx.units ; nil
xx.units.reload ; nil
xx.units_dataset ; nil
xx.units_dataset.to_a ; nil
xx.units
xx.units ; nil
xx.reloadd
xx.reload
xx.units ; nil
:foo"
:foo?
:foo-asdf
:fooasdf!
Array(nil)
xx = Pancakes::Stack.last
xx.units.map(&:resource_name)
xx.units.map(&:container_name)
xx = Pancakes::Stack.last
xx.units.map(&:container_name)
Pancakes::Unit.last.stack
Pancakes::Unit.first.stack
Logging.logger['honesty'].level
Logging.logger['stats'].level
Logging.logger['stathat'].level
Logging.logger['stathat'].name
xx = Pancakes::Stack.last
xx.service_addrs
xx = Pancakes::Stack.last
xx.service_addrs
xx = Pancakes::Stack.last
xx.service_addrs
xx = Pancakes::Stack.last
xx.service_addrs(['mysql', 'postgres', 'mailcatcher'])
xx.service_addrs(['mysql', 'postgres', 'mailcatcher', 'jarvis'])
xx.service_addrs(['mysql', 'postgres', 'mailcatcher', 'jarvis-server'])
xx = Pancakes::Stack.last
xx.service_addrs(['mysql', 'postgres', 'mailcatcher', 'jarvis-server'])
xx.service_addrs(['mysql', 'postgres', 'mailcatcher', 'jarvis'])
xx.service_addrs(['mysql', 'postgres', 'mailcatcher'])
xx.service_addrs([])
xx.service_addrs()
xx = Pancakes::Stack.last
xx.service_addrs()
xx.service_addrs(force: true)
xx = Pancakes::Stack.last
xx.service_addrs(allow_nil: true)
xx.service_addrs([], allow_nil: true)
xx.service_addrs(['mysql', 'wp'], allow_nil: true)
xx[2]
xx.units[2]
xx.units[3]
xx.units[3].service_aliases
%w[].intersection(xx.units[3].service_aliases)
Set.new(%w[]).intersection(xx.units[3].service_aliases)
Set.new(%w[stark]).intersection(xx.units[3].service_aliases)
Set.new(%w[stark jarvis]).intersection(xx.units[3].service_aliases)
%w[stark].intersection(xx.units[3].service_aliases)
%w[stark].to_set.intersection(xx.units[3].service_aliases)
%w[stark].to_set.intersection(xx.units[3].service_aliases).empty?
xx = Pancakes::Stack.last
xx.service_addrs(['mysql', 'wp'], allow_nil: true)
xx.service_addrs(['mysql', 'wp', 'foo'], allow_nil: true)
xx.service_addrs(['mysql', 'wp', 'jarvis'], allow_nil: true)
xx.service_addrs(['mysql', 'wp', 'tp'], allow_nil: true)
xx.service_addrs(['mysql', 'wp', 'tp'], allow_nil: false)
Integrity.logger.info []
Integrity.logger.info "hi"
Pancakes.logger.name
Pancakes.to_s
Pancakes::Stack.last.pull_images
Pancakes::Stack.last.images
Pancakes::Stack.last.image_names
Pancakes::Stack.last.pull_images
fg
Pancakes::Stack.last.pull_images
docker_api_pull_image(image_name)
Pancakes::Stack.last.pull_images
docker_api_pull_image(image_name)
Pancakes::Stack.last.pull_images
Docker::Image.create({'fromImage' => image_name})
Pancakes::Stack.last.pull_images
"FOO_BAR_LOG_LEVEL".split('_LOG_LEVEL')
"FOO_BAR_LOG_LEVEL".split('_LOG_LEVEL').first
"FOO_BAR_LOG_LEVEL".split('_LOG_LEVEL').first.downcase
ENV.each {|k,v| puts k ; puts v }
ENV.each {|k,v| puts k ; puts v } ; nil
JSON.parse("foo")
Pancakes::Stack.last.pocs
Pancakes::Stack.last.pocs.to_s
Pancakes::Stack.last.pocs.join(', ')
DataMapper.auto_upgrade!
Pancakes::Stack.last.units[4].commit
Pancakes::Stack.last.units[4].commit.refresh_branch_head
Pancakes::Stack.last.units[1].commit.refresh_branch_head
Pancakes::Stack.last.units[1].commit
Pancakes::Stack.last.units[1]
xx = Pancakes::Stack.last
xx.service_addrs
xx.service_addrs(allow_nil: true)
xx = Pancakes::Stack.last
xx.service_addrs(allow_nil: true)
xx = Pancakes::Stack.last
xx.service_addrs(allow_nil: true)
required_addrs
units.reject { |unit| required_addrs.to_set.intersection(unit.service_aliases).empty? }
c
xx.service_addrs(allow_nil: true)
units.reject { |unit| required_addrs.to_set.intersection(unit.service_aliases).empty? }
units
w
units.map(&:to_address_hash)
units.uniq{|unit| unit.service_name }.map(&:to_address_hash)
units.uniq{|unit| unit.codebase }.map(&:to_address_hash)
units
units.map(&:codebase)
units.uniq{|unit| unit.resource_name }.map(&:to_address_hash)
units.uniq{|unit| unit.service_type }.map(&:to_address_hash)
units.uniq{|unit| unit.service_type }.map(&:to_address_hash).reduce(&:merge)
units.uniq{|unit| unit.service_name }.map(&:to_address_hash).reduce(&:merge)
units.reject{|unit| unit.exposed_ports }.map(&:to_address_hash).reduce(&:merge)
units.select{|unit| unit.exposed_ports }.map(&:to_address_hash).reduce(&:merge)
yy = units.first
yy.machine_ip = yy.port = nil ; yy.save
units.select{|unit| unit.exposed_ports }.map(&:to_address_hash).reduce(&:merge)
yy.machine_ip '10.0.80.27' ; yy.port = 33128 ; yy.save
xx = Pancakes::Stack.last
yy = xx.unit_named('mysql')
yy.machine_ip '10.0.80.27' ; yy.port = 33128 ; yy.save
yy.machine_ip = '10.0.80.27' ; yy.port = 33128 ; yy.save
xx.service_addrs(allow_nil: true)
xx.units.map(&:resource_name)
xx.units.map(&:service_name)
xx = Pancakes::Stack.last
xx.units.map(&:service_name)
xx.service_addrs(allow_nil: true)
xx = Pancakes::Stack.last
xx.service_addrs(allow_nil: true)
xx.id
xx = Pancakes::Stack.last
JSON.dump xx.service_addrs(allow_nil: true)
File.open('/tmp/foo', 'w') {|f| f.write JSON.dump(xx.service_addrs(allow_nil: true)) }
xx = Pancakes::Stack.last
xx.owner
xx.sevice_addrs
xx.service_addrs
xx.service_addrs(allow_nil: true)
xx = Pancakes::Stack.last
xx.service_addrs(allow_nil: true)
addrs
required_addrs
service_addrs_all_present?(addrs, required_addrs)
addrs.values.map{|v| !!v }.reduce(true){|memo,val| memo && val } &&
addrs.values.map{|v| !!v }.reduce(true){|memo,val| memo && val }
addrs.keys.sort == required_addrs.sort
addrs.keys.to_set.intersection(required_addrs.to_set)
required_addrs
ra = ['mysql', 'jarvis-server]
ra = ['mysql', 'jarvis-server']
addrs.keys.to_set.intersection(ra.to_set)
ra = ['mysql', 'jarvis-serverx']
addrs.keys.to_set.intersection(ra.to_set)
addrs.keys.to_set.intersection(ra.to_set) == ['jarvis-server', 'mysql']
ra = ['mysql', 'jarvis-server']
addrs.keys.to_set.intersection(ra.to_set) == ['jarvis-server', 'mysql']
addrs.keys.to_set.intersection(ra.to_set) == ['jarvis-server', 'mysql'].to_set
xx = Pancakes::Stack.last
xx.service_addrs(allow_nil: true)
service_addrs_all_present?(addrs, required_addrs)
service_addrs_all_present?(addrs, ['mysql'])
service_addrs_all_present?(addrs, ['mysqlx'])
xx = Pancakes::Stack.last
xx.units.first.last_health_check
['foo', 'var'].join
xx = Pancakes::Stack.last
xx.service_addrs(allow_nil: true)
doc = Nokogiri::HTML(tmpl_ec2_instance)
doc.css('#main-col-body .section')
doc.css('#main-col-body')
doc.to_s
require 'open-uri'
doc = Nokogiri::HTML(open(tmpl_ec2_instance))
doc.css('#main-col-body .section')
doc.css('#main-col-body .section').size
doc.css('#main-col-body .section').size.detect{|section| section.css('.titlepage').text =~ /Syntax/ }
doc.css('#main-col-body .section').detect{|section| section.css('.titlepage').text =~ /Syntax/ }
xx = doc.css('#main-col-body .section').detect{|section| section.css('.titlepage').text =~ /Syntax/ }
xx
xx = doc.css('#main-col-body > .section .section').detect{|section| section.css('.titlepage').text =~ /Syntax/ }
xx.css('pre')
code = xx.css('pre')
code.size
code = xx.css('pre').first
puts code.text
xx
ode
code
code.css('code').first
code.css('code').first.text
code.css('code').first.contents
code.css('code').first.to_s
code.css('code').first.inner_text
code = code.css('code').first
code.children
code.children = []
code.children = NodeSet.new
code.children = Nokogiri::XML::NodeSet.new
code.children = Nokogiri::XML::NodeSet.new([])
code.children = Nokogiri::XML::NodeSet.new()
code.children = Nokogiri::XML::NodeSet.new(Nokogiri::XML::Document.new)
code.text
code.children
code.children.text
puts code.text.sub(code.children.text, '')
code
code.children
code.children.to_a
code.children.first
code.children.first.class
code.children.select{|cc| cc.is_a?(Nokogiri::XML::Text) }
code.text
puts code.text
puts code.text.lines
puts code.text.lines.to_a
code.text.lines
code.text.lines.map(&:chomp)
code.text.lines.map(&:chomp).reject(&:empty?)
parser.parse('{}')
parser.parse('"foo"')
parser.parse('""')
parser.parse('"foo"')
xx - parser.parse('"foo"')
xx = parser.parse('"foo"')
xx.to_s
ls xx
xx.text_value
xx.word
xx.input
xx.dot_it
xx.dot_id
xx.wrote_dot_file
xx.write_dot
ls xx.write_dot
cat xx.write_dot
xx = parser.parse('"foo"')
xx = parser.parse('"----"')
xx = parser.parse('"_"')
xx = parser.parse('"000"')
xx = parser.parse('"00::0"')
xx = parser.parse('"foo": "bar"')
xx = parser.parse('"foo" : "bar"')
xx = parser.parse('"foo" "bar"')
xx = parser.parse('"foo"')
xx = parser.parse('""')
xx = parser.parse('"foo"')
Treetop.load('grammars/cfn_syntax.treetop')
xx = parser.parse('"foo"')
Treetop.load('grammars/cfn_syntax.treetop')
xx = parser.parse('"foo"')
xx = parser.parse('""')
xx = parser.parse('word')
parser.parse('word')
parser
parser.parse('word')
parser.parse(':')
xx = parser.parse(':')
xx.class
xx
ls xx
xx.inspect
xx.elements
Treetop.load('grammars/cfn_syntax.treetop')
xx = parser.parse(':')
ls xx
xx = parser.parse(':')
ls xx
xx = parser.parse(':')
xx = parser.parse('"foo"')
xx = parser.parse('"foo":"bar"')
xx = parser.parse('"foo": "bar"')
xx = parser.parse('"foo" : "bar"')
xx = parser.parse('"foo": "bar"')
xx = parser.parse('"foo" : "bar"')
xx = parser.parse('"foo" "bar"')
xx = parser.parse('"foo"')
xx = parser.parse('"foo" "foo"')
xx = parser.parse('"foo""foo"')
xx = parser.parse('"foo":"foo"')
xx = parser.parse('{"foo":"foo"}')
xx = parser.parse('{"foo""foo"}')
xx = parser.parse('{"foo":"foo"}')
xx = parser.parse('{"foo""foo"}')
ls xx
ls parser
parser.failure_reason
xx = parser.parse('{"foo":"foo"}')
xx = parser.parse('{"foo":"foo", }')
xx = parser.parse('{"foo":"foo", "bar": "bar"}')
xx = parser.parse('{"foo":"foo", "bar":"bar"}')
xx = parser.parse('{"foo":"foo","bar":"bar"}')
xx = parser.parse('{"foo" :  "foo", "bar"    :            "bar"}')
xx = parser.parse('{"foo" :  "foo", "bar"    :            "bar"     }')
xx = parser.parse('{        "foo" :  "foo", "bar"    :            "bar"     }')
xx = parser.parse('{        "foo" :  "foo", "bar"    :            "bar    "     }')
c
cd
cd ..
parser.parse('{}')
parser.parse('{"foo":"bar"}')
c
xx = parser.parse(input)
ls xx
xx.inspect
ls xx
xx.elemnts
xx.elements
xx.key_value_pair
xx.object_innards
xx.whitespace1object_innards
xx.whitespace1
xx.whitespace2
parser.terminal_failures
parser.failure_index
parser.failure_line
parser.failure_reason
result
c
result
parser.failure_reason
code
code.text
xx = parser.parse(code)
parser.failure_reason
xx = parser.parse(code)
xx.to_a
xx.children
xx.elements
xx.elements.to_a
xx.elements.flatten
xx = parser.parse(code)
ASTCleaner.clean(xx)
xx = parser.parse(code)
ASTCleaner.clean(xx)
xx = parser.parse(code)
ASTCleaner.clean(xx)
xx = parser.parse(code)
ASTCleaner.clean(xx)
xx = parser.parse(code)
root_node.elements.delete_if{|node| node.class.name == 'Treetop::Runtime::SyntaxNode' }
xx.elements.delete_if{|node| node.class.name == 'Treetop::Runtime::SyntaxNode' }
xx = parser.parse(code)
xx.elements.delete_if{|node| node.class.name == 'Treetop::Runtime::SyntaxNode' }
xx
xx.class
xx.class.name
xx.elements.first
xx = parser.parse(code)
xx.elements.first
xx.elements
xx.elements[1]
xx.elements[2]
ASTCleaner.clean(xx)
xx
xx = parser.parse(code)
ASTCleaner.clean(xx)
xx
puts xx
xx.elements
xx = parser.parse(code)
ASTCleaner.clean(xx)
xx.elements
xx = parser.parse('{}')
ASTCleaner.clean(xx)
xx.elements
xx = parser.parse('{}')
wtf?
xx = parser.parse('{}')
xx = parser.parse(code)
ASTCleaner.clean(xx)
xx
xx.elemnts
xx.elements
xx = parser.parse(code)
ASTCleaner.clean(xx)
xx
xx = parser.parse(code)
ASTCleaner.clean(xx)
xx.elements
xx.elements.last
xx.elements.last.last
xx.elements.last.elements
xx.elements.last.elements.last
xx.elements.last.elements.last.elements
xx = parser.parse(code)
xx
xx.class
ls xx
xx.extension_modules
xx.elements.first.extension_modules
xx = parser.parse('{}')
x = 1 && y = 2
x = 1 && y = 2 ; puts x, y
x = 1 && y = 2 ; puts "x=#{x}, y"
x = 1 && y = 2 ; puts "x=#{x}, y=#{y}"
(x = 1) && (y = 2) ; puts "x=#{x}, y=#{y}"
x = 1 &&  2 ; puts "x=#{x}, y=#{y}"
(x = 1) && 2 ; puts "x=#{x}, y=#{y}"
x = 1 && 2 ; puts "x=#{x}, y=#{y}"
x = 1 and 2 ; puts "x=#{x}, y=#{y}"
x = 1 and y=2 ; puts "x=#{x}, y=#{y}"
a = 1 && b = 2 ; puts "a=#{a}, b=#{b}" ; x = 1 and y = 2 ; puts "x=#{x}, y=#{y}"
a
ttt
postpone_expert_selection_case_path(kase.id)
post postpone_expert_selection_case_path(kase.id)
post :postpone_expert_selection
post :postpone_expert_selection, case_id: 'foo'
post :postpone_expert_selection, cid: 'foo'
post :postpone_expert_selection, id: 'foo'
postpone_expert_selection_case_path()
postpone_expert_selection_case_path('foo')
post postpone_expert_selection_case_path(kase.id)
post :postpone_expert_selection, id: kase.id
post postpone_expert_selection_case_path(kase.id)
controller
controller(CasesController)
controller(CasesController) {}
controller
controller.class
c
f
whereami
c
whereami
f
help
foo
ls foo
foo.object
ls foo.object
c
params
@case
params[:case]
params[:case][:case_panel_expert]
params[:case][:case_panel_expert][:doctor_info]
params[:case][:case_panel_expert][:doctor_info][:npi]
DoctorInfo.find_by_npi(0000000075)
xx = DoctorInfo.find_by_npi(0000000075)
xx.case
@case.case_panel_experts
@case.case_panel_experts.build()
@case.case_panel_experts.create!()
@case.case_panel_experts.create!(doctor_info: xx)
@case.case_panel_experts.create!(doctor_info_id: xx.id)
@case.case_panel_experts
foo = @case.case_panel_experts.create(doctor_info: xx)
foo.new_record?
foo.save
foo.valid?
foo.errors
xx = DoctorInfo.find_by_npi(0010000075)
cc
c
DoctorInfo.find_by_npi(params[:case][:case_panel_expert][:doctor_info][:npi])
flsh = {}
redirect_to admin_stark_case_path(@case), flash: flsh
c
flash
c
flash
c
params
exit-program
params
c
params
params[:cargo_debug]
c
params[:cargo_debug]
@stack
@stack.carg
exit-program
@stack
@stack.carg
exit-program
doctor_info = DoctorInfo.find_by_npi(params[:case][:case_panel_expert][:doctor_info][:npi])
params[:case][:case_panel_expert][:doctor_info][:npi]
DoctorInfo.all.map(&:npi)
DoctorInfo.all.map(&:npi).count
c
mk_npi
whereami
CasePanelExpert.last
CasePanelExpert.all
doctor_info
case_panel_experts.create(doctor_info: doctor_info, user_specified: true)
doctor_info.id
foo = case_panel_experts.create(doctor_info: doctor_info, user_specified: true)
foo
foo.doctor_info
foo = case_panel_experts.create(doctor_info: doctor_info, user_specified: true)
foo.doctor_info
foo.reload
foo.doctor_info
foo = case_panel_experts.create(doctor_info: nil, user_specified: true)
foo
foo.valid?
foo.new_record?
foo = case_panel_experts.create!(doctor_info: nil, user_specified: true)
foo = create_case_panel_expert(doctor_info: nil, user_specified: true)
foo = create_case_panel_experts(doctor_info: nil, user_specified: true)
include HonestyCluster::SwarmBackend::DockerUtil
containers_filtered
containers_filtered(          'cargo-type' => cargo_type,)
'cargo-id'   => cargo_id,
containers_filtered(      'cargo-id'   => 730    'cargo-type' => 'pancakes-service')
containers_filtered(      'cargo-id'   => 730,    'cargo-type' => 'pancakes-service')
containers_filtered('cargo-id'   => 730,    'cargo-type' => 'pancakes-service')
foo = containers_filtered('cargo-id'   => 730,    'cargo-type' => 'pancakes-service')
foo.first
foo
foo = containers_filtered('cargo-id'   => '730',    'cargo-type' => 'pancakes-service')
foo = containers_filtered('cargo-group-id'   => '730',    'cargo-type' => 'pancakes-service')
foo.last.port_mapping
foo[0].port_mapping
foo[0].info
foo[0].info.Ports
foo[0].port_mapping
foo = containers_filtered('cargo-group-id'   => '777',    'cargo-type' => 'pancakes-service')
include HonestyCluster::SwarmBackend::DockerUtil
foo = containers_filtered('cargo-group-id'   => '777',    'cargo-type' => 'pancakes-service')
xx = foo[11]
xx.port_mapping
xx.first_port_mapping
xx.port_mapping
xx = Integrity::Build.last
xx
xx.project
xx = Integrity::Build.last
xx.project
xx = Integrity::Build.get(18198)
xx.project
xx.repo
xx.repo.name
xx.repo.full_name
rr = Integrity::RepoConfig.fetch(xx.repo.full_nam)
rr = Integrity::RepoConfig.fetch(xx.repo.full_name)
rr = Integrity::RepoConfig.fetch(Integrity::Build.get(18198).repo.full_name)
rr.config
rr = Integrity::RepoConfig.fetch(Integrity::Build.get(18198).repo.full_name, 'cargo-testing')
rr = Integrity::RepoConfig.new(Integrity::Build.get(18198).repo.full_name, 'cargo-testing')
rr.config
rr = Integrity::RepoConfig.new(Integrity::Build.get(18198).repo.full_name, 'cargo-testing')
rr.honesty
rr.config.honesty
rr.config.honesty.cargo
rr.config.honesty.cargo.branch
Integrity::Build.get(18198).branch_name
Integrity::Build.get(18198).repo_config
rr = Integrity::Build.get(18198).repo_config
rr.honesty.cargo.branch
rr.honesty.cargo.branch_name
rr.config.honesty.cargo
rr.config
rr.config.honesty.cargo
rr = Integrity::RepoConfig.new(Integrity::Build.get(18198).repo.full_name, 'cargo-testing')
rr.config.honesty.cargo
rr.config.honesty.cargo.branchx
rr.config.honesty.cargox.branchx
rr = Integrity::Build.get(18198).repo_config
rr.repo_config_branch_name
bb = Integrity::Build.get(18198)
bb.repo_config_cargo_branch
raw = File.read('/tmp/payloads/payload_20151106T02:33:07.json')
raw = File.read('tmp/payload_20151106T02:33:07.json')
puts raw
xx = Integrity::Payload.new(raw)
xx.project
xx.repo
xx.commit
xx.commits
yy = Integrity::ProjectFinder.new(xx.repo)
yy.project
repo.uri
xx.repo.uri
xx.repo.uri.to_s
raw = File.read('tmp/payload_20151106T02:33:07.json')
xx = Integrity::Payload.new(raw)
xx.project.name
xx.project
Integrity::Project.last
Integrity::Project.last.name
File.open('/tmp/baz.fifo', 'r'){|f| loop{ f.read(1) } }
File.open('/tmp/baz.fifo', 'r'){|f| loop{ puts f.read(1) } }
Integrity::BundleCacheLock.where(service: 'jarvis')
Integrity::BundleCacheLock.get(service: 'jarvis')
Integrity::BundleCacheLock.find(service: 'jarvis')
Integrity::BundleCacheLock.first(service: 'jarvis')
Integrity::BundleCacheLock.first(project: 'jarvis')
Integrity::BundleCacheLock.all(service: 'jarvis')
Integrity::BundleCacheLock.find(service: 'jarvis')
Integrity::BundleCacheLock.all(project: 'jarvis')
DataMapper.auto_upgrade!
Integrity::Lock.all(name: 'jarvis')
Integrity::Lock.insert(name: 'jarvis')
Integrity::Lock.create(name: 'jarvis')
Integrity::Lock.create!(name: 'jarvis')
xx = Integrity::Lock.create(name: 'jarvis')
xx.save!
xx.save
Time.now.utc
Integrity::Lock.create(name: 'jarvis', locked_at: Time.now.utc)
Integrity::Lock.find(name: 'jarvis').update(locked_at: Time.now.utc)
Integrity::Lock.first(name: 'jarvis').update(locked_at: Time.now.utc)
Integrity::Lock.update(locked_at: Time.now.utc)
Integrity::Lock.all.update(locked_at: Time.now.utc)
DataMapper.repository(:default).adapter
adapter = DataMapper.repository(:default).adapter
adapter
adapter.execute("UPDATE integrity_locks SET locked_at=UTC_TIMESTAMP() WHERE name=?", 'foo')
adapter.execute("UPDATE integrity_locks SET locked_at=UTC_TIMESTAMP() WHERE name=?", 'jarvis')
adapter.execute("UPDATE integrity_locks SET locked_at=UTC_TIMESTAMP() WHERE name=?", 'jarvis') ; nil
adapter.execute("UPDATE integrity_locks SET locked_at=UTC_TIMESTAMP() WHERE name=? AND locked_at ", 'jarvis') ; nil
adapter = DataMapper.repository(:default).adapter
DataMapper.auto_upgrade!
adapter = DataMapper.repository(:default).adapter
res = adapter.execute("UPDATE integrity_locks SET locked_at=UTC_TIMESTAMP() WHERE TIME_TO_SEC(TIMEDIFF(UTC_TIMESTAMP(), locked_at)) <= ttl", 'jarvis')
res = adapter.execute("UPDATE integrity_locks SET locked_at=UTC_TIMESTAMP() WHERE name=? AND TIME_TO_SEC(TIMEDIFF(UTC_TIMESTAMP(), locked_at)) <= ttl", 'jarvis')
res
res = adapter.execute("UPDATE integrity_locks SET locked_at=UTC_TIMESTAMP() WHERE name=? AND TIME_TO_SEC(TIMEDIFF(UTC_TIMESTAMP(), locked_at)) > ttl", 'jarvis')
res = adapter.execute("UPDATE integrity_locks SET locked_at=UTC_TIMESTAMP() WHERE name=? AND TIME_TO_SEC(TIMEDIFF(UTC_TIMESTAMP(), locked_at)) > 3", 'jarvis')
adapter.execute("UPDATE integrity_locks SET locked_at=UTC_TIMESTAMP() WHERE name=? AND TIME_TO_SEC(TIMEDIFF(UTC_TIMESTAMP(), locked_at)) > 3", 'jarvis').affected_rows
adapter.execute("INSERT INTO integrity_locks (name, locked_at) VALUES (?, UTC_TIMESTAMP())", 'jarvis')
adapter.execute("INSERT INTO integrity_locks (name, locked_at) VALUES (?, UTC_TIMESTAMP())", 'jarvis2')
adapter.execute("INSERT INTO integrity_locks (name, locked_at) VALUES (?, UTC_TIMESTAMP())", 'jarvis2').affected_rows == 1
adapter.execute("INSERT INTO integrity_locks (name, locked_at) VALUES (?, UTC_TIMESTAMP())", 'jarvis3').affected_rows == 1
xx = Pancakes::Stack.last
xx.units.first
xx.units[1]
xx.units[2]
yy = xx.units[2]
yy.ports
xx.units[2].ports
xx.units[3].ports
xx.units[4].ports
xx = Pancakes::Stack.last
xx.units[2]
yy = xx.units[2]
yy.ping_url_public_port
yy.ping_url_private_port
yy.ping_url_public_port
yy.to_address_hash
require 'fileutils'
FileUtils.cd
FileUtils.cd 'foo'
fg
FileUtils.cp_r '../../monger/deploy', 'monger'
FileUtils.cdw
FileUtils.cwd
FileUtils.pwd
File.open
File.open('../../monger/deploy
')
File.openFile.join(Rails.root,'../../monger/deploy'))
File.open(File.join(Rails.root,'../../monger/deploy'))
File.open('../monger/deploy
')
FileUtils.pwd
q
xx = Pancakes::Stack.create_with_git_strings(pocs: 'foo')
Pancakes::Stack.last.pull_images
Integrity::Project.all
Integrity::Project.all(name: 'puppet')
xx = Integrity::Project.get(15)
xx
xx.builds.last
xx.last_build
xx.branches
xx.branches.all(name: 'feature/108587242/provision-swarm-instances-with-puppet')
br = xx.branches.all(name: 'feature/108587242/provision-swarm-instances-with-puppet')
br.last_build
bb = br.last_build
bb
'pancakes_aff1bb1_1010_jarvis-delayed_job'
'pancakes_aff1bb1_1010_jarvis-delayed_job'.split('_', 3)
'pancakes_aff1bb1_1010_jarvis-delayed_job'.split('_', 4)
'pancakes_aff1bb1_1010_jarvis-delayed_job'.split('_', 4).drop(1)
xx = Pancakes::Stack.last
xx.units[3].service_name
xx.units[3].service_type
xx.units[3].subtype
xx.units[2].subtype
xx = %w[ jarvis  banyan  puppet  rogers  metal  engineering  cargo  frick  warehouse  twinkleshine  honesty  parker  tim  tp  service  black  bain stone  status  sendwithus  monger ]
xx.size
xx.map {|x| shorten(x) }
Pry.config.editor
edit shorten
shorten("bar')
shorten("bar")
def shorten(x) ; end
edit shorten
shorten
shorten(3)
xx.map {|x| shorten(x) }
fg
edit shorten
shorten(3)
edit shorten
edit :shorten
edit shorten
edit -p shorten
xx.map {|x| shorten(x) }
edit -p shorten
xx.map {|x| shorten(x) }
"asdf".take(1)
"asdf".to_a.take(1)
"asdf".chars.take(1)
fg
edit -p shorten
xx = %w[ jarvis  banyan  puppet  rogers  metal  engineering  cargo  frick  warehouse  twinkleshine  honesty  parker  tim  tp  service  black  bain stone  status  sendwithus  monger ]
xx.map {|x| shorten(x) }
edit -p shorten
def shorten(x) ; end
edit -p shorten
"asdf".chars
"asdf".chars.to_s
"asdf".chars
"asdf".chars.to_a
"asdf".chars.join('')
"asdf".chars.to_a.join('')
"asdf".chars.to_a.join()
"asdf".chars.to_a.join
"asdf".chars.to_a.take(1)
"asdf".chars.to_a.take(1) + [2]
"asdf".chars.to_set
"asdf".chars.to_a.to_set
require 'set'
"asdf".chars.to_a.to_set
"asdf".chars.to_a.to_set.intersection(['a'].to_set)
"asdf".chars.to_a.to_set.intersection(['a'].to_set).empty?
nil.to_s
nil.inspect
"nil".inspect
format('%s %d', "foo", 3)
ls notification
ls notification.example
notification.example.to_h
notification.example.metadata
notification.example.clock
notification.example.file_path
notification.example.location
ls notification
ls notification.example
notification.example.example_group
ls notification.example.example_group
notification.example.example_group.to_s
notification.example.example_group.parent_groups
notification.example.example_group.description
c
require 'rake'
Rake::Task
Rake::Task.al
Rake::Task.all
ls notification.example
ls notification.example.example_group
notification.example.example_group
notification.example.example_group.description
notification.example.example_group.parent_group.description
ls notification.example.example_group
ls notification.example.example_group.parent_groups
notification.example.example_group.parent_groups
notification.example.example_group.parent_groups.map(&:description)
notification.example.example_group.parent_groups.map(&:description).reverse
notification.example.location
Time.now.utc
Time.now.utc.strftime("%Y")
Time.now.utc.strftime("%Y-%m")
Time.now.utc.strftime("%Y-%B")
Time.now.utc.strftime("%Y-%B").downcase
Time.now.utc.strftime("%Y-%m")
Time.now.utc.strftime("rspec-%Y-%m")
def foo ; @xx ||= Time.now.utc.strftime("rspec-%Y-%m") ; end
foo
def foo ; @xx ||= Time.now.utc.strftime("rspec-%Y-%m-%s") ; end
foo
def foo ; @xx ||= Time.now.utc.strftime("rspec-%Y-%m-%s") ; end
foo
Time.now.utc.strftime("rspec-%Y-%m-%s")
Time.now.utc.strftime("rspec-%Y-%U")
Time.now.utc
Time.now.utc.to_i
ls PlaysExporter.new
exporter = PlaysExporter.new(0, 15000, 30000)
exporter.run
exporter = PlaysExporter.new(0, 15, 5)
exporter.run
exporter = PlaysExporter.new(0, 15000, 10_000_000)
exporter.run
exporter = PlaysExporter.new(0, 15000, 8_000_000)
@redis = Redis.new(url: ENV['REDIS_URL_EXPORTER'])
@redis.get("exporter-timing")
timing = @redis.get("exporter-timing").to_f ; count = @redis.get("exporter-count").to_i ; avg = timing / count ; puts format("count: %d\navg:   %.2fs\n\n", count, avg) ; nil
def foo ; timing = @redis.get("exporter-timing").to_f ; count = @redis.get("exporter-count").to_i ; avg = timing / count ; puts format("count: %d\navg:   %.2fs\n\n", count, avg) ; nil ; end
foo
loop { foo ; sleep 1 }
exporter.run
def foo ; timing = @redis.get("export-timing").to_f ; count = @redis.get("export-count").to_i ; avg = timing / count ; puts format("count: %d\navg:   %.2fs\n\n", count, avg) ; nil ; end
loop { foo ; sleep 1 }
def foo ; tot=8000000/15000 ; timing=@redis.get("export-timing").to_f ; count=@redis.get("export-count").to_i ; avg=timing/count ; puts format("count: %d\navg:   %.2fs\nest:   %.2f\n\n", count, avg, (tot-count)*avg) ; nil ; end
loop { foo ; sleep 1 }
def foo ; tot=8000000/15000 ; timing=@redis.get("export-timing").to_f ; count=@redis.get("export-count").to_i ; avg=timing/count ; puts format("count: %d\navg:   %.2fs\nest:   %.2fm\n\n", count, avg, ((tot-count)*avg)/60.0) ; nil ; end
loop { foo ; sleep 1 }
def foo ; tot=8000000.0/15000 ; timing=@redis.get("export-timing").to_f ; count=@redis.get("export-count").to_i ; avg=timing/count ; puts format("count: %d\navg:   %.2fs\nest:   %.2fm\n\n", count, avg, ((tot-count)*avg)/60.0) ; nil ; end
loop { foo ; sleep 1 }
def foo ; tot=8000000.0/15000 ; timing=@redis.get("export-timing").to_f ; count=@redis.get("export-count").to_i ; avg=timing/count ; puts format("count: %d\navg:   %.2fs\nest:   %.2fm\n\n", count, avg, ((tot-count)*avg)/60.0/20.0) ; nil ; end
loop { foo ; sleep 1 }
exporter = PlaysExporter.new(6_450_000, 15000, 8_000_000)
exporter.run
[1, 2, 3, 4, 5]
[1, 2, 3, 4, 5].map
[1, 2, 3, 4, 5].map{|x| puts x }
[1, 2, 3, 4, 5].map{|x| x }
[1, 2, 3, 4, 5].map{|x| :foo }
[1, 2, 3, 4, 5].map{|x| x*2 }
[1, 2, 3, 4, 5].map{|x| x*2 }.reduce(0){|acc,val| acc + val }
[1, 2, 3, 4, 5].map{|x| x*2 }.reduce(0){|acc,val| [val, acc] }
[1, 2, 3, 4, 5].map{|x| x*2 }.reduce(0){|acc,val| [val, acc].flatten }
def foo(x)
  puts x
end
foo("hi")
[1, 2, 3, 4, 5].map{|x| x*2 }.reduce{|acc,val| [val, acc].flatten }
[[1, 2], [3, 4], [5, 6]].map{|x| x*2 }.reduce{|acc,val| [val, acc].flatten }
[[1, 2], [3, 4], [5, 6]].map{|x| x*2 }.reduce{|acc,val|  }
[[1, 2], [3, 4], [5, 6]].map{|x| x*2 }.reduce{|acc,val| [] }
[[1, 2], [3, 4], [5, 6]].map{|x| x*2 }.reduce(&:+)
[1, 2, 3, 4, 5].map{|x| x*2 }.reduce(&:+)
[1, 2, 3, 4, 5].map{|x| x*1 }.reduce(&:+)
[1, 2, 3, 4, 5].map{|x| x*2 }.reduce(&:+)
[1, 2, 3, 4, 5].reduce(&:+)*2
[1, 2, 3, 4, 5].reduce(&:+) * 2
Time.now.utc.year
Time.now.utc.mont
Time.now.utc.month
Time.now.utc.strftime("%YYYY")
Time.now.utc.strftime("%YM")
Time.now.utc.strftime("%Y-%M")
Time.now.utc.strftime("%Y-%m")
exporter = PlaysExporter.new(0, 15000, 7_517_101)
exporter.redis.ping
exporter = PlaysExporter.new(0, 15000, 7_517_101)
exporter.run
exporter = PlaysExporter.new(0, 15000, 7_517_101)
ENV
ls exporter
exporter.send(:notify_done)
x = "\x16\x03\x01\x00\x95\x01\x00\x00\x91\x03\x03\x8B\x99\xA7\x049\xA3\xE1\xFA<1\xB5+]q\xB1\x95\x9B\x10\xE3\xF2\xF41\x84\xDD:\xD9X\xF1K\x90\x00/\x00\x00@\x00\xA3\x00\x9F\x00k\x00j\x009\x008\x00\x88\x00\x87\x00\x9D\x00=\x005\x00\x84\x00\xA2\x00\x9E\x00g\x00@\x003\x002\x00E\x00D\x00\x9C\x00<\x00/\x00A\x00\x05\x00\x16\x00\x13\x00"
x = "\x16\x03\x01\x00\x95\x01\x00\x00\x91\x03\x03\x8B\x99\xA7\x049\xA3\xE1\xFA<1\xB5+]q\xB1\x95\x9B\x10\xE3\xF2\xF41\x84\xDD:\xD9X\xF1K\x90\x00/\x00\x00@\x00\xA3\x00\x9F\x00k\x00j\x009\x008\x00\x88\x00\x87\x00\x9D\x00=\x005\x00\x84\x00\xA2\x00\x9E\x00g\x00@\x003\x002\x00E\x00D\x00\x9C\x00<\x00/\x00A\x00\x05\x00\x16\x00\x13\x00".pack("h*")
x = "\x16\x03\x01\x00\x95\x01\x00\x00\x91\x03\x03\x8B\x99\xA7\x049\xA3\xE1\xFA<1\xB5+]q\xB1\x95\x9B\x10\xE3\xF2\xF41\x84\xDD:\xD9X\xF1K\x90\x00/\x00\x00@\x00\xA3\x00\x9F\x00k\x00j\x009\x008\x00\x88\x00\x87\x00\x9D\x00=\x005\x00\x84\x00\xA2\x00\x9E\x00g\x00@\x003\x002\x00E\x00D\x00\x9C\x00<\x00/\x00A\x00\x05\x00\x16\x00\x13\x00".pack("H*")
(x = "\x16\x03\x01\x00\x95\x01\x00\x00\x91\x03\x03\x8B\x99\xA7\x049\xA3\xE1\xFA<1\xB5+]q\xB1\x95\x9B\x10\xE3\xF2\xF41\x84\xDD:\xD9X\xF1K\x90\x00/\x00\x00@\x00\xA3\x00\x9F\x00k\x00j\x009\x008\x00\x88\x00\x87\x00\x9D\x00=\x005\x00\x84\x00\xA2\x00\x9E\x00g\x00@\x003\x002\x00E\x00D\x00\x9C\x00<\x00/\x00A\x00\x05\x00\x16\x00\x13\x00").pack("H*")
x = "\x16\x03\x01\x00\x95\x01\x00\x00\x91\x03\x03\x8B\x99\xA7\x049\xA3\xE1\xFA<1\xB5+]q\xB1\x95\x9B\x10\xE3\xF2\xF41\x84\xDD:\xD9X\xF1K\x90\x00/\x00\x00@\x00\xA3\x00\x9F\x00k\x00j\x009\x008\x00\x88\x00\x87\x00\x9D\x00=\x005\x00\x84\x00\xA2\x00\x9E\x00g\x00@\x003\x002\x00E\x00D\x00\x9C\x00<\x00/\x00A\x00\x05\x00\x16\x00\x13\x00"
[x].pack("H*")
x.to_a.pack("H*")
x.chars.pack("H*")
x.chars
name = -2016-01-13-1309-nyc3'
name = '
quux-2016-01-13-1408-nyc3'
name = '
name = 'quux-2016-01-13-1408-nyc3'
include Meatballs::DropletActions
include Meatballs::Droplets
get_droplet_by_name(name)
include Meatballs::Util
get_droplet_by_name(name)
dd = get_droplet_by_name(name)
name
rename_droplet(dd, name)
foo = Object.new
foo
Play
AWS
AWS::S3.new
x = AWS::S3.new
x
ls x
x.buckets
x.buckets.to_a
y = Aws::S3::Client.new
y.buckets
ls y
y.list_buckets
x.buckets.to_a
datapipeline
Librato
Librato::Metrics
Librato::Metrics.submit foo: 42
ls Librato::Metrics
HonestyCluster::SwarmBackend.node_count
false.to_i
ls datapipeline
query_topk("eBOFBmryw6rtXDs7fcr7yfXM9zyLdt8K")
query_topk(client, "eBOFBmryw6rtXDs7fcr7yfXM9zyLdt8K")
xx = query_topk(client, "eBOFBmryw6rtXDs7fcr7yfXM9zyLdt8K")
xx.topk
xx.item
xx.item.topk
xx.item["topk"]
xx.item["topk"].first
puts xx.item["topk"].first
xx.item["topk"].first.to_s
"%d" % xx.item["topk"].first
xx.item["topk"].map{|x| x.to_s }
xx.item["topk"].map{|x| x.to_int }
xx = query_topk(client, "eBOFBmryw6rtXDs7fcr7yfXM9zyLdt8K")
topk_npis(client, xx)
xx = query_topk(client, "eBOFBmryw6rtXDs7fcr7yfXM9zyLdt8K")
xx = table_attrs(client, 'Providers3')
xx
xx.attribute_definitions
xx.table
xx.table.attribute_definitions
xx.table.attributes
xx.table
d = FactoryGirl.create :data_source, :with_customer
d.url
d.url = 
d.url = "sftp://engineering:bXi*LJ2C+gjLwSE.@ftp.grandrounds.com/bird/"
d.save
xx = query_topk(client, "eBOFBmryw6rtXDs7fcr7yfXM9zyLdt8K")
xx = query_topk(client, "8Yioe9cdT1bW7DejdmIGciWEdwzPilw1")
xx = query_topk(client, "lMEKK0yk01vOUM7i3aI5qhdEDysJGNtU")
xx = query_topk(client, "lMEKK0yk01vOUM7i3aI5qhdEDysJGNtU") ; nil
Time.now
Time.now.to_s
Time.now.to_i
Time.now.to_f
Time.now.to_f * 1000
(Time.now.to_f * 1000).to_i
time {sleep 1}
top_pids
[:a, :b, :c].repeated_combination
[:a, :b, :c].repeated_combination(1)
[:a, :b, :c].repeated_combination(1).to_a
[:a, :b, :c].repeated_combination(4).to_a
[:a, :b, :c].repeated_combination(3).to_a
[:a, :b, :c].repeated_permutation(3).to_a
[:a, :b, :c].repeated_combination(3).to_a
[:a, :b, :c].repeated_combination(3).to_a.flatten
[:a, :b, :c].repeated_combination(3).to_a.flatten.rand
[:a, :b, :c].repeated_combination(3).to_a.flatten.random
[:a, :b, :c].repeated_combination(3).to_a.flatten.randomize
[:a, :b, :c].repeated_combination(3).to_a.flatten
[:a, :b, :c].repeated_combination(3).to_a.flatten.partition(3)
[:a, :b, :c].repeated_combination(3).to_a.flatten.partition()
[:a, :b, :c].repeated_combination(3).to_a.flatten.partition.to_a
[1, 5].mean
[1, 5].avg
[1, 5].average
mean(time_topks)
mean(time_topks(top_pids))
mean(time_topks(client, top_pids))
Octokit.config
ls Octokit
ls Octokit.client
ls Octokit.client.content("ConsultingMD/ConsultingMD")
ls Octokit.client.content("ConsultingMD/ConsultingMD", { path: 'Gemfile', ref: 'master' })
event
n
next
workflow_event.process
next
s
step
finish
step
finish
ex
c
continue
step
finish
next
process_definition
continue
Docker.info
Docker.creds
Docker.creds = :foo
Docker.creds
image = Docker::Image.create('fromImage' => 'grnds/jarvis_ci:latest')
Docker::Image.create('fromImage' => 'grnds/jarvis_ci:latest')
Docker.creds = nil
Docker::Image.create('fromImage' => 'grnds/jarvis_ci:latest')
xx = Docker::Image.create('fromImage' => 'grnds/jarvis_ci:latest')
xx
xx.push
xx = Docker::Image.get('grnds/jarvis_ci:latest')
xx
xx.push
foo = xx.push
xx.push {|x| puts x }
xx.push({"username" => "grnds", "email" => "ops@grnds.com", "password" => "TiQM*wOIjwcnIRET"}) {|x| puts x }
xx.push({"username" => "grnds", "email" => "ops@grnds.com", "password" => "TiQM*wOIjwcnIRETx"}) {|x| puts x }
ls Aws::ECS
ls Aws::ECS::Client
ls Aws::ECR
ls Aws::ECR::Client
Aws::ECR::Client.get_authorization_token
Aws::ECR::Client.new.get_authorization_token
Aws::ECR.client.new.get_authorization_token
Aws::ECR::Client.new.get_authorization_token
Aws::ECR::Client.new.get_authorization_token[:authorization_data]
Aws::ECR::Client.new.get_authorization_token
xx = Aws::ECR::Client.new.get_authorization_token
xx
xx[:authorization_data]
Aws::ECR::Client.new.get_authorization_token[:authorization_data]
Aws::ECR::Client.new.get_authorization_token.authorization_data
Aws::ECR::Client.new.get_authorization_token.authorization_data.authorization_token
Aws::ECR::Client.new.get_authorization_token.authorization_data[0].authorization_token
Base64.decode64(Aws::ECR::Client.new.get_authorization_token.authorization_data[0].authorization_token)
Base64.decode64(Aws::ECR::Client.new.get_authorization_token.authorization_data[0].authorization_token).split(':')
xx = CacheContainerManager.new
xx = CacheContainerManager.new("master")
cache_key
sha
xx = CacheContainerManager.new("09599e6093a4b6210cdf9cb944661cf3531b0578")
xx = CacheContainerManager.new("master")
container = Docker::Container.create(docker_json, HonestyCluster::SwarmBackend.docker_conn_manager.conn(:engine))
xx = CacheContainerManager.new("master")
n
c
xx = CacheContainerManager.new("master")
c
xx = CacheContainerManager.new("master")
n
c
xx = CacheContainerManager.new("master")
n
c
xx = CacheContainerManager.new("master")
Docker::Container.create(docker_json, HonestyCluster::SwarmBackend.docker_conn_manager.conn(:engine))
Docker::Container.create(docker_json, HonestyCluster::SwarmBackend.docker_conn_manager.conn(:swarm))
HonestyCluster::SwarmBackend.docker_conn_manager.conn(:swarm)
HonestyCluster::SwarmBackend.docker_conn_manager.conn(:engine)
Docker::Container.create(docker_json, HonestyCluster::SwarmBackend.docker_conn_manager.conn(:engine))
container.start!
cc = Docker::Container.create(docker_json, HonestyCluster::SwarmBackend.docker_conn_manager.conn(:engine))
cc.start!
xx = CacheContainerManager.new("master")
n
c
xx = CacheContainerManager.new("master")
n
f
c
xx = CacheContainerManager.new("master")
n
exit_code
xx = CacheContainerManager.new("master")
c
xx = CacheContainerManager.new("master")
xx = CacheContainerManager.new("70abfe02389c25ec931e03bd0e333663e3f57b20")
fg
xx = CacheContainerManager.new("70abfe02389c25ec931e03bd0e333663e3f57b20")
def sleep_t(n) ; Thread.new { puts "start #{n}, sleep 10." ; sleep 10 ; puts "end #{n}" } ; end
sleep_t(1)
ls
sleep_t(1)
sleep_t(2)
sleep_t(3)
sleep_t(2)
sleep_t(1)
sleep_t(2)
5.times { |n| sleep_t(n) }
puts "foo
"
DataMapper.auto_upgrade!
Dir['/tmp/payloads']
Dir['/tmp/payloads'].to_a
Dir['/tmp/payloads/*.json'].each
xx = load_payloads('/tmp/payloads')
load './script/payloads'
load './script/payloads.rb'
xx = load_payloads('/tmp/payloads')
load './script/payloads.rb'
xx = load_payloads('/tmp/payloads')
load './script/payloads.rb'
xx = load_payloads('/tmp/payloads')
load './script/payloads.rb'
xx = load_payloads('/tmp/payloads')
load './script/payloads.rb'
xx = load_payloads('/tmp/payloads') ; nil
xx.size
xx.first
xx.last
xx.select {|x| x["action"] != "opened" }.last
xx.select {|x| x["action"] != "opened" && !x["commits"].empty? }.last
xx.select {|x| x["action"] != "opened" && !x["commits"].try(:empty?) }.last
xx.select {|x| x["action"] != "opened" && !x["commits"].try!(:empty?) }.last
xx.select {|x| x["action"] != "opened" && x["commits] && !x["commits"].empty? }.last
xx.select {|x| x["action"] != "opened" && x["commits"] && !x["commits"].empty? }.last
xx = load_payloads('/tmp/payloads') ; nil
load './script/payloads.rb'
xx = load_payloads('/tmp/payloads') ; nil
xx.last
prs(xx).last
load './script/payloads.rb'
prs(xx).last
require 'rest-client'
xx = load_payloads('/tmp/payloads') ; nil
prs(xx).last
prs(xx).detect {|x| x[:parsed]["repository"] }
prs(xx).detect {|x| x[:parsed]["repository"]["name"] == "stone" }
load './script/payloads.rb'
repo(prs(xx), 'stone')
repo(prs(xx), 'stone').first
hitup_honesty repo(prs(xx), 'stone').first
load './script/payloads.rb'
hitup_honesty repo(prs(xx), 'stone').first
load './script/payloads.rb'
hitup_honesty repo(prs(xx), 'stone').first
load './script/payloads.rb'
repo(prs(xx), '9d2bf9d').first
repo_commit(prs(xx), '9d2bf9d').first
repo_commit(prs(xx), 'stone', '9d2bf9d').first
hitup_honesty repo_commit(prs(xx), 'stone', '9d2bf9d').first
DataMapper.auto_upgrade!
Integrity::Build.last
DataMapper.auto_upgrade!
Integrity::Build.get(23548)
Integrity::Build.get(23548).branch
xx = Integrity::Build.get(23548)
xx.docker_json
xx.datastore_cargo
xx.datastore_cargo.docker_json
Docker::Container.create(xx.datastore_cargo.docker_json)
Docker::Container.create(xx.datastore_cargo.docker_json).start!
xx.datastore_cargo.docker_json
jj = xx.datastore_cargo.docker_json
jj[:Hostname]
jj[:Hostname] = "jarvis_845eb8d_23548_mysql_jarvis_app_d9d8bc6668cf10c8da7da1359dd0fab891961bf2"
jj
Docker::Container.create(xx.datastore_cargo.docker_json).start!
Docker::Container.create(jj).start!
jj = xx.datastore_cargo.docker_json
jj
jj["name"] = "jarvis_845eb8d_23548_mysql_jarvis_app_d9d8bc6668cf10c8da7da1359dd0fab891961bf2"
jj
Docker::Container.create(jj).start!
xx = Integrity::Commit.last
DataMapper.auto_upgrade!
xx = Integrity::Commit.last
xx.cache_key
Integrity::Project.get(10)
jj = Integrity::Project.get(10)
jj.branch
jj.branches
jj.branches.detect{|x| x.name == 'master' }
br = jj.branches.detect{|x| x.name == 'master' }
br.commits.last
br.builds.last
br.builds.last.commit
ci = br.builds.last.commit
ci.cache_key
DataMapper.auto_upgrade!
jj = Integrity::Project.get(10)
br = jj.branches.detect{|x| x.name == 'master' }
ci = br.builds.last.commit
ci.cache_key
DataMapper.auto_upgrade!
jj = Integrity::Project.get(10)
br = jj.branches.detect{|x| x.name == 'master' }
ci = br.builds.last.commit
ci.cache_key
ci.reload
ci.cache_key
Docker::Container.all.select{ |container| container.info.Name =~ Regexp.new('swarm-ip-10-0-80-7.hq.test/b23547-222f') }.empty?
Docker::Container.all.select{ |container| container.info.Name =~ Regexp.new('swarm-ip-10-0-80-7.hq.test/b23547-222f') }
Docker::Container.all.select{ |container| container.info.Name =~ Regexp.new('b23547-222f') }
Docker::Container.all.count
Docker::Container.all(all: true).count
Docker::Container.all(all: true).select{ |container| container.info.Name =~ Regexp.new('swarm-ip-10-0-80-7.hq.test/b23547-222f') }
Docker::Container.all(all: true).select{ |container| container.info.Name =~ Regexp.new('b23547-222f') }
ENV
ENV['DOCKER_HOST']
Docker::Container.all(all: true)
Docker::Container.all(all: true).first
Docker::Container.all(all: true).select{ |container| container.info.Names.first =~ Regexp.new('b23547-222f') }
jj = Integrity::Project.get(10)
br = jj.branches.detect{|x| x.name == 'master' }
br.builds.last
xx = br.builds.last
xx.cache_container_manager
xx.cache_container_manager.cache_key
xx.cache_container_manager.container
xx.cache_container_manager.run_container
xx.cache_container_manager.container
xx.cache_container_manager.container.exited?
jj = Integrity::Project.get(10)
br = jj.branches.detect{|x| x.name == 'master' }
xx = br.builds.last
xx.cache_container_manager.container
xx.cache_container_manager.container.exited?
Pancakes::Fleet::Templating.config.cargo_branch
Integrity::Build.last.cargo_branch
Integrity::Build.last.repo_config_cargo_branch
xx = Integrity::Build.last
xx.repo_config
xx = Integrity::Build.last
xx.cache_key
xx = Integrity::Background::CommitContainerJob.new(25393)
xx = Integrity::Background::CommitContainerJob.new("25393")
xx.perform
!!0
!1
!!1
xx = Integrity::Build.get(25396)
xx.cache_container_manager.image_ready?
xx.runnable?
xx = Integrity::Build.get(25396)
xx.runnable?
xx = Integrity::Build.get(25396)
xx = Integrity::Build.get(25397)
xx.successful = nil
xx.started_at = xx.completed_at = nil
xx.save
Docker::Container.all(all: true).select{ |container| container.info.Names.first =~ Regexp.new('b23547-222f') }
Docker::Container.all(all: true).last
Docker::Container.all(all: true).last.created
Docker::Container.all(all: true).last.exitstatus
Docker::Container.all(all: true).last.status
Docker::Container.all(all: true)[-2].status
Docker::Container.all(all: true)[-3].status
Delayed::Job.count
Delayed::Job.first
Pancakes::Services.buttermilk_services
Pancakes::Services.buttermilk_services.map{|service| service.split('-', 2) }
Pancakes::Services.buttermilk_services.map{|service| service.split('-', 2) }.select{|e| e.size >= 2 }
Pancakes::Services.buttermilk_services.map{|service| service.split('-', 2) }.select{|e| e.size >= 2 }.map(&:first)
Pancakes::Services.buttermilk_services.map{|service| service.split('-', 2) }.select{|e| e.size >= 2 }.map(&:first).uniq
Pancakes::Services.buttermilk_services.map{|service| service.split('-', 2) }.select{|e| e.size >= 2 }.map(&:first).uniq.sort
Pancakes::Services.honesty_services.map{|service| service.split('-', 2) }.select{|e| e.size >= 2 }.map(&:first).uniq.sort
nil.empty?
{a: "foo", b: "23"}.select{|x| true }
{a: "foo", b: "23"}.select{|x| {a: "foo"}.include?(x) }
{a: "foo", b: "23"}.select{|x| {a: "foox"}.include?(x) }
{a: "foo", b: "23"}.select{|x| {aa: "foox"}.include?(x) }
{a: "foo", b: "23"}.select{|x| {a: "foox"}.include?(x) }
{a: "foo", b: "23"}.select{|k,v| {a: "foox"}.include?(k) }
{a: "foo", b: "23"}.select{|k,v| {a: "foox"}.include?(v) }
{a: "foo", b: "23"}.select{|k,v| {a: "foox"}.has_key?(v) }
{a: "foo", b: "23"}.select{|k,v| {a: "foox"}.has_key?(k) }
Integrity::Project.first(name: 'honesty')
xx = Integrity::Project.first(name: 'honesty')
xx.uri
xx.url
xx.uri
puts xx.uri
xx.uri = 'git://github.com/ConsultingMD/honesty'
xx.save
Pancakes::Services.get('jarvis-dj')
Grnds::Service::Urls.port(:jarvis)
Grnds::Service::Urls.port(:honesty)
Pancakes::Services.service_def(:honesty)
Pancakes::Services.service_def(:honesty, 'setup')
Pancakes::Services.service_def(:honesty)
Pancakes::Services.service_def('honesty-dj')
Pancakes::Services.service_def('honesty-foo')
Grnds::Service::Urls.port(:jarvis)
Grnds::Service::Urls.env
Grnds::Service.env
ls Pancakes::Stack
ls Pancakes::Stack.create_with_commit_specs()
ls Pancakes::Stack.create_with_commit_specs
show Pancakes::Stack.create_with_commit_specs
ls Pancakes::Stack.create_with_commit_specs
Pancakes::Stack.create_with_commit_specs(owner: 'bird, stack_type: 'honesty', commit_specs: { honesty: 'master' })
Pancakes::Stack.create_with_commit_specs(owner: 'bird, stack_type: 'honesty', commit_specs: {honesty: 'master'})
Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'honesty', commit_specs: {honesty: 'master'})
ENV['RAILS_ENV']
Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'honesty', commit_specs: {honesty: 'master'})
rails.try(:env)
ENV['RAILS_ENV']
@env
Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'honesty', commit_specs: {honesty: 'master'})
@env
Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'honesty', commit_specs: {honesty: 'master'})
Pancakes::Stack[1419]
xx = Pancakes::Stack[1419]
xx.run_containers
xx = Pancakes::Stack[1419]
xx.destroy_containers
xx.run_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'honesty', carch_branch: 'feature/114986971/extract-setup-pattern', commit_specs: {honesty: 'master'})
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'honesty', cargo_branch: 'feature/114986971/extract-setup-pattern', commit_specs: {honesty: 'master'})
xx.run_containers
xx.destroy_containers
xx.run_containers
xx = Pancakes::Stack[1420]
xx.destroy_containers
xx.run_containers
xx = Pancakes::Stack[1420]
xx.destroy_containers
xx.run_containers
xx.destroy_containers
xx.run_containers
xx = Pancakes::Stack[1420]
xx.destroy_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'honesty', cargo_branch: 'feature/114986971/extract-setup-pattern', commit_specs: {honesty: 'master'})
xx.destroy_containers
xx.run_containers
xx = Pancakes::Stack[1421]
xx.destroy_containers
xx.run_containers
xx = Pancakes::Stack[1421]
xx.destroy_containers
xx.run_containers
ENV
xx = Pancakes::Stack[1421]
xx.destroy_containers
xx.run_containers
xx.destroy_containers
xx.run_containers
xx.destroy_containers
xx.run_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'honesty', cargo_branch: 'feature/114986971/extract-setup-pattern', commit_specs: {honesty: 'master'})
xx.run_containers
xx.destroy_containers
xx.run_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'buttermilk', cargo_branch: 'feature/114986971/extract-setup-pattern', commit_specs: {})
xx.run_containers
"foo-bar".split('-', 2)
"foobar".split('-', 2)
xx = Pancakes::Stack[1423]
xx.destroy_containers
xx.run_containers
xx = Pancakes::Stack[1423]
xx.restart_containers
xx.destroy_containers
xx.restart_containers
xx = Pancakes::Stack[1423]
xx.restart_containers
xx = Pancakes::Stack[1423]
xx.restart_containers
Pancakes::Stack::STACK_SCHEMAS
xx = Pancakes::Stack[1423]
xx.destroy_containers
HonestyCluster::SwarmBackend.ecr_creds
xx = Pancakes::Stack[1423]
xx.images
xx.image
xx.image_names
xx = Pancakes::Stack[1423]
xx.image_names
xx.pull_images
xx = Pancakes::Stack[1423]
xx.pull_images
xx = Pancakes::Stack[1423]
xx.image_names
Pancakes.config.aws_dkr_registry
case x= "foo" when "foo" then puts "hi" else puts "bar" end
case x= "foo" when "foo" then puts x else puts "bar" end
xx = Pancakes::Stack[1423]
xx.image_names
xx = Pancakes::Stack[1423]
xx.image_names
xx.pull_images
xx = Pancakes::Stack[1423]
xx.as_json
xx.name
xx = Pancakes::Stack[1423]
xx.as_json
xx = Pancakes::Stack[1423]
xx.as_json
xx.run_containers
xx.as_json
xx.tcp_info
xx = Pancakes::Stack[1423]
xx.as_json
xx.units
xx.containers
xx.containers.map(:name)
xx.containers.map(&:name)
xx.units
xx.units.map {|x| x.service_name }
xx.units.map {|x| x.service_name +  }
xx.units.map {|x| x.service_name + ' ' + x.container_name }
xx.units.map {|x| [x.service_name, x.container_name] }
xx.units.map {|x| [x.service_name, x.container_name] }.to_h
xx = Pancakes::Stack[1423]
xx.as_json
xx.restart_containers
xx.as_json
xx.services
xx.units
xx = Pancakes::Stack[1423]
xx.as_json
xx.units.first
xx.units.first.as_json
xx = Pancakes::Stack[1423]
xx.units.first.as_json
xx.units.first.to_json
xx.units.first.as_json
xx.units.first.as_json(except: [:id])
xx = Pancakes::Stack[1423]
xx.units.first.as_json(except: [:id])
xx.units.first.to_json(except: [:id])
xx = Pancakes::Stack[1423]
xx.units.first.to_json(except: [:id])
xx.units.first.as_json(except: [:id])
xx.units.first.as_json(only: [:id])
xx.units.first.as_json(only: :id)
xx = Pancakes::Stack[1423]
xx.units.first.as_json(only: :id)
xx.units.first.as_json
xx.units.first.as_json()
xx.units.first.as_json(only: :id)
xx.units.first.as_json(only: [:id])
xx.units.first.as_json(only: [:port])
xx.units.first.to_json(only: [:port])
xx.units.first.to_json(only: [:id])
JSON.parse xx.units.first.to_json(only: [:id])
JSON.parse xx.units.first.to_json(except: [:machine_ip])
Sequel::Migrator
xx = Pancakes::Stack[1423]
JSON.parse xx.units.first.to_json(except: [:machine_ip])
xx = Pancakes::Stack[1423]
JSON.parse xx.to_json
xx = Pancakes::Stack[1423]
JSON.parse xx.to_json
xx = Pancakes::Stack[1423]
JSON.parse xx.to_json
xx.name
JSON.parse xx.to_json
JSON.parse xx.to_json(include: :name)
JSON.parse xx.to_json(include: [:name])
JSON.parse xx.to_json(include: [:name], except: [:pocs])
xx.name
JSON.parse xx.to_json
xx = Pancakes::Stack[1423]
JSON.parse xx.to_json
JSON.parse xx.to_json(include: [:name], except: [:pocs])
JSON.parse xx.to_json(include: [:name])
xx = Pancakes::Stack[1423]
JSON.parse xx.to_json
xx = Pancakes::Stack[1423]
JSON.parse xx.to_json
xx = Pancakes::Stack[1423]
JSON.parse xx.to_json
JSON.parse xx.to_json(include: [:name])
JSON.parse xx.to_json(include: [:name, :units])
JSON.parse xx.to_json(include: [:name, {units: [:id]}])
JSON.parse xx.to_json(include: {name: {}, units: [:id]})
JSON.parse xx.to_json(include: {name: {}, units: { include: :id}})
xx.to_json(include: {name: {}, units: { include: :id}})
JSON.parse xx.to_json(include: {name: {}, units: { include: :id}})
JSON.parse xx.to_json(include: {name: {}, units: { only: :id}})
JSON.parse xx.to_json(include: {name: {}, units: { only: [:id] }})
xx = Pancakes::Stack[1423]
xx.as_json
ls {}
xx = Pancakes::Stack[1423]
xx.as_json
xx = Pancakes::Stack[1423]
xx.as_json
xx = Pancakes::Stack[1423]
xx.as_json
xx = Pancakes::Stack[1423]
xx.as_json
xx = Pancakes::Stack[1423]
xx.as_json
JSON.dump xx.as_json
xx = Pancakes::Stack[1423]
xx.as_json
xx.unit_with_service_name(:mysql)
xx.unit_with_service_name(:mysql).addr
xx.unit_with_service_name(:mysql).tcp_str
xx = Pancakes::Stack[1423]
xx.unit_with_service_name("jarvis-setup").env_string
xx.unit_with_service_name("jarvis-setup")
xx = Pancakes::Stack[1423]
xx.unit_with_service_name("jarvis-setup")
xx.unit_with_service_name("jarvis-setup").env_string
type
xx = Pancakes::Stack[1423]
xx.unit_with_service_name("jarvis-setup").env_string
c
xx = Pancakes::Stack[1423]
xx.unit_with_service_name("jarvis-setup").env_string
xx.unit_with_service_name("jarvis-setup").env_str
puts xx.unit_with_service_name("jarvis-setup").env_str
xx.as_json
xx = Pancakes::Stack[1423]
xx.as_json
xx = Pancakes::Stack[1423]
xx.restart_containers
xx.as_json
xx = Pancakes::Stack[1423]
xx.restart_containers
xx = Pancakes::Stack[1423]
xx.destroy_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'buttermilk', cargo_branch: 'feature/114986971/extract-setup-pattern', commit_specs: {})
xx.restart_containers
xx.destroy_containers
xx.restart_containers
xx = Pancakes::Stack[1424]
xx.restart_containers
xx = Pancakes::Stack[1424]
xx.restart_containers
xx = Pancakes::Stack[1424]
xx.restart_containers
xx = Pancakes::Stack[1424]
xx.restart_containers
xx = Pancakes::Stack[1424]
xx.restart_containers
xx = Pancakes::Stack[1424]
xx.restart_containers
xx = Pancakes::Stack[1424]
xx.restart_containers
xx.as_json
xx.units.map(&:addr)
xx = Pancakes::Stack[1424]
xx.units.map(&:addr)
xx.units.select{|uu| uu.exposed_ports }
xx.units.select{|uu| !!uu.exposed_ports }
xx.units.select{|uu| !!uu.exposed_ports }.map(&:service_name)
xx.units.select{|uu| !!uu.exposed_ports }.map(&:name)
xx.units.select{|uu| !!uu.exposed_ports }.map(&:service_name)
xx = Pancakes::Stack[1424]
ss.service_addrs
xx.service_addrs
xx = Pancakes::Stack[1424]
xx.service_addrs
xx.service_addrs_get_addrs
xx.service_addrs_get_addrs([])
xx.service_addrs_get_addrs([:foo])
xx.service_addrs_get_addrs(["mysql"])
xx.units.map {|x| x.address_hash_aliases }
xx.units.map {|x| x.address_hash_aliases }.flatten
xx.units.select{|x| !!x.exposed_ports }.map {|x| x.address_hash_aliases }.flatten
xx = Pancakes::Stack[1424]
xx.required_services
xx = Pancakes::Stack[1424]
xx.required_services
xx = Pancakes::Stack[1424]
xx.required_services
xx.services_with_addr_required
xx.units_with_required_addr
xx = Pancakes::Stack[1424]
xx.units_with_required_addr
xx.units_with_required_addr.map{ |unit| !!unit.addr }
xx.units_with_required_addr.map{ |unit| !!unit.addr }.detect{|bool| !bool }
[true, true, true].detect{|bool| !bool }
!![true, true, true].detect{|bool| !bool }
!![true, true, false].detect{|bool| !bool }
![true, true, false].detect{|bool| !bool }
![true, true, true].detect{|bool| !bool }
[true, true, false].reduce(true){|acc,val| acc && val  }
[true, true, true].reduce(true){|acc,val| acc && val  }
xx = Pancakes::Stack[1424]
xx.restart_containers
xx = Pancakes::Stack[1424]
xx.restart_containers
xx.addrs_ready?
xx.units_with_required_addr
xx.refresh_service_addrs
xx.units_with_required_addr
xx = Pancakes::Stack[1424]
xx.restart_containers
xx = Pancakes::Stack[1424]
xx.restart_containers
xx = Pancakes::Stack[1424]
xx.restart_containers
xx.check_health!
xx = Pancakes::Stack[1424]
xx.restart_containers
xx = Pancakes::Stack[1424]
xx.restart_containers
xx.destroy_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'buttermilk', cargo_branch: 'feature/114986971/extract-setup-pattern', commit_specs: {})
xx.restart_containers
xx.check_health!
xx.restart_containers
xx = Pancakes::Stack[1425]
xx.restart_containers
xx = Pancakes::Stack[1425]
xx.restart_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'buttermilk', cargo_branch: 'feature/114986971/extract-setup-pattern', commit_specs: {})
xx.restart_containers
xx = Pancakes::Stack[1426]
xx.restart_containers
xx = Pancakes::Stack[1426]
xx.restart_containers
xx.destroy_containers
xx = Pancakes::Stack[1426]
xx.restart_containers
xx.pull_images
xx.restart_containers
xx = Pancakes::Stack[1426]
xx.restart_containers
xx.unit_with_service_name('wp-server')
yy = xx.unit_with_service_name('wp-server')
yy.ping_url
Net::HTTP.get_response(URI(yy.ping_url))
xx = Net::HTTP.get_response(URI(yy.ping_url))
xx.vode
xx.code
RestClient::Request.execute(method: :get, url: yy.ping_url, timeout: 5, open_timeout: 5)
xx = Pancakes::Stack[1426]
xx.as_json
xx = Pancakes::Stack[1426]
xx.restart_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'honesty', cargo_branch: 'feature/114986971/extract-setup-pattern', commit_specs: {})
xx.restart_containers
xx = Pancakes::Stack[1426]
xx.destroy_containers
xx = Pancakes::Stack[1427]
xx.destroy_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'buttermilk', cargo_branch: 'feature/114986971/extract-setup-pattern', commit_specs: {})
xx.restart_containers
xx = Pancakes::Stack[1428]
xx.restart_containers
xx = Pancakes::Stack[1428]
xx.restart_containers
xx.destroy_containers
xx = Pancakes::Stack[1428]
xx.restart_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'buttermilk', cargo_branch: 'feature/114986971/extract-setup-pattern', commit_specs: {})
xx.restart_containers
xx = Pancakes::Stack[1428]
xx.destroy_containers
Grnds::Service::Urls::SERVICE_PORTS
Pancakes::Service.last
Pancakes::Unit.last
Pancakes::Unit.last.service_name
Pancakes::Unit[17138].service_name
Pancakes::Unit['17138'].service_name
"foobar/atomized/fluxx" =~ /atomized/
"foobar/atomized/fluxx" =~ /atomizedx/
"foobar/atomized/fluxx" =~ %R|/atomized/|
"foobar/atomized/fluxx" =~ %r|/atomized/|
"foobar/atomized/fluxx" =~ %R|/atomized/|
"foobar/atomized/fluxx" =~ %r|/atomized/|
DataMapper.auto_upgrade!
Integrity::Branch.all(name: 'master')
Integrity::Branch.all(name: 'master').map{|x|x.id  }
Integrity::Branch.all(name: 'master').map{|x| x.last_build_id  }
Integrity::Branch.all(name: 'master').map{|x| [x.last_build_id, x.project_id]  }
Integrity::Branch.all(name: 'master').each{|x| prj = x.project ; prj.update_last_build(x.last_build)  }
Integrity::Branch.first.last_build
Integrity::Branch.all(name: 'master').each{|x| prj = x.project ; puts prj.name ; prj.update_last_build(x.last_build)  }
Integrity::Project.all().each{|x| x.update_last_build(x.master_branch.last_build) }
Integrity::Project.all().each{|x| puts x.name }
Integrity::Project.all().each{|x| puts x.name } ; nil
Integrity::Project.all().each{|x| puts x.name, x.master_branch.last_build }
Integrity::Project.all().each{|x| puts x.name, x.master_branch.try(:last_build) }
Integrity::Project.all().each{|x| puts x.name, x.master_branch.try(:last_build) } ; nil
Integrity::Project.get(10)
Integrity::Project.get(10).master
Integrity::Project.get(10).master_branch
Integrity::Project.get(10).master_branch.last_build
Integrity::Project.get(10).master_branch.builds.count
Integrity::Build.get(25962)
xx = Integrity::Project.get(10).master_branch
xx.last_build_id = 25953
xx.save!
Integrity::Project.all().each{|x| puts x.name, x.master_branch.try(:last_build) } ; nil
Integrity::Project.all().each{|x| puts x.name, x.master_branch.last_build } ; nil
Integrity::Project.all
Integrity::Project.all.branches
Integrity::Project.all.branches(name: 'master');nil
Integrity::Project.all.branches; nil
x=Integrity::Project.all.branches;1
x=Integrity::Project.all.branches
x=Integrity::Project.all.branches;x.count
x=Integrity::Project.all.branches.to_a;x.size
Integrity::Build.all.to_a; nil
Integrity::Build.project; nil
Integrity::Build.project.to_a; nil
Integrity::Build.all.project.to_a; nil
Integrity::Build.all.projects.to_a; nil
Integrity::Project.first
Integrity::Project.first.builds.count
repository { Integrity::Project.all.each {|p| p.last_master_build } }
repository { Integrity::Project.all.each {|p| p.last_master_build } } ; nil
repository { Integrity::Project.all.each {|p| p.last_master_build.success } } ; nil
repository { Integrity::Project.all.each {|p| p.last_master_build.success? } } ; nil
repository { Integrity::Project.all.each {|p| p.last_master_build.successful } } ; nil
repository { Integrity::Project.all.each {|p| p.last_master_build.try(:successful) } } ; nil
repository { Integrity::Project.all.each {|p| p.last_master_build.try(:successful) } }
xx = repository { Integrity::Project.all.each {|p| p.last_master_build.try(:successful) } }
xx = repository { Integrity::Project.all.map {|p| p.last_master_build.try(:successful) } }
xx = repository { Integrity::Project.all.map {|p| [p, p.last_master_build.try(:successful)] } }
xx = repository { Integrity::Project.all.map {|p| [p, p.last_master_build] } }
Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc)
Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.latest_build}
foo = Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.latest_build}.to_a;nil
foo = repository { Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.latest_build}.to_a};nil
foo = repository { Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.latest_build}};nil
@foo =  repository { Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.latest_build }}
DataMapper.auto_upgrade!
Tracker::Story.first.latest_build_id
Tracker::Story.first.last_build_id
Tracker::Story.first.populate_latest_build
Tracker::Story.last.populate_latest_build
Tracker::Story.all.to_a.detect{|s| s.inefficient_latest_build }
Tracker::Story.all.to_a.detect{|s| s.inefficient_latest_build }.iod
Tracker::Story.all.to_a.detect{|s| s.inefficient_latest_build }.id
Tracker::Story.get(86826004).populate_latest_build
Tracker::Story.get(86826004).populate_last_build
Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).populate_last_build
end
Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|x| x.populate_last_build }
repository { Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.latest_build }} ; nil
repository { Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build }} ; nil
xx = repository { Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build }}
xx.last.last_build
xx[1].last_build
xx[2].last_build
xx[3].last_build
xx = repository { Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build ; x.project }}
xx = repository { Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build ; s.project }}
xx = repository { Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build ; s.project }} ; nil
xx = repository { Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build ; }} ; nil
xx = repository { Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s|  s.project }} ; nil
xx = repository { Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build ; s.project }} ; nil
xx[2].project
xx = repository {foo = [] Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| foo << s.last_build  }} ; nil
xx = repository {foo = []; Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| foo << s.last_build  }} ; nil
xx = repository {foo = []; Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| foo << s.last_build  } ; foo.each{|x| x.project }} ; nil
xx = repository {foo = []; Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| foo << s.last_build  } ; foo.each{|x| x.try(:project) }} ; nil
xx = repository {foo = []; Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| foo << s.last_build  } ; foo.map(&:project_id).each{|x| puts x }} ; nil
xx = repository {foo = []; Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| foo << s.last_build  } ; foo.compact.map(&:project_id).each{|x| puts x }} ; nil
xx = repository {foo = []; Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| foo << s.last_build  } ; Integrity::Project.all(foo.compact.map(&:project_id)) } ; nil
xx = repository {foo = []; Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| foo << s.last_build  } ; Integrity::Project.find(foo.compact.map(&:project_id)) } ; nil
xx = repository {foo = []; Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| foo << s.last_build  } ; Integrity::Project.get(foo.compact.map(&:project_id)) } ; nil
xx = repository {foo = []; Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| bb = s.last_build ; bb.project } } ; nil
xx = repository {foo = []; Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| bb = s.last_build ; bb.try(:project) } } ; nil
"fluxx/atomized/asdf" =~ %r|/atomized/| ? :atomized : :legacy
"fluxx/atomize/asdf" =~ %r|/atomized/| ? :atomized : :legacy
"fluxx/atomized/asdf" =~ %r|/atomized/| ? :atomized : :legacy
cd HonestyCluster
cd SwarmBackend
cd DockerUtil
docker_api_container_json
docker_api_container_json()
cd ..
include Docker
include DockerUtil
docker_api_container_json()
include HonestyCluster::SwarmBackend::DockerUtil
json = {name: 'foo', hostname: 'foo', image:, cmd:, links:nil, exposed_ports:nil, env:{'FOO'=>'bar'}, labels: nil, cpu_shares: nil, memory: nil, log_identifier: 'foo'}
json = {name: 'foo', hostname: 'foo', image:'009840661483.dkr.ecr.us-east-1.amazonaws.com/king-cargo:dev', cmd:cmd, links:nil, exposed_ports:nil, env:{'FOO'=>'bar'}, labels: nil, cpu_shares: nil, memory: nil, log_identifier: 'foo'}
ENV['GR_HOME']
File.join(ENV['GR_HOME'], 'cargo')
File.directory? File.join(ENV['GR_HOME'], 'cargo')
File.join(ENV['GR_HOME'], 'cargo', 'tmp')
FileUtils.mkdir_p File.join(ENV['GR_HOME'], 'cargo', 'tmp')
File.join(ENV['GR_HOME'], 'cargo', 'tmp', nil)
File.join(ENV['GR_HOME'], 'cargo', 'tmp', '')
File.join(ENV['GR_HOME'], 'cargo', 'tmp')
File.join(*[ENV['GR_HOME'], 'cargo', path].compact)
File.join(*[ENV['GR_HOME'], 'cargo', ''].compact)
File.join(*[ENV['GR_HOME'], 'cargo', 'asdf'].compact)
File.join(*[ENV['GR_HOME'], 'cargo', nil].compact)
ENV['SWARM_ENV']
Integrity::Build.last
Integrity::Build.last.rebuild
Integrity::Build.last.rebuild!
xx = Integrity::Build.last
xx.project
xx.project.build(xx.commit, true)
xx = Integrity::Build.last
xx.project.build(xx.commit, true)
xx = Integrity::Build.last
xx.project.build(xx.commit, true)
xx = Integrity::Build.last
xx.project.build(xx.commit, true)
xx = Integrity::Build.last
xx.project.build(xx.commit, true)
Integrity::Project.get(13)
Integrity::Project.get(14)
Integrity::Project.get(15)
Integrity::Project.get(16)
Integrity::Project.get(17)
Integrity::Project.get(18)
Integrity::Project.get(19
Integrity::Project.get(19)
Integrity::Project.get(3)
Integrity::Project.get(4)
Integrity::Project.get(5)
Integrity::Project.get(6)
Integrity::Project.get(7)
Integrity::Project.get(8)
Integrity::Project.get(9)
Integrity::Project.get(10)
Integrity::Project.get(20)
Integrity::Project.get(21)
Integrity::Project.get(22)
Integrity::Project.get(23)
Integrity::Project.get(24)
Integrity::Project.get(25)
Integrity::Project.get(2666)
Integrity::Project.get(26)
Integrity::Project.get(27)
Integrity::Project.get(28)
Integrity::Project.get(29)
Integrity::Project.all
Integrity::Project.get(12)
Integrity::Project.get(12).last_build
xx = Integrity::Project.get(12) ; xx.build(xx.last_build.commit, true)
xx = Integrity::Project.get(12) ; xx.build(xx.last_master_build.commit, true)
xx = Integrity::Project.get(12) ; xx.build(xx.master_branch.build.commit, true)
xx = Integrity::Project.get(12) ; xx.build(xx.master_branch.commit, true)
xx = Integrity::Project.get(12) ; xx.build(xx.master_branch.last_build.commit, true)
ENV['HONESTY_PRIVATE_SSH_KEY']
puts ENV['HONESTY_PRIVATE_SSH_KEY']
xx = Integrity::Project.get(12) ; xx.build(xx.master_branch.last_build.commit, true)
Dallli
Dalli
xx = Dalli::Client.new('gr-ca-15ts3hkl6xuyl.ih2bym.cfg.use1.cache.amazonaws.com:11211', {namespace: 'cargo', compress: true})
xx.put('foo', 'bar')
xx.set('foo', 'bar')
xx.get('foo')
Integrity.memcached
ENV['MEMCACHED_HOST']
Integrity.memcached
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'buttermilk', cargo_branch: 'master', commit_specs: {})
xx.run_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'buttermilk', cargo_branch: 'master', commit_specs: {})
xx.run_containers
xx.unit_with_service_name(:mailcatcher)
xx.unit_with_service_name(:mailcatcher).ports
xx.unit_with_service_name(:mailcatcher).public_ports
xx.unit_with_service_name(:mailcatcher).to_address_hash
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'pipeline', cargo_branch: 'master', commit_specs: {})
xx.restart_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'pipeline', cargo_branch: 'master', cargo_debug: true, commit_specs: {})
xx.restart_containers
xx = Pancakes::Stack.last
xx = Pancakes::Stack[1436]
xx = Pancakes::Stack[1436] ; xx.restart_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'pipeline', cargo_branch: 'master', cargo_debug: true, commit_specs: {})
xx = Pancakes::Stack[1437] ; xx.restart_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'pipeline', cargo_branch: 'master', cargo_debug: true, commit_specs: {})
xx = Pancakes::Stack[1438] ; xx.restart_containers
xx.unit_with_service_name('jarvis-setup')
xx.unit_with_service_name('jarvis-setup').env_var_string
xx.unit_with_service_name('jarvis-setup').env_str
xx = Pancakes::Stack[1438] ; xx.restart_containers
xx.unit_with_service_name('jarvis-setup').env_str
xx = Pancakes::Stack[1438] 
xx.unit_with_service_name('jarvis-setup').env_str
bn
bn.empty?
xx = Pancakes::Stack[1438] ; xx.restart_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'pipeline', cargo_branch: 'master', cargo_debug: true, commit_specs: {})
xx = Pancakes::Stack[1439] ; xx.restart_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'pipeline', cargo_branch: 'master', cargo_debug: true, commit_specs: {})
xx = Pancakes::Stack[1440] ; xx.restart_containers
xx.check_health!
loop { xx.check_health! ; sleep 3.1231 ]
loop { xx.check_health! ; sleep 3.1231 }
loop { puts xx.check_health!.healthy ; sleep 3.1231 }
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'pipeline', cargo_branch: 'master', cargo_debug: true, commit_specs: {})
xx = Pancakes::Stack[1441] ; xx.restart_containers
loop { puts xx.check_health!.healthy ; sleep 3.1231 }
xx = Pancakes::Stack[1441] ; xx.restart_containers
loop { puts xx.check_health!.healthy ; sleep 3.1231 }
xx = Pancakes::Stack[1441] ; xx.restart_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'pipeline', cargo_branch: 'master', cargo_debug: true, commit_specs: {})
xx = Pancakes::Stack[1442] ; xx.restart_containers
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'pipeline', cargo_branch: 'master', cargo_debug: true, commit_specs: {})
xx = Pancakes::Stack[1443] ; xx.restart_containers
xx.unit_with_service_name("warehouse-setup")
xx.unit_with_service_name("warehouse-setup").env_str
xx.unit_with_service_name("warehouse-setup").as_json
xx = Pancakes::Stack[1443] ; xx.restart_containers
Integrity.memcached
Integrity.memcached.set('foo', 'var')
Integrity.memcached.get('foo')
Integrity.memcached.stats
Integrity.memcached.delete('foo')
Integrity.memcached.stats
Integrity.memcached.stats(:items)
Integrity.memcached.stats('items')
Integrity.memcached.stats(:items)
Integrity.memcached.stats(:cachedump)
Integrity.memcached.stats(:items)
Integrity.memcached.stats(:cachedump, 1, 100)
Integrity.memcached.stats(:cachedump, [1, 100])
Integrity.memcached.stats(:cachedump, 1)
Integrity.memcached.stats(:cachedump)
Integrity.memcached.stats('cachedump 1 100')
Integrity.memcached.stats(:cachedump)
Integrity.memcached.stats(:items)
Integrity.memcached.flush
Integrity.memcached.stats(:items)
fg
include HonestyCluster::SwarmBackend::DockerUtil
containers_filtered
include HonestyCluster::SwarmBackend::DockerUtil
containers_filtered
containers_filtered({'honesty'=>''})
containers_filtered({'honesty'=>''}).count
containers_filtered({'honesty'=>''}).map{|x| info.Name}
containers_filtered({'honesty'=>''}).map{|x| x.info.Name }
containers_filtered({'honesty'=>''}).map{|x| x.info.Names.first }
containers_filtered({'honesty'=>''}).map{|x| x.info.name }
containers_filtered({'honesty'=>''}).map{|x| x.name }
HonestyCluster::SwarmBackend::CacheOfContainers.index_containers(containers_filtered({'honesty'=>''}))
include HonestyCluster::SwarmBackend::DockerUtil
HonestyCluster::SwarmBackend::CacheOfContainers.index_containers(containers_filtered({'honesty'=>''}))
include HonestyCluster::SwarmBackend::DockerUtil
HonestyCluster::SwarmBackend::CacheOfContainers.index_containers(containers_filtered({'honesty'=>''}))
indexed_by_cargo_id
indexed_by_cargo_id.keys.take(3)
w
indexed_by_cargo_group
Integrity.memcached.set('foo', 'bar', 2)
ls
include HonestyCluster::SwarmBackend::DockerUtil
docker_api_label_filters
docker_api_label_filters({'vargpo' => 234, 'casdf' => 2343})
xx = Hashie::Mash.new
require 'hashie'
xx = Hashie::Mash.new
xx['foo']
xx['foo']['bar'] = asdf
xx['foo']['bar'] = 123
xx = Hashie::Clash.new
xx['foo']['bar'] = 123
xx.foo.bar = 123
xx.foo.bar(:foo)
xx.foo(:asdf).bar(:foo)
HonestyCluster::Cargo.config
HonestyCluster::Cargo.config.cargo
HonestyCluster::Cargo.config.bind_locally
ENV['SWARM_ENV']
fg
xx = Pancakes::Stack.create_with_commit_specs(owner: 'bird', stack_type: 'honesty', cargo_branch: 'master', cargo_debug: true, commit_specs: {})
xx.run_containers
xx = Pancakes::Stack[1444] ; xx.restart_containers
foo = nil
bar = :foo
foo || bar
foo && bar
foo = false
foo && bar
(true && true) || :foo
(true && "bob") || :foo
(true && "cfoo") || :foo
(false && "cfoo") || :foo
(true && "cfoo") || :foo
(true && nil) || :foo
xx = Pancakes::Stack[1444] ; xx.restart_containers
backtrace!
backtrace
xx = Pancakes::Stack[1444] ; xx.restart_containers
logger_name
c
logger_name
xx = Pancakes::Stack[1444] ; xx.restart_containers
logger_name
c
logger_name
xx = Pancakes::Stack[1444] ; xx.restart_containers
logger_name
c
logger_name
c
xx.check_health!.healty?
xx.check_health!.healthy?
loop { xx.check_health!.healthy? ; sleep 43}
loop { xx.check_health!.healthy? ; sleep 3}
loop { puts xx.check_health!.healthy? ; sleep 3}
xx = Pancakes::Stack[1444] ; xx.restart_containers
loop { puts xx.check_health!.healthy? ; sleep 3}
Cargo::DockerApi.store_containers
cd Cargo
cd DockerApi
memcached.delete 'foo'
memcached.delete 'bar'
store_containers
Cargo::DockerApi.store_containers
Cargo.config.memcached.get 'bar'
Cargo.config.memcached_clienbt.get 'bar'
Cargo.config.memcached_client.get 'bar'
Cargo.config.memcached_client.get 'barx'
{}.object_id
{}.object_id.to_s(16)
nil.object_id.to_s(16)
def self.foo ; {} ; end
foo
foo.object_id
def self.foo ; @__foo ||= {} ; end
foo.object_id
@stories_in_flight = repository do
  +            Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each do |s|
    +              s.last_build
    +              s.project
  +            end
+          end
Story.last
Tracker::Story.last
Tracker::Story.last.project
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build; s.project }}
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build; s.project }}; nil
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build }}; nil
Integrity::Build.last
Integrity::Build.last.project
Integrity::Build.last.project.name
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build.class }}; nil
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build.class }}
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| puts s.last_build.class }}
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| puts s.last_build.class }}; nil
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| xx = s.last_build ; xx.project }}; nil
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| xx = s.last_build ; xx.try(:project) }}; nil
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| xx = s.last_build ; xx.try(:project) }}; nil
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| xx = s.last_build ; xx.try(:project) }};
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| xx = s.last_build ; xx.try(:project) }}
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build }}
repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build }}; 1
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build }}.map{|b| b.project } }
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).each {|s| s.last_build }}.map{|b| b.class} }
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| s.last_build }}.map{|b| b.class} }
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| s.last_build } } }
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| s.last_build }}.map{|b| b.project} }
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| s.last_build }}.map{|b| b.try(:project) } }
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| s.last_build }}.map{|b| b.try(:project).name } }
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| s.last_build }}.map{|b| b.try(:project).try(:name) } }
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| s.last_build }}.map{|b| b.try(:id) } }
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| s.last_build }}.map{|b| b.try(:project_id) } }
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| s.last_build }}.map{|b| b.try(:project_id) } }.compact
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| s.last_build }}.map{|b| b.try(:project_id) } }.uniq
repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| s.last_build }}.map{|b| b.try(:project_id) } }.compact.uniq
project_ids = repository { repository {Tracker::Story.all(:current_state => ['started', 'finished'], :order => :started_at.asc).map {|s| s.last_build }}.map{|b| b.try(:project_id) } }.compact.uniq
Integrity::Project.get(project_ids)
Integrity::Project.get_all(project_ids)
Integrity::Project.all(project_ids)
Integrity::Project.all(id: project_ids)
Integrity::Project.all(id: project_ids).map {|r| r.project}
Integrity::Project.all(id: project_ids).map {|r| r.project_name}
Integrity::Project.all(id: project_ids).map {|r| r.name}
Integrity::Project.all(id: project_ids).collect {|r| {r.id => r.name} }
Integrity::Project.all(id: project_ids).map {|r| {r.id => r.name} }.reduce
Integrity::Project.all(id: project_ids).map {|r| {r.id => r.name} }
Integrity::Project.all(id: project_ids).map {|r| {r.id => r.name} }.reduce(&:merge)
